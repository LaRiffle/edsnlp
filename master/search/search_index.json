{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>EDS-NLP provides a set of spaCy components that are used to extract information from clinical notes written in French.</p> <p>If it's your first time with spaCy, we recommend you familiarise yourself with some of their key concepts by looking at the \"spaCy 101\" page.</p>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#installation","title":"Installation","text":"<p>You can install EDS-NLP via <code>pip</code>:</p> <pre><code>$ pip install edsnlp\n---&gt; 100%\ncolor:green Successfully installed!\n</code></pre> <p>We recommend pinning the library version in your projects, or use a strict package manager like Poetry.</p> <pre><code>pip install edsnlp==0.8.0\n</code></pre>"},{"location":"#a-first-pipeline","title":"A first pipeline","text":"<p>Once you've installed the library, let's begin with a very simple example that extracts mentions of COVID19 in a text, and detects whether they are negated.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")  # (1)\n\nterms = dict(\n    covid=[\"covid\", \"coronavirus\"],  # (2)\n)\n\n# Sentencizer component, needed for negation detection\nnlp.add_pipe(\"eds.sentences\")  # (3)\n# Matcher component\nnlp.add_pipe(\"eds.matcher\", config=dict(terms=terms))  # (4)\n# Negation detection\nnlp.add_pipe(\"eds.negation\")\n\n# Process your text in one call !\ndoc = nlp(\"Le patient est atteint de covid\")\n\ndoc.ents  # (5)\n# Out: (covid,)\n\ndoc.ents[0]._.negation  # (6)\n# Out: False\n</code></pre> <ol> <li>We only need spaCy's French tokenizer.</li> <li>This example terminology provides a very simple, and by no means exhaustive, list of synonyms for COVID19.</li> <li>In spaCy, pipelines are added via the <code>nlp.add_pipe</code> method. EDS-NLP pipelines are automatically discovered by spaCy.</li> <li>See the matching tutorial for mode details.</li> <li>spaCy stores extracted entities in the <code>Doc.ents</code> attribute.</li> <li>The <code>eds.negation</code> pipeline has added a <code>negation</code> custom attribute.</li> </ol> <p>This example is complete, it should run as-is. Check out the spaCy 101 page if you're not familiar with spaCy.</p>"},{"location":"#available-pipeline-components","title":"Available pipeline components","text":"CoreQualifiersMiscellaneousNERTrainable Pipeline Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual-matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection Pipeline Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.measurements</code> Measure extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection Pipeline Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.emergency.priority</code> A priority score extractor <code>eds.emergency.ccmu</code> A CCMU score extractor <code>eds.emergency.gemsa</code> A GEMSA score extractor <code>eds.TNM</code> A TNM score extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.drugs</code> A Drug mentions extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.umls</code> A UMLS terminology matcher Pipeline Description <code>eds.nested-ner</code> Nested and overlapping named entity recogntion"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>The performances of an extraction pipeline may depend on the population and documents that are considered.</p>"},{"location":"#contributing-to-eds-nlp","title":"Contributing to EDS-NLP","text":"<p>We welcome contributions ! Fork the project and propose a pull request. Take a look at the dedicated page for detail.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use EDS-NLP, please cite us as below.</p> <pre><code>@misc{edsnlp,\nauthor = {Dura, Basile and Wajsburt, Perceval and Petit-Jean, Thomas and Cohen, Ariel and Jean, Charline and Bey, Romain},\ndoi    = {10.5281/zenodo.6424993},\ntitle  = {EDS-NLP: efficient information extraction from French clinical notes},\nurl    = {http://aphp.github.io/edsnlp}\n}\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li><code>eds.tables</code>: new pipeline to identify formatted tables</li> </ul>"},{"location":"changelog/#unreleased_1","title":"Unreleased","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add range measurements (like <code>la tumeur fait entre 1 et 2 cm</code>) to <code>eds.measurements</code> matcher</li> <li>Add <code>eds.spaces</code> (or <code>eds.normalizer</code> with <code>spaces=True</code>) to detect space tokens, and add <code>ignore_space_tokens</code> to <code>EDSPhraseMatcher</code> and <code>SimstringMatcher</code> to skip them</li> <li>Add <code>ignore_space_tokens</code> option in most components</li> <li>New <code>merge_mode</code> parameter in <code>eds.measurements</code> to normalize existing entities or detect   measures only inside existing entities</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Abbreviation and number tokenization issues in the <code>eds</code> tokenizer</li> </ul>"},{"location":"changelog/#v080-2023-03-09","title":"v0.8.0 (2023-03-09)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Tokenization exceptions (<code>Mr.</code>, <code>Dr.</code>, <code>Mrs.</code>) and non end-of-sentence periods are now tokenized with the next letter in the <code>eds</code> tokenizer</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Disable <code>EDSMatcher</code> preprocessing auto progress tracking by default</li> <li>Moved dependencies to a single pyproject.toml: support for <code>pip install -e '.[dev,docs,setup]'</code></li> <li>ADICAP matcher now allow dot separators (e.g. <code>B.H.HP.A7A0</code>)</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li><code>eds.adicap</code> : reparsed the dictionnary used to decode the ADICAP codes (some of them were wrongly decoded)</li> </ul>"},{"location":"changelog/#v074-2022-12-12","title":"v0.7.4 (2022-12-12)","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li><code>eds.history</code> : Add the option to consider only the closest dates in the sentence (dates inside the boundaries and if there is not, it takes the closest date in the entire sentence).</li> <li><code>eds.negation</code> : It takes into account following past participates and preceding infinitives.</li> <li><code>eds.hypothesis</code>: It takes into account following past participates hypothesis verbs.</li> <li><code>eds.negation</code> &amp; <code>eds.hypothesis</code> : Introduce new patterns and remove unnecessary patterns.</li> <li><code>eds.dates</code> : Add a pattern for preceding relative dates (ex: l'embolie qui est survenue \u00e0 10 jours).</li> <li>Improve patterns in the <code>eds.pollution</code> component to account for multiline footers</li> <li>Add <code>QuickExample</code> object to quickly try a pipeline.</li> <li>Add UMLS terminology matcher <code>eds.umls</code></li> <li>New <code>RegexMatcher</code> method to create spans from groupdicts</li> <li>New <code>eds.dates</code> option to disable time detection</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Improve date detection by removing false positives</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li><code>eds.hypothesis</code> : Remove too generic patterns.</li> <li><code>EDSTokenizer</code> : It now tokenizes <code>\"rechereche d'\"</code> as <code>[\"recherche\", \"d'\"]</code>, instead of <code>[\"recherche\", \"d\", \"'\"]</code>.</li> <li>Fix small typos in the documentation and in the docstring.</li> <li>Harmonize processing utils (distributed custom_pipe) to have the same API for Pandas and Pyspark</li> <li>Fix BratConnector file loading issues with complex file hierarchies</li> </ul>"},{"location":"changelog/#v072-2022-10-26","title":"v0.7.2 (2022-10-26)","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Improve the <code>eds.history</code> component by taking into account the date extracted from <code>eds.dates</code> component.</li> <li>New pop up when you click on the copy icon in the termynal widget (docs).</li> <li>Add NER <code>eds.elston-ellis</code> pipeline to identify Elston Ellis scores</li> <li>Add flags=re.MULTILINE to <code>eds.pollution</code> and change pattern of footer</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Remove the warning in the <code>eds.sections</code> when <code>eds.normalizer</code> is in the pipe.</li> <li>Fix filter_spans for strictly nested entities</li> <li>Fill eds.remove-lowercase \"assign\" metadata to run the pipeline during EDSPhraseMatcher preprocessing</li> <li>Allow back spaCy components whose name contains a dot (forbidden since spaCy v3.4.2) for backward compatibility.</li> </ul>"},{"location":"changelog/#v071-2022-10-13","title":"v0.7.1 (2022-10-13)","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Add new patterns (footer, web entities, biology tables, coding sections) to pipeline normalisation (pollution)</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Improved TNM detection algorithm</li> <li>Account for more modifiers in ADICAP codes detection</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Add nephew, niece and daughter to family qualifier patterns</li> <li>EDSTokenizer (<code>spacy.blank('eds')</code>) now recognizes non-breaking whitespaces as spaces and does not split float numbers</li> <li><code>eds.dates</code> pipeline now allows new lines as space separators in dates</li> </ul>"},{"location":"changelog/#v070-2022-09-06","title":"v0.7.0 (2022-09-06)","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>New nested NER trainable <code>nested_ner</code> pipeline component</li> <li>Support for nested entities and attributes in BratDataConnector</li> <li>Pytorch wrappers and experimental training utils</li> <li>Add attribute <code>section</code> to entities</li> <li>Add new cases for separator pattern when components of the TNM score are separated by a forward slash</li> <li>Add NER <code>eds.adicap</code> pipeline to identify ADICAP codes</li> <li>Add patterns to <code>pollution</code> pipeline and simplifies activating or deactivating specific patterns</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Simplified the configuration scheme of the <code>pollution</code> pipeline</li> <li>Update of the <code>ContextualMatcher</code> (and all pipelines depending on it), rendering it more flexible to use</li> <li>Rename R component of score TNM as \"resection_completeness\"</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Prevent section titles from capturing surrounding tokens, causing overlaps (#113)</li> <li>Enhance existing patterns for section detection and add patterns for previously ignored sections (introduction, evolution, modalites de sortie, vaccination) .</li> <li>Fix explain mode, which was always triggered, in <code>eds.history</code> factory.</li> <li>Fix test in <code>eds.sections</code>. Previously, no check was done</li> <li>Remove SOFA scores spurious span suffixes</li> </ul>"},{"location":"changelog/#v062-2022-08-02","title":"v0.6.2 (2022-08-02)","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>New <code>SimstringMatcher</code> matcher to perform fuzzy term matching, and <code>algorithm</code> parameter in terminology components and <code>eds.matcher</code> component</li> <li>Makefile to install,test the application and see the documentation</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Add consultation date pattern \"CS\", and False Positive patterns for dates (namely phone numbers and pagination).</li> <li>Update the pipeline score <code>eds.TNM</code>. Now it is possible to return a dictionary where the results are either <code>str</code> or <code>int</code> values</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Add new patterns to the negation qualifier</li> <li>Numpy header issues with binary distributed packages</li> <li>Simstring dependency on Windows</li> </ul>"},{"location":"changelog/#v061-2022-07-11","title":"v0.6.1 (2022-07-11)","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Now possible to provide regex flags when using the RegexMatcher</li> <li>New <code>ContextualMatcher</code> pipe, aiming at replacing the <code>AdvancedRegex</code> pipe.</li> <li>New <code>as_ents</code> parameter for <code>eds.dates</code>, to save detected dates as entities</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Faster <code>eds.sentences</code> pipeline component with Cython</li> <li>Bump version of Pydantic in <code>requirements.txt</code> to 1.8.2 to handle an incompatibility with the ContextualMatcher</li> <li>Optimise space requirements by using <code>.csv.gz</code> compression for verbs</li> </ul>"},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li><code>eds.sentences</code> behaviour with dot-delimited dates (eg <code>02.07.2022</code>, which counted as three sentences)</li> </ul>"},{"location":"changelog/#v060-2022-06-17","title":"v0.6.0 (2022-06-17)","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Complete revamp of the measurements detection pipeline, with better parsing and more exhaustive matching</li> <li>Add new functionality to the method <code>Span._.date.to_datetime()</code> to return a result infered from context for those cases with missing information.</li> <li>Force a batch size of 2000 when distributing a pipeline with Spark</li> <li>New patterns to pipeline <code>eds.dates</code> to identify cases where only the month is mentioned</li> <li>New <code>eds.terminology</code> component for generic terminology matching, using the <code>kb_id_</code> attribute to store fine-grained entity label</li> <li>New <code>eds.cim10</code> terminology matching pipeline</li> <li>New <code>eds.drugs</code> terminology pipeline that maps brand names and active ingredients to a unique ATC code</li> </ul>"},{"location":"changelog/#v053-2022-05-04","title":"v0.5.3 (2022-05-04)","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Support for strings in the example utility</li> <li>TNM detection and normalisation with the <code>eds.TNM</code> pipeline</li> <li>Support for arbitrary callback for Pandas multiprocessing, with the <code>callback</code> argument</li> </ul>"},{"location":"changelog/#v052-2022-04-29","title":"v0.5.2 (2022-04-29)","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Support for chained attributes in the <code>processing</code> pipelines</li> <li>Colour utility with the category20 colour palette</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Correct a REGEX on the date detector (both <code>nov</code> and <code>nov.</code> are now detected, as all other months)</li> </ul>"},{"location":"changelog/#v051-2022-04-11","title":"v0.5.1 (2022-04-11)","text":""},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Updated Numpy requirements to be compatible with the <code>EDSPhraseMatcher</code></li> </ul>"},{"location":"changelog/#v050-2022-04-08","title":"v0.5.0 (2022-04-08)","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>New <code>eds</code> language to better fit French clinical documents and improve speed</li> <li>Testing for markdown codeblocks to make sure the documentation is actually executable</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>Complete revamp of the date detection pipeline, with better parsing and more exhaustive matching</li> <li>Reimplementation of the EDSPhraseMatcher in Cython, leading to a x15 speed increase</li> </ul>"},{"location":"changelog/#v044","title":"v0.4.4","text":"<ul> <li>Add <code>measures</code> pipeline</li> <li>Cap Jinja2 version to fix mkdocs</li> <li>Adding the possibility to add context in the processing module</li> <li>Improve the speed of char replacement pipelines (accents and quotes)</li> <li>Improve the speed of the regex matcher</li> </ul>"},{"location":"changelog/#v043","title":"v0.4.3","text":"<ul> <li>Fix regex matching on spans.</li> <li>Add fast_parse in date pipeline.</li> <li>Add relative_date information parsing</li> </ul>"},{"location":"changelog/#v042","title":"v0.4.2","text":"<ul> <li>Fix issue with <code>dateparser</code> library (see scrapinghub/dateparser#1045)</li> <li>Fix <code>attr</code> issue in the <code>advanced-regex</code> pipelin</li> <li>Add documentation for <code>eds.covid</code></li> <li>Update the demo with an explanation for the regex</li> </ul>"},{"location":"changelog/#v041","title":"v0.4.1","text":"<ul> <li>Added support to Koalas DataFrames in the <code>edsnlp.processing</code> pipe.</li> <li>Added <code>eds.covid</code> NER pipeline for detecting COVID19 mentions.</li> </ul>"},{"location":"changelog/#v040","title":"v0.4.0","text":"<ul> <li>Profound re-write of the normalisation :</li> <li>The custom attribute <code>CUSTOM_NORM</code> is completely abandoned in favour of a more spacyfic alternative</li> <li>The <code>normalizer</code> pipeline modifies the <code>NORM</code> attribute in place</li> <li>Other pipelines can modify the <code>Token._.excluded</code> custom attribute</li> <li>EDS regex and term matchers can ignore excluded tokens during matching, effectively adding a second dimension to normalisation (choice of the attribute and possibility to skip pollution tokens regardless of the attribute)</li> <li>Matching can be performed on custom attributes more easily</li> <li>Qualifiers are regrouped together within the <code>edsnlp.qualifiers</code> submodule, the inheritance from the <code>GenericMatcher</code> is dropped.</li> <li><code>edsnlp.utils.filter.filter_spans</code> now accepts a <code>label_to_remove</code> parameter. If set, only corresponding spans are removed, along with overlapping spans. Primary use-case: removing pseudo cues for qualifiers.</li> <li>Generalise the naming convention for extensions, which keep the same name as the pipeline that created them (eg <code>Span._.negation</code> for the <code>eds.negation</code> pipeline). The previous convention is kept for now, but calling it issues a warning.</li> <li>The <code>dates</code> pipeline underwent some light formatting to increase robustness and fix a few issues</li> <li>A new <code>consultation_dates</code> pipeline was added, which looks for dates preceded by expressions specific to consultation dates</li> <li>In rule-based processing, the <code>terms.py</code> submodule is replaced by <code>patterns.py</code> to reflect the possible presence of regular expressions</li> <li>Refactoring of the architecture :</li> <li>pipelines are now regrouped by type (<code>core</code>, <code>ner</code>, <code>misc</code>, <code>qualifiers</code>)</li> <li><code>matchers</code> submodule contains <code>RegexMatcher</code> and <code>PhraseMatcher</code> classes, which interact with the normalisation</li> <li><code>multiprocessing</code> submodule contains <code>spark</code> and <code>local</code> multiprocessing tools</li> <li><code>connectors</code> contains <code>Brat</code>, <code>OMOP</code> and <code>LabelTool</code> connectors</li> <li><code>utils</code> contains various utilities</li> <li>Add entry points to make pipeline usable directly, removing the need to import <code>edsnlp.components</code>.</li> <li>Add a <code>eds</code> namespace for components: for instance, <code>negation</code> becomes <code>eds.negation</code>. Using the former pipeline name still works, but issues a deprecation warning.</li> <li>Add 3 score pipelines related to emergency</li> <li>Add a helper function to use a spaCy pipeline as a Spark UDF.</li> <li>Fix alignment issues in RegexMatcher</li> <li>Change the alignment procedure, dropping clumsy <code>numpy</code> dependency in favour of <code>bisect</code></li> <li>Change the name of <code>eds.antecedents</code> to <code>eds.history</code>.   Calling <code>eds.antecedents</code> still works, but issues a deprecation warning and support will be removed in a future version.</li> <li>Add a <code>eds.covid</code> component, that identifies mentions of COVID</li> <li>Change the demo, to include NER components</li> </ul>"},{"location":"changelog/#v032","title":"v0.3.2","text":"<ul> <li>Major revamp of the normalisation.</li> <li>The <code>normalizer</code> pipeline now adds atomic components (<code>lowercase</code>, <code>accents</code>, <code>quotes</code>, <code>pollution</code> &amp; <code>endlines</code>) to the processing pipeline, and compiles the results into a new <code>Doc._.normalized</code> extension. The latter is itself a spaCy <code>Doc</code> object, wherein tokens are normalised and pollution tokens are removed altogether. Components that match on the <code>CUSTOM_NORM</code> attribute process the <code>normalized</code> document, and matches are brought back to the original document using a token-wise mapping.</li> <li>Update the <code>RegexMatcher</code> to use the <code>CUSTOM_NORM</code> attribute</li> <li>Add an <code>EDSPhraseMatcher</code>, wrapping spaCy's <code>PhraseMatcher</code> to enable matching on <code>CUSTOM_NORM</code>.</li> <li>Update the <code>matcher</code> and <code>advanced</code> pipelines to enable matching on the <code>CUSTOM_NORM</code> attribute.</li> <li>Add an OMOP connector, to help go back and forth between OMOP-formatted pandas dataframes and spaCy documents.</li> <li>Add a <code>reason</code> pipeline, that extracts the reason for visit.</li> <li>Add an <code>endlines</code> pipeline, that classifies newline characters between spaces and actual ends of line.</li> <li>Add possibility to annotate within entities for qualifiers (<code>negation</code>, <code>hypothesis</code>, etc), ie if the cue is within the entity. Disabled by default.</li> </ul>"},{"location":"changelog/#v031","title":"v0.3.1","text":"<ul> <li>Update <code>dates</code> to remove miscellaneous bugs.</li> <li>Add <code>isort</code> pre-commit hook.</li> <li>Improve performance for <code>negation</code>, <code>hypothesis</code>, <code>antecedents</code>, <code>family</code> and <code>rspeech</code> by using spaCy's <code>filter_spans</code> and our <code>consume_spans</code> methods.</li> <li>Add proposition segmentation to <code>hypothesis</code> and <code>family</code>, enhancing results.</li> </ul>"},{"location":"changelog/#v030","title":"v0.3.0","text":"<ul> <li>Renamed <code>generic</code> to <code>matcher</code>. This is a non-breaking change for the average user, adding the pipeline is still :</li> </ul> <pre><code>nlp.add_pipe(\"matcher\", config=dict(terms=dict(maladie=\"maladie\")))\n</code></pre> <ul> <li>Removed <code>quickumls</code> pipeline. It was untested, unmaintained. Will be added back in a future release.</li> <li>Add <code>score</code> pipeline, and <code>charlson</code>.</li> <li>Add <code>advanced-regex</code> pipeline</li> <li>Corrected bugs in the <code>negation</code> pipeline</li> </ul>"},{"location":"changelog/#v020","title":"v0.2.0","text":"<ul> <li>Add <code>negation</code> pipeline</li> <li>Add <code>family</code> pipeline</li> <li>Add <code>hypothesis</code> pipeline</li> <li>Add <code>antecedents</code> pipeline</li> <li>Add <code>rspeech</code> pipeline</li> <li>Refactor the library :</li> <li>Remove the <code>rules</code> folder</li> <li>Add a <code>pipelines</code> folder, containing one subdirectory per component</li> <li>Every component subdirectory contains a module defining the component, and a module defining a factory, plus any other utilities (eg <code>terms.py</code>)</li> </ul>"},{"location":"changelog/#v010","title":"v0.1.0","text":"<p>First working version. Available pipelines :</p> <ul> <li><code>section</code></li> <li><code>sentences</code></li> <li><code>normalization</code></li> <li><code>pollution</code></li> </ul>"},{"location":"contributing/","title":"Contributing to EDS-NLP","text":"<p>We welcome contributions ! There are many ways to help. For example, you can:</p> <ol> <li>Help us track bugs by filing issues</li> <li>Suggest and help prioritise new functionalities</li> <li>Develop a new pipeline ! Fork the project and propose a new functionality through a pull request</li> <li>Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.</li> </ol>"},{"location":"contributing/#development-installation","title":"Development installation","text":"<p>To be able to run the test suite, run the example notebooks and develop your own pipeline, you should clone the repo and install it locally.</p> <pre><code># Clone the repository and change directory\n$ git clone https://github.com/aphp/edsnlp.git\n---&gt; 100%\n$ cd edsnlp\n\n# Optional: create a virtual environment\n$ python -m venv venv\n$ source venv/bin/activate\n\n# Install the package with common, dev, setup dependencies in editable mode\n$ pip install -e '.[dev,docs,setup]'\n# And build resources\n$ python scripts/conjugate_verbs.py\n</code></pre> <p>To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the <code>pre-commit</code> Python library. To use it, simply install it:</p> <pre><code>$ pre-commit install\n</code></pre> <p>The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong.</p> <p>The hooks only run on staged changes. To force-run it on all files, run:</p> <pre><code>$ pre-commit run --all-files\n---&gt; 100%\ncolor:green All good !\n</code></pre>"},{"location":"contributing/#proposing-a-merge-request","title":"Proposing a merge request","text":"<p>At the very least, your changes should :</p> <ul> <li>Be well-documented ;</li> <li>Pass every tests, and preferably implement its own ;</li> <li>Follow the style guide.</li> </ul>"},{"location":"contributing/#testing-your-code","title":"Testing your code","text":"<p>We use the Pytest test suite.</p> <p>The following command will run the test suite. Writing your own tests is encouraged !</p> <pre><code>python -m pytest\n</code></pre> <p>Testing Cython code</p> <p>Make sure the package is installed in editable mode. Otherwise <code>Pytest</code> won't be able to find the Cython modules.</p> <p>Should your contribution propose a bug fix, we require the bug be thoroughly tested.</p>"},{"location":"contributing/#architecture-of-a-pipeline","title":"Architecture of a pipeline","text":"<p>Pipelines should follow the same pattern :</p> <pre><code>edsnlp/pipelines/&lt;pipeline&gt;\n   |-- &lt;pipeline&gt;.py                # Defines the component logic\n   |-- patterns.py                  # Defines matched patterns\n   |-- factory.py                   # Declares the pipeline to spaCy\n</code></pre>"},{"location":"contributing/#style-guide","title":"Style Guide","text":"<p>We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short :</p> <p>Black reformats entire files in place. It is not configurable.</p> <p>Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use <code>pre-commit</code> to keep our codebase clean.</p> <p>Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be.</p> <p>We use <code>MkDocs</code> for EDS-NLP's documentation. You can checkout the changes you make with:</p> <pre><code># Install the requirements\n$ pip install -e '.[docs]'\n---&gt; 100%\ncolor:green Installation successful\n\n# Run the documentation\n$ mkdocs serve\n</code></pre> <p>Go to <code>localhost:8000</code> to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.</p>"},{"location":"tokenizers/","title":"Tokenizers","text":"<p>In addition to the standard spaCy <code>FrenchLanguage</code> (<code>fr</code>), EDS-NLP offers a new language better fit for French clinical documents: <code>EDSLanguage</code> (<code>eds</code>). Additionally, the <code>EDSLanguage</code> document creation should be around 5-6 times faster than the <code>fr</code> language. The main differences lie in the tokenization process.</p> <p>A comparison of the two tokenization methods is demonstrated below:</p> Example FrenchLanguage EDSLanguage <code>ACR 5</code> [<code>ACR5</code>] [<code>ACR</code>, <code>5</code>] <code>26.5/</code> [<code>26.5/</code>] [<code>26.5</code>, <code>/</code>] <code>\\n \\n CONCLUSION</code> [<code>\\n \\n</code>, <code>CONCLUSION</code>] [<code>\\n</code>, <code>\\n</code>, <code>CONCLUSION</code>] <code>l'art\u00e8re</code> [<code>l'</code>, <code>art\u00e8re</code>] [<code>l'</code>, <code>art\u00e8re</code>] (same) <code>Dr. Pichon</code> [<code>Dr</code>, <code>.</code>, <code>Pichon</code>] [<code>Dr.</code>, <code>Pichon</code>] <code>B.H.HP.A.7.A</code> [<code>B.H.HP.A.7.A</code>] [<code>B.</code>, <code>H.</code>, <code>HP.</code>, <code>A</code>, <code>7</code>, <code>A</code>, <code>0</code>] <p>To instantiate one of the two languages, you can call the <code>spacy.blank</code> method.</p> EDSLanguageFrenchLanguage <pre><code>import spacy\n\nnlp = spacy.blank(\"eds\")\n</code></pre> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n</code></pre>"},{"location":"advanced-tutorials/","title":"Advanced use cases","text":"<p>In this section, we review a few advanced use cases:</p> <ul> <li>Adding pre-computed word vectors to spaCy</li> <li>Deploying your spaCy pipeline as an API</li> <li>Creating your own component</li> </ul>"},{"location":"advanced-tutorials/fastapi/","title":"Deploying as an API","text":"<p>In this section, we will see how you can deploy your pipeline as a REST API using the power of FastAPI.</p>"},{"location":"advanced-tutorials/fastapi/#the-nlp-pipeline","title":"The NLP pipeline","text":"<p>Let's create a simple NLP model, that can:</p> <ul> <li>match synonyms of COVID19</li> <li>check for negation, speculation and reported speech.</li> </ul> <p>You know the drill:</p> pipeline.py<pre><code>import spacy\n\nnlp = spacy.blank('fr')\n\nnlp.add_pipe(\"eds.sentences\")\n\nconfig = dict(\n    regex=dict(\n        covid=[\n            \"covid\",\n            r\"covid[-\\s]?19\",\n            r\"sars[-\\s]?cov[-\\s]?2\",\n            r\"corona[-\\s]?virus\",\n        ],\n    ),\n    attr=\"LOWER\",\n)\nnlp.add_pipe('eds.matcher', config=config)\n\nnlp.add_pipe(\"eds.negation\")\nnlp.add_pipe(\"eds.family\")\nnlp.add_pipe(\"eds.hypothesis\")\nnlp.add_pipe(\"eds.reported_speech\")\n</code></pre>"},{"location":"advanced-tutorials/fastapi/#creating-the-fastapi-app","title":"Creating the FastAPI app","text":"<p>FastAPI is a incredibly efficient framework, based on Python type hints from the ground up, with the help of Pydantic (another great library for building modern Python). We won't go into too much detail about FastAPI in this tutorial. For further information on how the framework operates, go to its excellent documentation!</p> <p>We'll need to create two things:</p> <ol> <li>A module containing the models for inputs and outputs.</li> <li>The script that defines the application itself.</li> </ol> models.py<pre><code>from typing import List\n\nfrom pydantic import BaseModel\n\n\nclass Entity(BaseModel):  # (1)\n\n    # OMOP-style attributes\n    start: int\n    end: int\n    label: str\n    lexical_variant: str\n    normalized_variant: str\n\n    # Qualifiers\n    negated: bool\n    hypothesis: bool\n    family: bool\n    reported_speech: bool\n\n\nclass Document(BaseModel):  # (2)\n    text: str\n    ents: List[Entity]\n</code></pre> <ol> <li>The <code>Entity</code> model contains attributes that define a matched entity, as well as variables that contain the output of the qualifier components.</li> <li>The <code>Document</code> model contains the input text, and a list of detected entities</li> </ol> <p>Having defined the output models and the pipeline, we can move on to creating the application itself:</p> app.py<pre><code>from typing import List\n\nfrom fastapi import FastAPI\n\nfrom pipeline import nlp\nfrom models import Entity, Document\n\n\napp = FastAPI(title=\"EDS-NLP\", version=edsnlp.__version__)\n\n\n@app.post(\"/covid\", response_model=List[Document])  # (1)\nasync def process(\n    notes: List[str],  # (2)\n):\n\n    documents = []\n\n    for doc in nlp.pipe(notes):\n        entities = []\n\n        for ent in doc.ents:\n            entity = Entity(\n                start=ent.start_char,\n                end=ent.end_char,\n                label=ent.label_,\n                lexical_variant=ent.text,\n                normalized_variant=ent._.normalized_variant,\n                negated=ent._.negation,\n                hypothesis=ent._.hypothesis,\n                family=ent._.family,\n                reported_speech=ent._.reported_speech,\n            )\n            entities.append(entity)\n\n        documents.append(\n            Document(\n                text=doc.text,\n                ents=entities,\n            )\n        )\n\n    return documents\n</code></pre> <ol> <li>By telling FastAPI what output format is expected, you get automatic data validation.</li> <li>In FastAPI, input and output schemas are defined through Python type hinting.    Here, we tell FastAPI to expect a list of strings in the <code>POST</code> request body.    As a bonus, you get data validation for free.</li> </ol>"},{"location":"advanced-tutorials/fastapi/#running-the-api","title":"Running the API","text":"<p>Our simple API is ready to launch! We'll just need to install FastAPI along with a ASGI server to run it. This can be done in one go:</p> <pre><code>$ pip install 'fastapi[uvicorn]'\n---&gt; 100%\ncolor:green Successfully installed fastapi\n</code></pre> <p>Launching the API is trivial:</p> <pre><code>$ uvicorn app:app --reload\n</code></pre> <p>Go to <code>localhost:8000/docs</code> to admire the automatically generated documentation!</p>"},{"location":"advanced-tutorials/fastapi/#using-the-api","title":"Using the API","text":"<p>You can try the API directly from the documentation. Otherwise, you may use the <code>requests</code> package:</p> <pre><code>import requests\n\nnotes = [\n    \"Le p\u00e8re du patient n'est pas atteint de la covid.\",\n    \"Probable coronavirus.\",\n]\n\nr = requests.post(\n    \"http://localhost:8000/covid\",\n    json=notes,\n)\n\nr.json()\n</code></pre> <p>You should get something like:</p> <pre><code>[\n{\n\"text\": \"Le p\u00e8re du patient n'est pas atteint de la covid.\",\n\"ents\": [\n{\n\"start\": 43,\n\"end\": 48,\n\"label\": \"covid\",\n\"lexical_variant\": \"covid\",\n\"normalized_variant\": \"covid\",\n\"negated\": true,\n\"hypothesis\": false,\n\"family\": true,\n\"reported_speech\": false\n}\n]\n},\n{\n\"text\": \"Probable coronavirus.\",\n\"ents\": [\n{\n\"start\": 9,\n\"end\": 20,\n\"label\": \"covid\",\n\"lexical_variant\": \"coronavirus\",\n\"normalized_variant\": \"coronavirus\",\n\"negated\": false,\n\"hypothesis\": true,\n\"family\": false,\n\"reported_speech\": false\n}\n]\n}\n]\n</code></pre>"},{"location":"advanced-tutorials/word-vectors/","title":"Word embeddings","text":"<p>The only ready-to-use components in EDS-NLP are rule-based components. However, that does not prohibit you from exploiting spaCy's machine learning capabilities! You can mix and match machine learning pipelines, trainable or not, with EDS-NLP rule-based components.</p> <p>In this tutorial, we will explore how you can use static word vectors trained with Gensim within spaCy.</p> <p>Training the word embedding, however, is outside the scope of this post. You'll find very well designed resources on the subject in Gensim's documenation.</p> <p>Using Transformer models</p> <p>spaCy v3 introduced support for Transformer models through their helper library <code>spacy-transformers</code> that interfaces with HuggingFace's <code>transformers</code> library.</p> <p>Using transformer models can significantly increase your model's performance.</p>"},{"location":"advanced-tutorials/word-vectors/#adding-pre-trained-word-vectors","title":"Adding pre-trained word vectors","text":"<p>spaCy provides a <code>init vectors</code> CLI utility that takes a Gensim-trained binary and transforms it to a spaCy-readable pipeline.</p> <p>Using it is straightforward :</p> <pre><code>$ spacy init vectors fr /path/to/vectors /path/to/pipeline\n---&gt; 100%\ncolor:green Conversion successful!\n</code></pre> <p>See the documentation for implementation details.</p>"},{"location":"pipelines/","title":"Pipelines overview","text":"<p>EDS-NLP provides easy-to-use spaCy components.</p> CoreQualifiersMiscellaneousNERTrainable Pipeline Description <code>eds.normalizer</code> Non-destructive input text normalisation <code>eds.sentences</code> Better sentence boundary detection <code>eds.matcher</code> A simple yet powerful entity extractor <code>eds.terminology</code> A simple yet powerful terminology matcher <code>eds.contextual-matcher</code> A conditional entity extractor <code>eds.endlines</code> An unsupervised model to classify each end line Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based medical history detection Pipeline Description <code>eds.dates</code> Date extraction and normalisation <code>eds.consultation_dates</code> Identify consultation dates <code>eds.measurements</code> Measure extraction and normalisation <code>eds.sections</code> Section detection <code>eds.reason</code> Rule-based hospitalisation reason detection <code>eds.tables</code> Tables detection Pipeline Description <code>eds.covid</code> A COVID mentions detector <code>eds.charlson</code> A Charlson score extractor <code>eds.elstonellis</code> An Elston &amp; Ellis code extractor <code>eds.emergency.priority</code> A priority score extractor <code>eds.emergency.ccmu</code> A CCMU score extractor <code>eds.emergency.gemsa</code> A GEMSA score extractor <code>eds.sofa</code> A SOFA score extractor <code>eds.TNM</code> A TNM score extractor <code>eds.adicap</code> A ADICAP codes extractor <code>eds.drugs</code> A drug mentions extractor <code>eds.cim10</code> A CIM10 terminology matcher <code>eds.umls</code> An UMLS terminology matcher Pipeline Description <code>eds.nested-ner</code> A nested NER trainable model <p>You can add them to your spaCy pipeline by simply calling <code>add_pipe</code>, for instance:</p> <pre><code># \u2191 Omitted code that defines the nlp object \u2191\nnlp.add_pipe(\"eds.normalizer\")\n</code></pre>"},{"location":"pipelines/architecture/","title":"Basic Architecture","text":"<p>Most pipelines provided by EDS-NLP aim to qualify pre-extracted entities. To wit, the basic usage of the library:</p> <ol> <li>Implement a normaliser (see <code>normalizer</code>)</li> <li>Add an entity recognition component (eg the simple but powerful <code>matcher</code> pipeline)</li> <li>Add zero or more entity qualification components, such as <code>negation</code>, <code>family</code> or <code>hypothesis</code>. These qualifiers typically help detect false-positives.</li> </ol>"},{"location":"pipelines/architecture/#scope","title":"Scope","text":"<p>Since the basic usage of EDS-NLP components is to qualify entities, most pipelines can function in two modes:</p> <ol> <li>Annotation of the extracted entities (this is the default). To increase throughput, only pre-extracted entities (found in <code>doc.ents</code>) are processed.</li> <li>Full-text, token-wise annotation. This mode is activated by setting the <code>on_ents_only</code> parameter to <code>False</code>.</li> </ol> <p>The possibility to do full-text annotation implies that one could use the pipelines the other way around, eg detecting all negations once and for all in an ETL phase, and reusing the results consequently. However, this is not the intended use of the library, which aims to help researchers downstream as a standalone application.</p>"},{"location":"pipelines/architecture/#result-persistence","title":"Result persistence","text":"<p>Depending on their purpose (entity extraction, qualification, etc), EDS-NLP pipelines write their results to <code>Doc.ents</code>, <code>Doc.spans</code> or in a custom attribute.</p>"},{"location":"pipelines/architecture/#extraction-pipelines","title":"Extraction pipelines","text":"<p>Extraction pipelines (matchers, the date detector or NER pipelines, for instance) keep their results to the <code>Doc.ents</code> attribute directly.</p> <p>Note that spaCy prohibits overlapping entities within the <code>Doc.ents</code> attribute. To circumvent this limitation, we filter spans, and keep all discarded entities within the <code>discarded</code> key of the <code>Doc.spans</code> attribute.</p> <p>Some pipelines write their output to the <code>Doc.spans</code> dictionary. We enforce the following doctrine:</p> <ul> <li>Should the pipe extract entities that are directly informative (typically the output of the <code>eds.matcher</code> component), said entities are stashed in the <code>Doc.ents</code> attribute.</li> <li>On the other hand, should the entity be useful to another pipe, but less so in itself (eg the output of the <code>eds.sections</code> or <code>eds.dates</code> component), it will be stashed in a specific key within the <code>Doc.spans</code> attribute.</li> </ul>"},{"location":"pipelines/architecture/#entity-tagging","title":"Entity tagging","text":"<p>Moreover, most pipelines declare spaCy extensions, on the <code>Doc</code>, <code>Span</code> and/or <code>Token</code> objects.</p> <p>These extensions are especially useful for qualifier pipelines, but can also be used by other pipelines to persist relevant information. For instance, the <code>eds.dates</code> pipeline:</p> <ol> <li>Populates <code>Doc.spans[\"dates\"]</code></li> <li>For each detected item, keeps the normalised date in <code>Span._.date</code></li> </ol>"},{"location":"pipelines/core/","title":"Core Pipelines","text":"<p>This section deals with \"core\" functionalities offered by EDS-NLP:</p> <ul> <li>Matching a terminology</li> <li>Normalising a text</li> <li>Detecting sentence boundaries</li> </ul>"},{"location":"pipelines/core/contextual-matcher/","title":"Contextual Matcher","text":"<p>During feature extraction, it may be necessary to search for additional patterns in their neighborhood, namely:</p> <ul> <li>patterns to discard irrelevant entities</li> <li>patterns to enrich these entities and store some information</li> </ul> <p>For example, to extract mentions of non-benign cancers, we need to discard all extractions that mention \"benin\" in their immediate neighborhood. Although such a filtering is feasible using a regular expression, it essentially requires modifying each of the regular expressions.</p> <p>The ContextualMatcher allows to perform this extraction in a clear and concise way.</p>"},{"location":"pipelines/core/contextual-matcher/#the-configuration-file","title":"The configuration file","text":"<p>The whole ContextualMatcher pipeline is basically defined as a list of pattern dictionaries. Let us see step by step how to build such a list using the example stated just above.</p>"},{"location":"pipelines/core/contextual-matcher/#a-finding-mentions-of-cancer","title":"a. Finding mentions of cancer","text":"<p>To do this, we can build either a set of <code>terms</code> or a set of <code>regex</code>. <code>terms</code> will be used to search for exact matches in the text. While less flexible, it is faster than using regex. In our case we could use the following lists (which are of course absolutely not exhaustives):</p> <pre><code>terms = [\n    \"cancer\",\n    \"tumeur\",\n]\n\nregex = [\n    \"adeno(carcinom|[\\s-]?k)\",\n    \"neoplas\",\n    \"melanom\",\n]\n</code></pre> <p>Maybe we want to exclude mentions of benign cancers:</p> <pre><code>benign = \"benign|benin\"\n</code></pre>"},{"location":"pipelines/core/contextual-matcher/#b-find-mention-of-a-stage-and-extract-its-value","title":"b. Find mention of a stage and extract its value","text":"<p>For this we will forge a RegEx with one capturing group (basically a pattern enclosed in parentheses):</p> <pre><code>stage = \"stade (I{1,3}V?|[1234])\"\n</code></pre> <p>This will extract stage between 1 and 4</p> <p>We can add a second regex to try to capture if the cancer is in a metastasis stage or not:</p> <pre><code>metastase = \"(metasta)\"\n</code></pre>"},{"location":"pipelines/core/contextual-matcher/#c-the-complete-configuration","title":"c. The complete configuration","text":"<p>We can now put everything together:</p> <pre><code>cancer = dict(\n    source=\"Cancer solide\",\n    regex=regex,\n    terms=terms,\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=benign,\n        window=3,\n    ),\n    assign=[\n        dict(\n            name=\"stage\",\n            regex=stage,\n            window=(-10,10),\n            replace_entity=True,\n            reduce_mode=None,\n        ),\n        dict(\n            name=\"metastase\",\n            regex=metastase,\n            window=10,\n            replace_entity=False,\n            reduce_mode=\"keep_last\",\n        ),\n    ]\n)\n</code></pre> <p>Here the configuration consists of a single dictionary. We might want to also include lymphoma in the matcher:</p> <pre><code>lymphome = dict(\n    source=\"Lymphome\",\n    regex=[\"lymphom\", \"lymphangio\"],\n    regex_attr=\"NORM\",\n    exclude=dict(\n        regex=[\"hodgkin\"],  # (1)\n        window=3,\n    ),\n)\n</code></pre> <ol> <li>We are excluding \"Lymphome de Hodgkin\" here</li> </ol> <p>In this case, the configuration can be concatenated in a list:</p> <pre><code>patterns = [cancer, lymphome]\n</code></pre>"},{"location":"pipelines/core/contextual-matcher/#available-parameters-for-more-flexibility","title":"Available parameters for more flexibility","text":"<p>3 main parameters can be used to refine how entities will be formed</p>"},{"location":"pipelines/core/contextual-matcher/#the-include_assigned-parameter","title":"The <code>include_assigned</code> parameter","text":"<p>Following the previous example, you might want your extracted entities to include, if found, the cancer stage and the metastasis status. This can be achieved by setting <code>include_assigned=True</code> in the pipe configuration.</p> <p>For instance, from the sentence \"Le patient a un cancer au stade 3\", the extracted entity will be:</p> <ul> <li>\"cancer\" if <code>include_assigned=False</code></li> <li>\"cancer au stade 3\" if <code>include_assigned=True</code></li> </ul>"},{"location":"pipelines/core/contextual-matcher/#the-reduce_mode-parameter","title":"The <code>reduce_mode</code> parameter","text":"<p>It may happen that an assignment matches more than once. For instance, in the (nonsensical) sentence \"Le patient a un cancer au stade 3 et au stade 4\", both \"stade 3\" and \"stade 4\" will be matched by the <code>stage</code> assign key. Depending on your use case, you may want to keep all the extractions, or just one.</p> <ul> <li>If <code>reduce_mode=None</code> (default), all extractions are kept in a list</li> <li>If <code>reduce_mode=\"keep_first\"</code>, only the extraction closest to the main matched entity will be kept (in this case, it would be \"stade 3\" since it is the closest to \"cancer\")</li> <li>If <code>reduce_mode==\"keep_last\"</code>, only the furthest extraction is kept.</li> </ul>"},{"location":"pipelines/core/contextual-matcher/#the-replace_entity-parameter","title":"The <code>replace_entity</code> parameter","text":"<p>This parameter can be se to <code>True</code> only for a single assign key per dictionary. This limitation comes from the purpose of this parameter: If set to <code>True</code>, the corresponding <code>assign</code> key will be returned as the entity, instead of the match itself. For clarity, let's take the same sentence \"Le patient a un cancer au stade 3\" as an example:</p> <ul> <li>if <code>replace_entity=True</code> in the <code>stage</code> assign key, then the extracted entity will be \"stade 3\" instead of \"cancer\"</li> <li>if <code>replace_entity=False</code> for every assign key, the returned entity will be, as expected, \"cancer\"</li> </ul> <p>Please notice that with <code>replace_entity</code> set to True, if the correponding assign key matches nothing, the entity will be discarded.</p>"},{"location":"pipelines/core/contextual-matcher/#usage","title":"Usage","text":"<pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n\nnlp.add_pipe(\"sentences\")\nnlp.add_pipe(\"normalizer\")\n\nnlp.add_pipe(\n    \"eds.contextual-matcher\",\n    name=\"Cancer\",\n    config=dict(\n        patterns=patterns,\n    ),\n)\n</code></pre> <p>Let us see what we can get from this pipeline with a few examples</p> Simple matchExclusion ruleExtracting additional infos <pre><code>txt = \"Le patient a eu un cancer il y a 5 ans\"\ndoc = nlp(txt)\nent = doc.ents[0]\n\nent.label_\n# Out: Cancer\n\nent._.source\n# Out: Cancer solide\n\nent.text, ent.start, ent.end\n# Out: ('cancer', 5, 6)\n</code></pre> <p>Let us check that when a benign mention is present, the extraction is excluded:</p> <pre><code>txt = \"Le patient a eu un cancer relativement b\u00e9nin il y a 5 ans\"\ndoc = nlp(txt)\n\ndoc.ents\n# Out: ()\n</code></pre> <p>All informations extracted from the provided <code>assign</code> configuration can be found in the <code>assigned</code> attribute under the form of a dictionary:</p> <pre><code>txt = \"Le patient a eu un cancer de stade 3.\"\ndoc = nlp(txt)\n\ndoc.ents[0]._.assigned\n# Out: {'stage': '3'}\n</code></pre>"},{"location":"pipelines/core/contextual-matcher/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> </p> <p>However, most of the configuration is provided in the <code>patterns</code> key, as a pattern dictionary or a list of pattern dictionaries</p>"},{"location":"pipelines/core/contextual-matcher/#the-pattern-dictionary","title":"The pattern dictionary","text":""},{"location":"pipelines/core/contextual-matcher/#description","title":"Description","text":"<p>A patterr is a nested dictionary with the following keys:</p> <code>source</code><code>regex</code><code>regex_attr</code><code>terms</code><code>exclude</code><code>assign</code> <p>A label describing the pattern</p> <p>A single Regex or a list of Regexes</p> <p>An attributes to overwrite the given <code>attr</code> when matching with Regexes.</p> <p>A single term or a list of terms (for exact matches)</p> <p>A dictionary (or list of dictionaries) to define exclusion rules. Exclusion rules are given as Regexes, and if a match is found in the surrounding context of an extraction, the extraction is removed. Each dictionary should have the following keys:</p> <code>window</code><code>regex</code> <p>Size of the context to use (in number of words). You can provide the window as:</p> <ul> <li>A positive integer, in this case the used context will be taken after the extraction</li> <li>A negative integer, in this case the used context will be taken before the extraction</li> <li>A tuple of integers <code>(start, end)</code>, in this case the used context will be the snippet from <code>start</code> tokens before the extraction to <code>end</code> tokens after the extraction</li> </ul> <p>A single Regex or a list of Regexes.</p> <p>A dictionary to refine the extraction. Similarily to the <code>exclude</code> key, you can provide a dictionary to use on the context before and after the extraction.</p> <code>name</code><code>window</code><code>regex</code><code>replace_entity</code><code>reduce_mode</code> <p>A name (string)</p> <p>Size of the context to use (in number of words). You can provide the window as:</p> <ul> <li>A positive integer, in this case the used context will be taken after the extraction</li> <li>A negative integer, in this case the used context will be taken before the extraction</li> <li>A tuple of integers <code>(start, end)</code>, in this case the used context will be the snippet from <code>start</code> tokens before the extraction to <code>end</code> tokens after the extraction</li> </ul> <p>A dictionary where keys are labels and values are Regexes with a single capturing group</p> <p>If set to <code>True</code>, the match from the corresponding assign key will be used as entity, instead of the main match. See this paragraph</p> <p>Set how multiple assign matches are handled. See the documentation of the <code>reduce_mode</code> parameter</p>"},{"location":"pipelines/core/contextual-matcher/#a-full-pattern-dictionary-example","title":"A full pattern dictionary example","text":"<pre><code>dict(\n    source=\"AVC\",\n    regex=[\n        \"accidents? vasculaires? cerebr\",\n    ],\n    terms=\"avc\",\n    regex_attr=\"NORM\",\n    exclude=[\n        dict(\n            regex=[\"service\"],\n            window=3,\n        ),\n        dict(\n            regex=[\" a \"],\n            window=-2,\n        ),\n    ],\n    assign=[\n        dict(\n            name=\"neo\",\n            regex=r\"(neonatal)\",\n            expand_entity=True,\n            window=3,\n        ),\n        dict(\n            name=\"trans\",\n            regex=\"(transitoire)\",\n            expand_entity=True,\n            window=3,\n        ),\n        dict(\n            name=\"hemo\",\n            regex=r\"(hemorragique)\",\n            expand_entity=True,\n            window=3,\n        ),\n        dict(\n            name=\"risk\",\n            regex=r\"(risque)\",\n            expand_entity=False,\n            window=-3,\n        ),\n    ]\n)\n</code></pre>"},{"location":"pipelines/core/contextual-matcher/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/core/endlines/","title":"Endlines","text":"<p>The <code>eds.endlines</code> pipeline classifies newline characters as actual end of lines or mere spaces. In the latter case, the token is removed from the normalised document.</p> <p>Behind the scenes, it uses a <code>endlinesmodel</code> instance, which is an unsupervised algorithm based on the work of Zweigenbaum et al1.</p>"},{"location":"pipelines/core/endlines/#usage","title":"Usage","text":"<p>The following example shows a simple usage.</p>"},{"location":"pipelines/core/endlines/#training","title":"Training","text":"<pre><code>import spacy\nfrom edsnlp.pipelines.core.endlines.endlinesmodel import EndLinesModel\n\nnlp = spacy.blank(\"fr\")\n\ntexts = [\n\"\"\"Le patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010;\nFumeur, il est arret\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diabete\n\n\"\"\",\n\"\"\"J'aime le \\nfromage...\\n\"\"\",\n]\n\ndocs = list(nlp.pipe(texts))\n\n# Train and predict an EndLinesModel\nendlines = EndLinesModel(nlp=nlp)\n\ndf = endlines.fit_and_predict(docs)\ndf.head()\n\nPATH = \"/tmp/path_to_save\"\nendlines.save(PATH)\n</code></pre>"},{"location":"pipelines/core/endlines/#inference","title":"Inference","text":"<pre><code>import spacy\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = spacy.blank(\"fr\")\n\nPATH = \"/tmp/path_to_save\"\nnlp.add_pipe(\"eds.endlines\", config=dict(model_path=PATH))\n\ndocs = list(nlp.pipe(texts))\n\ndoc_exemple = docs[1]\n\ndoc_exemple.ents = tuple(\n    Span(doc_exemple, token.i, token.i + 1, \"excluded\")\n    for token in doc_exemple\n    if token.tag_ == \"EXCLUDED\"\n)\n\ndisplacy.render(doc_exemple, style=\"ent\", options={\"colors\": {\"space\": \"red\"}})\n</code></pre>"},{"location":"pipelines/core/endlines/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>model_path</code> Path to the pre-trained pipeline Required"},{"location":"pipelines/core/endlines/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.endlines</code> pipeline declares one spaCy extensions, on both <code>Span</code> and <code>Token</code> objects. The <code>end_line</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the new line is an end line character. Otherwise, it is set to <code>False</code> if the new line is classified as a space.</p> <p>The pipeline also sets the <code>excluded</code> custom attribute on newlines that are classified as spaces. It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p>"},{"location":"pipelines/core/endlines/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.endlines</code> pipeline was developed by AP-HP's Data Science team based on the work of Zweigenbaum et al1.</p> <ol> <li> <p>Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters), 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"pipelines/core/matcher/","title":"Matcher","text":"<p>EDS-NLP simplifies the matching process by exposing a <code>eds.matcher</code> pipeline that can match on terms or regular expressions.</p>"},{"location":"pipelines/core/matcher/#usage","title":"Usage","text":"<p>Let us redefine the pipeline :</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    patient=\"patient\",  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n        term_matcher=\"exact\",\n        term_matcher_config={},\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"pipelines/core/matcher/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>'TEXT'</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>GenericTermMatcher</code> DEFAULT: <code>GenericTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the label of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"pipelines/core/matcher/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.matcher</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/core/normalisation/","title":"Normalisation","text":"<p>The normalisation scheme used by EDS-NLP adheres to the non-destructive doctrine. In other words,</p> <pre><code>nlp(text).text == text\n</code></pre> <p>is always true.</p> <p>To achieve this, the input text is never modified. Instead, our normalisation strategy focuses on two axes:</p> <ol> <li>Only the <code>NORM</code> and <code>tag_</code> attributes are modified by the <code>normalizer</code> pipeline ;</li> <li>Pipelines (eg the <code>pollution</code> pipeline) can mark tokens as excluded by setting the extension <code>Token.tag_</code> to <code>EXCLUDED</code> or as space by setting the extension <code>Token.tag_</code> to <code>SPACE</code>.    It enables downstream matchers to skip excluded tokens.</li> </ol> <p>The normaliser can act on the input text in five dimensions :</p> <ol> <li>Move the text to lowercase.</li> <li>Remove accents. We use a deterministic approach to avoid modifying the character-length of the text, which helps for RegEx matching.</li> <li>Normalize apostrophes and quotation marks, which are often coded using special characters.</li> <li>Detect spaces and new lines and mark them as such (to be skipped later)</li> <li>Detect tokens in pollutions patterns and mark them as such (to be skipped later)</li> </ol> <p>Note</p> <p>We recommend you also add an end-of-line classifier to remove excess new line characters (introduced by the PDF layout).</p> <p>We provide a <code>endlines</code> pipeline, which requires training an unsupervised model. Refer to the dedicated page for more information.</p>"},{"location":"pipelines/core/normalisation/#usage","title":"Usage","text":"<p>The normalisation is handled by the single <code>eds.normalizer</code> pipeline. The following code snippet is complete, and should run as is.</p> <pre><code>import spacy\nfrom edsnlp.matchers.utils import get_text\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\")\n\n# Notice the special character used for the apostrophe and the quotes\ntext = \"Le patient est admis \u00e0 l'h\u00f4pital le 23 ao\u00fbt 2021 pour une douleur \u02baaffreuse\u201d \u00e0 l`estomac.\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: le patient est admis a l'hopital le 23 aout 2021 pour une douleur \"affreuse\" a l'estomac.\n</code></pre>"},{"location":"pipelines/core/normalisation/#utilities","title":"Utilities","text":"<p>To simplify the use of the normalisation output, we provide the <code>get_text</code> utility function. It computes the textual representation for a <code>Span</code> or <code>Doc</code> object.</p> <p>Moreover, every span exposes a <code>normalized_variant</code> extension getter, which computes the normalised representation of an entity on the fly.</p>"},{"location":"pipelines/core/normalisation/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>accents</code> <p><code>Accents</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>quotes</code> <p><code>Quotes</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>spaces</code> <p><code>Spaces</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> configuration object.</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p>"},{"location":"pipelines/core/normalisation/#pipelines","title":"Pipelines","text":"<p>Let's review each subcomponent.</p>"},{"location":"pipelines/core/normalisation/#lowercase","title":"Lowercase","text":"<p>The <code>eds.lowercase</code> pipeline transforms every token to lowercase. It is not configurable.</p> <p>Consider the following example :</p> <pre><code>import spacy\nfrom edsnlp.matchers.utils import get_text\n\nconfig = dict(\n    lowercase=True,\n    accents=False,\n    quotes=False,\n    spaces=False,\n    pollution=False,\n)\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\", config=config)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: pneumopathie \u00e0 nbnbwbwbnbwbnbnbnbwbw 'coronavirus'\n</code></pre>"},{"location":"pipelines/core/normalisation/#accents","title":"Accents","text":"<p>The <code>eds.accents</code> pipeline removes accents. To avoid edge cases, the component uses a specified list of accentuated characters and their unaccented representation, making it more predictable than using a library such as <code>unidecode</code>.</p> <p>Consider the following example :</p> <pre><code>import spacy\nfrom edsnlp.matchers.utils import get_text\n\nconfig = dict(\n    lowercase=False,\n    accents=True,\n    quotes=False,\n    spaces=False,\n    pollution=False,\n)\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\", config=config)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'\n</code></pre>"},{"location":"pipelines/core/normalisation/#apostrophes-and-quotation-marks","title":"Apostrophes and quotation marks","text":"<p>Apostrophes and quotation marks can be encoded using unpredictable special characters. The <code>eds.quotes</code> component transforms every such special character to <code>'</code> and <code>\"</code>, respectively.</p> <p>Consider the following example :</p> <pre><code>import spacy\nfrom edsnlp.matchers.utils import get_text\n\nconfig = dict(\n    lowercase=False,\n    accents=False,\n    quotes=True,\n    spaces=False,\n    pollution=False,\n)\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\", config=config)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW 'coronavirus'\n</code></pre>"},{"location":"pipelines/core/normalisation/#spaces","title":"Spaces","text":"<p>This is not truly a normalisation component, but this allows us to detect spaces tokens ahead of the other components and encode it as using the <code>tag_</code> attribute for fast matching.</p> <p>Tip</p> <p>This component and its <code>spaces</code> option should be enabled if you ever set   <code>ignore_space_tokens</code> parameter token to True in a downstream component.</p> <pre><code>import spacy\n\nconfig = dict(\n    lowercase=False,\n    accents=False,\n    quotes=False,\n    spaces=True,\n    pollution=False,\n)\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.normalizer\", config=config)\n\ndoc = nlp(\"Phrase    avec des espaces \\n et un retour \u00e0 la ligne\")\n[t.tag_ for t in doc]\n# Out: ['', 'SPACE', '', '', '', 'SPACE', '', '', '', '', '', '']\n</code></pre>"},{"location":"pipelines/core/normalisation/#pollution","title":"Pollution","text":"<p>The pollution pipeline uses a set of regular expressions to detect pollutions (irrelevant non-medical text that hinders text processing). Corresponding tokens are marked as excluded (by setting <code>Token._.excluded</code> to <code>True</code>), enabling the use of the phrase matcher.</p> <p>Consider the following example :</p> <pre><code>import spacy\nfrom edsnlp.matchers.utils import get_text\n\nconfig = dict(\n    lowercase=False,\n    accents=True,\n    quotes=False,\n    spaces=False,\n    pollution=True,\n)\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\", config=config)\n\ntext = \"Pneumopathie \u00e0 NBNbWbWbNbWbNBNbNbWbW `coronavirus'\"\n\ndoc = nlp(text)\n\nget_text(doc, attr=\"NORM\", ignore_excluded=False)\n# Out: Pneumopathie a NBNbWbWbNbWbNBNbNbWbW `coronavirus'\n\nget_text(doc, attr=\"TEXT\", ignore_excluded=True)\n# Out: Pneumopathie \u00e0 `coronavirus'\n</code></pre> <p>This example above shows that the normalisation scheme works on two axes: non-destructive text modification and exclusion of tokens. The two are independent: a matcher can use the <code>NORM</code> attribute but keep excluded tokens, and conversely, match on <code>TEXT</code> while ignoring excluded tokens.</p> <p></p>"},{"location":"pipelines/core/normalisation/#types-of-pollution","title":"Types of pollution","text":"<p>Pollution can come in various forms in clinical texts. We provide a small set of possible pollutions patterns that can be enabled or disabled as needed.</p> <p>For instance, if we consider biology tables as pollution, we only need to instantiate the <code>normalizer</code> pipe as follows:</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\n    \"eds.normalizer\",\n    config=dict(\n        pollution=dict(\n            biology=True,\n        ),\n    ),\n)\n</code></pre> Type Description Example Included by default <code>information</code> Footnote present in a lot of notes, providing information to the patient about the use of its data \"L'AP-HP collecte vos donn\u00e9es administratives \u00e0 des fins ...\" <code>True</code> <code>bars</code> Barcodes wrongly parsed as text \"...NBNbWbWbNbWbNBNbNbWbW...\" <code>True</code> <code>biology</code> Parsed biology results table. It often contains disease names that often leads to false positives with NER pipelines. \"...\u00a6UI/L \u00a620 \u00a6 \u00a6 \u00a620-70 Polyarthrite rhumato\u00efde Facteur rhumatoide \u00a6UI/mL \u00a6 \u00a6&lt;10 \u00a6 \u00a6 \u00a6 \u00a60-14...\" <code>False</code> <code>doctors</code> List of doctor names and specialities, often found in left-side note margins. Also source of potential false positives. \"... Dr ABC - Diab\u00e8te/Endocrino ...\" <code>True</code> <code>web</code> Webpages URL and email adresses. Also source of potential false positives. \"... www.vascularites.fr ...\" <code>True</code> <code>coding</code> Subsection containing ICD-10 codes along with their description. Also source of potential false positives. \"... (2) E112 + Oeil (2) E113 + Neuro (2) E114 D\u00e9mence (2) F03 MA (2) F001+G301 DCL G22+G301 Vasc (2) ...\" <code>False</code> <code>footer</code> Footer of new page \"2/2Pat : NOM Prenom le 2020/01/01 IPP 12345678 Intitul\u00e9 RCP : Urologie HMN le \" <code>True</code>"},{"location":"pipelines/core/normalisation/#custom-pollution","title":"Custom pollution","text":"<p>If you want to exclude specific patterns, you can provide them as a RegEx (or a list of Regexes). For instance, to consider text between \"AAA\" and \"ZZZ\" as pollution you might use:</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\n    \"eds.normalizer\",\n    config=dict(\n        pollution=dict(\n            custom_pollution=r\"AAA.*ZZZ\",\n        ),\n    ),\n)\n</code></pre>"},{"location":"pipelines/core/normalisation/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.normalizer</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/core/sentences/","title":"Sentences","text":"<p>The <code>eds.sentences</code> pipeline provides an alternative to spaCy's default <code>sentencizer</code>, aiming to overcome some of its limitations.</p> <p>Indeed, the <code>sentencizer</code> merely looks at period characters to detect the end of a sentence, a strategy that often fails in a clinical note settings. Our <code>sentences</code> component also classifies end-of-lines as sentence boundaries if the subsequent token begins with an uppercase character, leading to slightly better performances.</p> <p>Moreover, the <code>eds.sentences</code> pipeline can use the output of the <code>eds.normalizer</code> pipeline, and more specifically the end-of-line classification. This is activated by default.</p>"},{"location":"pipelines/core/sentences/#usage","title":"Usage","text":"EDS-NLPspaCy sentencizer <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\\n\"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n)\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out:  &lt;\\s&gt;\n# Out: &lt;s&gt; Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"sentencizer\")\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\\n\"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans.\"\n)\n\ndoc = nlp(text)\n\nfor sentence in doc.sents:\n    print(\"&lt;s&gt;\", sentence, \"&lt;/s&gt;\")\n# Out: &lt;s&gt; Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac\n# Out: Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a deux ans. &lt;\\s&gt;\n</code></pre> <p>Notice how EDS-NLP's implementation is more robust to ill-defined sentence endings.</p>"},{"location":"pipelines/core/sentences/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>punct_chars</code> Punctuation patterns <code>None</code> (use pre-defined patterns) <code>use_endlines</code> Whether to use endlines prediction (see documentation) <code>True</code>"},{"location":"pipelines/core/sentences/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sentences</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/core/terminology/","title":"Terminology","text":"<p>EDS-NLP simplifies the terminology matching process by exposing a <code>eds.terminology</code> pipeline that can match on terms or regular expressions.</p> <p>The terminology matcher is very similar to the generic matcher, although the use case differs slightly. The generic matcher is designed to extract any entity, while the terminology matcher is specifically tailored towards high volume terminologies.</p> <p>There are some key differences:</p> <ol> <li>It labels every matched entity to the same value, provided to the pipeline</li> <li>The keys provided in the <code>regex</code> and <code>terms</code> dictionaries are used as the <code>kb_id_</code> of the entity,    which handles fine-grained labelling</li> </ol> <p>For instance, a terminology matcher could detect every drug mention under the top-level label <code>drug</code>, and link each individual mention to a given drug through its <code>kb_id_</code> attribute.</p>"},{"location":"pipelines/core/terminology/#usage","title":"Usage","text":"<p>Let us redefine the pipeline :</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],  # (1)\n    flu=[\"grippe saisonni\u00e8re\"],  # (2)\n)\n\nregex = dict(\n    covid=r\"coronavirus|covid[-\\s]?19|sars[-\\s]cov[-\\s]2\",  # (3)\n)\n\nnlp.add_pipe(\n    \"eds.terminology\",\n    config=dict(\n        label=\"disease\",\n        terms=terms,\n        regex=regex,\n        attr=\"LOWER\",\n    ),\n)\n</code></pre> <ol> <li>Every key in the <code>terms</code> dictionary is mapped to a concept.</li> <li>The <code>eds.matcher</code> pipeline expects a list of expressions, or a single expression.</li> <li>We can also define regular expression patterns.</li> </ol> <p>This snippet is complete, and should run as is.</p>"},{"location":"pipelines/core/terminology/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>label</code> <p>Top-level label</p> <p> TYPE: <code>str</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <p>Patterns, be they <code>terms</code> or <code>regex</code>, are defined as dictionaries where keys become the <code>kb_id_</code> of the extracted entities. Dictionary values are a either a single expression or a list of expressions that match the concept (see example).</p>"},{"location":"pipelines/core/terminology/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.terminology</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/misc/","title":"Miscellaneous","text":"<p>This section regroups pipelines that extract information that can be used by other components, but have little medical value in itself.</p> <p>For instance, the date detection and normalisation pipeline falls in this category.</p>"},{"location":"pipelines/misc/consultation-dates/","title":"Consultation Dates","text":"<p>This pipeline consists of two main parts:</p> <ul> <li>A matcher which finds mentions of consultation events (more details below)</li> <li>A date parser (see the corresponding pipeline) that links a date to those events</li> </ul>"},{"location":"pipelines/misc/consultation-dates/#usage","title":"Usage","text":"<p>Note</p> <p>It is designed to work ONLY on consultation notes (<code>CR-CONS</code>), so please filter accordingly before proceeding.</p> <pre><code>import spacy\n\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\n    \"eds.normalizer\",\n    config=dict(\n        lowercase=True,\n        accents=True,\n        quotes=True,\n        pollution=False,\n    ),\n)\nnlp.add_pipe(\"eds.consultation_dates\")\n\ntext = \"XXX \\n\" \"Objet : Compte-Rendu de Consultation du 03/10/2018. \\n\" \"XXX \"\n\ndoc = nlp(text)\n\ndoc.spans[\"consultation_dates\"]\n# Out: [Consultation du 03/10/2018]\n\ndoc.spans[\"consultation_dates\"][0]._.consultation_date.to_datetime()\n# Out: DateTime(2018, 10, 3, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))\n</code></pre>"},{"location":"pipelines/misc/consultation-dates/#consultation-events","title":"Consultation events","text":"<p>Three main families of terms are available by default to extract those events.</p>"},{"location":"pipelines/misc/consultation-dates/#the-consultation_mention-terms","title":"The <code>consultation_mention</code> terms","text":"<p>This list contains terms directly referring to consultations, such as \"Consultation du...\" or \"Compte rendu du...\". This list is the only one activated by default since it is fairly precise an not error-prone.</p>"},{"location":"pipelines/misc/consultation-dates/#the-town_mention-terms","title":"The <code>town_mention</code> terms","text":"<p>This list contains the towns of each AP-HP's hospital. Its goal is to fetch dates mentioned as \"Paris, le 13 d\u00e9cembre 2015\". It has a high recall but poor precision, since those dates can often be dates of letter redaction instead of consultation dates.</p>"},{"location":"pipelines/misc/consultation-dates/#the-document_date_mention-terms","title":"The <code>document_date_mention</code> terms","text":"<p>This list contains expressions mentioning the date of creation/edition of a document, such as \"Date du rapport: 13/12/2015\" or \"Sign\u00e9 le 13/12/2015\". As for <code>town_mention</code>, it has a high recall but is prone to errors since document date and consultation date aren't necessary similar.</p> <p>Note</p> <p>By default, only the <code>consultation_mention</code> are used</p>"},{"location":"pipelines/misc/consultation-dates/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>consultation_mention</code> Whether to use consultation patterns, or list of patterns <code>True</code> (use pre-defined patterns) <code>town_mention</code> Whether to use town patterns, or list of patterns <code>False</code> <code>document_date_mention</code> Whether to use document date patterns, or list of patterns <code>False</code> <code>attr</code> spaCy attribute to match on, eg <code>NORM</code> or <code>TEXT</code> <code>\"NORM\"</code>"},{"location":"pipelines/misc/consultation-dates/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.consultation_dates</code> pipeline declares one spaCy extensions on the <code>Span</code> object: the <code>consultation_date</code> attribute, which is a Python <code>datetime</code> object.</p>"},{"location":"pipelines/misc/consultation-dates/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.consultation_dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/misc/dates/","title":"Dates","text":"<p>The <code>eds.dates</code> pipeline's role is to detect and normalise dates within a medical document. We use simple regular expressions to extract date mentions.</p>"},{"location":"pipelines/misc/dates/#scope","title":"Scope","text":"<p>The <code>eds.dates</code> pipeline finds absolute (eg <code>23/08/2021</code>) and relative (eg <code>hier</code>, <code>la semaine derni\u00e8re</code>) dates alike. It also handles mentions of duration.</p> Type Example <code>absolute</code> <code>3 mai</code>, <code>03/05/2020</code> <code>relative</code> <code>hier</code>, <code>la semaine derni\u00e8re</code> <code>duration</code> <code>pendant quatre jours</code> <p>See the tutorial for a presentation of a full pipeline featuring the <code>eds.dates</code> component.</p>"},{"location":"pipelines/misc/dates/#usage","title":"Usage","text":"<pre><code>import spacy\n\nimport pendulum\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.dates\")\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur \u00e0 l'estomac. \"\n    \"Il lui \u00e9tait arriv\u00e9 la m\u00eame chose il y a un an pendant une semaine. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n\ndates = doc.spans[\"dates\"]\ndates\n# Out: [23 ao\u00fbt 2021, il y a un an, pendant une semaine, mai 1995]\n\ndates[0]._.date.to_datetime()\n# Out: 2021-08-23T00:00:00+02:00\n\ndates[1]._.date.to_datetime()\n# Out: -1 year\n\nnote_datetime = pendulum.datetime(2021, 8, 27, tz=\"Europe/Paris\")\n\ndates[1]._.date.to_datetime(note_datetime=note_datetime)\n# Out: DateTime(2020, 8, 27, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))\n\ndate_3_output = dates[3]._.date.to_datetime(\n    note_datetime=note_datetime,\n    infer_from_context=True,\n    tz=\"Europe/Paris\",\n    default_day=15,\n)\ndate_3_output\n# Out: DateTime(1995, 5, 15, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))\n</code></pre>"},{"location":"pipelines/misc/dates/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.dates</code> pipeline declares one spaCy extension on the <code>Span</code> object: the <code>date</code> attribute contains a parsed version of the date.</p>"},{"location":"pipelines/misc/dates/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>absolute</code> Absolute date patterns, eg <code>le 5 ao\u00fbt 2020</code> <code>None</code> (use pre-defined patterns) <code>relative</code> Relative date patterns, eg <code>hier</code>) <code>None</code> (use pre-defined patterns) <code>durations</code> Duration patterns, eg <code>pendant trois mois</code>) <code>None</code> (use pre-defined patterns) <code>false_positive</code> Some false positive patterns to exclude <code>None</code> (use pre-defined patterns) <code>detect_periods</code> Whether to look for periods <code>False</code> <code>detect_time</code> Whether to look for time around dates <code>True</code> <code>on_ents_only</code> Whether to look for dates around entities only <code>False</code> <code>as_ents</code> Whether to save detected dates as entities <code>False</code> <code>attr</code> spaCy attribute to match on, eg <code>NORM</code> or <code>TEXT</code> <code>\"NORM\"</code>"},{"location":"pipelines/misc/dates/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.dates</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/misc/measurements/","title":"Measurements","text":"<p>The <code>eds.measurements</code> pipeline's role is to detect and normalise numerical measurements within a medical document. We use simple regular expressions to extract and normalize measurements, and use <code>Measurement</code> classes to store them.</p> <p>Warning</p> <p>The <code>measurements</code> pipeline is still in active development and has not been rigorously validated. If you come across a measurement expression that goes undetected, please file an issue !</p>"},{"location":"pipelines/misc/measurements/#scope","title":"Scope","text":"<p>The <code>eds.measurements</code> pipeline can extract simple (eg <code>3cm</code>) measurements. It can detect elliptic enumerations (eg <code>32, 33 et 34kg</code>) of measurements of the same type and split the measurements accordingly.</p> <p>The normalized value can then be accessed via the <code>span._.value</code> attribute and converted on the fly to a desired unit.</p> <p>The current pipeline annotates the following measurements out of the box:</p> Measurement name Example <code>eds.size</code> <code>1m50</code>, <code>1.50m</code> <code>eds.weight</code> <code>12kg</code>, <code>1kg300</code> <code>eds.bmi</code> <code>BMI: 24</code>, <code>24 kg.m-2</code> <code>eds.volume</code> <code>2 cac</code>, <code>8ml</code>"},{"location":"pipelines/misc/measurements/#usage","title":"Usage","text":"<pre><code>import spacy\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\n    \"eds.measurements\",\n    config=dict(\n        measurements=[\"eds.size\", \"eds.weight\", \"eds.bmi\"],\n        extract_ranges=True,\n    ),\n)\n\ntext = \"\"\"\nLe patient est admis hier, fait 1m78 pour 76kg.\nLes deux nodules b\u00e9nins sont larges de 1,2 et 2.4mm.\nBMI: 24.\n\nLe nodule fait entre 1 et 1.5 cm\n\"\"\"\n\ndoc = nlp(text)\n\nmeasurements = doc.spans[\"measurements\"]\n\nmeasurements\n# Out: [1m78, 76kg, 1,2, 2.4mm, 24, entre 1 et 1.5 cm]\n\nmeasurements[0]\n# Out: 1m78\n\nstr(measurements[0]._.value)\n# Out: '1.78 m'\n\nmeasurements[0]._.value.cm\n# Out: 178.0\n\nmeasurements[2]\n# Out: 1,2\n\nstr(measurements[2]._.value)\n# Out: '1.2 mm'\n\nstr(measurements[2]._.value.mm)\n# Out: 1.2\n\nmeasurements[4]\n# Out: 24\n\nstr(measurements[4]._.value)\n# Out: '24 kg_per_m2'\n\nstr(measurements[4]._.value.kg_per_m2)\n# Out: 24\n\nstr(measurements[5]._.value)\n# Out: 1-1.5 cm\n</code></pre> <p>To extract all sizes in centimeters, and average range measurements, you can use the following snippet:</p> <pre><code>sizes = [\n    sum(item.cm for item in m._.value) / len(m._.value)\n    for m in doc.spans[\"measurements\"]\n    if m.label_ == \"eds.size\"\n]\nprint(sizes)\nsizes\n# Out: [178.0, 0.12, 0.24, 1.25]\n</code></pre>"},{"location":"pipelines/misc/measurements/#custom-measurement","title":"Custom measurement","text":"<p>You can declare custom measurements by changing the patterns</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\n    \"eds.measurements\",\n    config=dict(\n        measurements={\n            # this name will be used to define the labels of the matched entities\n            \"my_custom_surface_measurement\": {\n                # This measurement unit is homogenous to square meters\n                \"unit\": \"m2\",\n                # To handle cases like \"surface: 1.8\" (implied m2), we can use\n                # unitless patterns\n                \"unitless_patterns\": [\n                    {\n                        \"terms\": [\"surface\", \"aire\"],\n                        \"ranges\": [\n                            {\n                                \"unit\": \"m2\",\n                                \"min\": 0,\n                                \"max\": 9,\n                            }\n                        ],\n                    }\n                ],\n            },\n        }\n    ),\n)\n</code></pre>"},{"location":"pipelines/misc/measurements/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.measurements</code> pipeline declares a single spaCy extension on the <code>Span</code> object, the <code>value</code> attribute that is a <code>Measurement</code> instance.</p>"},{"location":"pipelines/misc/measurements/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p>"},{"location":"pipelines/misc/measurements/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.measurements</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/misc/reason/","title":"Reason","text":"<p>The <code>eds.reason</code> pipeline uses a rule-based algorithm to detect spans that relate to the reason of the hospitalisation. It was designed at AP-HP's EDS.</p>"},{"location":"pipelines/misc/reason/#usage","title":"Usage","text":"<p>The following snippet matches a simple terminology, and looks for spans of hospitalisation reasons. It is complete and can be run as is.</p> <pre><code>import spacy\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978, a \u00e9t\u00e9\nhospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = spacy.blank(\"fr\")\n\n# Extraction of entities\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\n\n\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.reason\", config=dict(use_sections=True))\ndoc = nlp(text)\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nreason._.is_reason\n# Out: True\n\nentities = reason._.ents_reason\nentities\n# Out: [asthme]\n\nentities[0].label_\n# Out: 'respiratoire'\n\nent = entities[0]\nent._.is_reason\n# Out: True\n</code></pre>"},{"location":"pipelines/misc/reason/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>reasons</code> Reasons patterns <code>None</code> (use pre-defined patterns) <code>attr</code> spaCy attribute to match on, eg <code>NORM</code> or <code>TEXT</code> <code>\"NORM\"</code> <code>use_sections</code> Whether to use sections <code>False</code> <code>ignore_excluded</code> Whether to ignore excluded tokens <code>False</code>"},{"location":"pipelines/misc/reason/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.reason</code> pipeline adds the key <code>reasons</code> to <code>doc.spans</code> and declares one spaCy extension, on the <code>Span</code> objects called <code>ents_reason</code>.</p> <p>The <code>ents_reason</code> extension is a list of named entities that overlap the <code>Span</code>, typically entities found in previous pipelines like <code>matcher</code>.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p>"},{"location":"pipelines/misc/reason/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reason</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/misc/sections/","title":"Sections","text":"<p>Detected sections are :</p> <ul> <li><code>allergies</code></li> <li><code>ant\u00e9c\u00e9dents</code></li> <li><code>ant\u00e9c\u00e9dents familiaux</code></li> <li><code>traitements entr\u00e9e</code></li> <li><code>conclusion</code></li> <li><code>conclusion entr\u00e9e</code></li> <li><code>habitus</code></li> <li><code>correspondants</code></li> <li><code>diagnostic</code></li> <li><code>donn\u00e9es biom\u00e9triques entr\u00e9e</code></li> <li><code>examens</code></li> <li><code>examens compl\u00e9mentaires</code></li> <li><code>facteurs de risques</code></li> <li><code>histoire de la maladie</code></li> <li><code>actes</code></li> <li><code>motif</code></li> <li><code>prescriptions</code></li> <li><code>traitements sortie</code></li> <li><code>evolution</code></li> <li><code>modalites sortie</code></li> <li><code>vaccinations</code></li> <li><code>introduction</code></li> </ul> <p>|</p> <p>The pipeline extracts section title. A \"section\" is then defined as the span of text between two titles.</p> <p>Remarks : - section <code>introduction</code> corresponds to the span of text between the header \"COMPTE RENDU D'HOSPITALISATION\" (usually denoting the beginning of the document) and the title of the following detected section - this pipeline works well for hospitalization summaries (CRH), but not necessarily for all types of documents (in particular for emergency or scan summaries CR-IMAGERIE)</p> <p>Use at your own risks</p> <p>Should you rely on <code>eds.sections</code> for critical downstream tasks, make sure to validate the pipeline to make sure that the component works. For instance, the <code>eds.history</code> pipeline can use sections to make its predictions, but that possibility is deactivated by default.</p>"},{"location":"pipelines/misc/sections/#usage","title":"Usage","text":"<p>The following snippet detects section titles. It is complete and can be run as is.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.sections\")\n\ntext = \"CRU du 10/09/2021\\n\" \"Motif :\\n\" \"Patient admis pour suspicion de COVID\"\n\ndoc = nlp(text)\n\ndoc.spans[\"section_titles\"]\n# Out: [Motif]\n</code></pre>"},{"location":"pipelines/misc/sections/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>sections</code> Sections patterns <code>None</code> (use pre-defined patterns) <code>add_patterns</code> Whether add endlines patterns <code>True</code> <code>attr</code> spaCy attribute to match on, eg <code>NORM</code> or <code>TEXT</code> <code>\"NORM\"</code> <code>ignore_excluded</code> Whether to ignore excluded tokens <code>True</code>"},{"location":"pipelines/misc/sections/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.sections</code> pipeline adds two fields to the <code>doc.spans</code> attribute :</p> <ol> <li>The <code>section_titles</code> key contains the list of all section titles extracted using the list declared in the <code>terms.py</code> module.</li> <li>The <code>sections</code> key contains a list of sections, ie spans of text between two section titles (or the last title and the end of the document).</li> </ol> <p>If the document has entities before calling this pipeline an attribute <code>section</code> is added to each entity.</p>"},{"location":"pipelines/misc/sections/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.sections</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/misc/tables/","title":"Tables","text":"<p>The <code>eds.tables</code> pipeline's role is to detect tables present in a medical document. We use simple regular expressions to extract tables like text.</p>"},{"location":"pipelines/misc/tables/#usage","title":"Usage","text":"<p><pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.tables\")\n\ntext = \"\"\"\nSERVICE\nMEDECINE INTENSIVE \u2013\nREANIMATION\nR\u00e9animation / Surveillance Continue\nM\u00e9dicale\n\nCOMPTE RENDU D'HOSPITALISATION du 05/06/2020 au 10/06/2020\nMadame DUPONT Marie, n\u00e9e le 16/05/1900, \u00e2g\u00e9e de 20 ans, a \u00e9t\u00e9 hospitalis\u00e9e en r\u00e9animation du\n05/06/1920 au 10/06/1920 pour intoxication m\u00e9dicamenteuse volontaire.\n\n\nExamens compl\u00e9mentaires\nH\u00e9matologie\nNum\u00e9ration\nLeucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\nH\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\nH\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\nH\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\nVGM \u00a6fL \u00a694.4 + \u00a679.6-94\nTCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\nCCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\nPlaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\nVMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\nSur le plan neurologique : Devant la persistance d'une confusion \u00e0 distance de l'intoxication au\n...\n\n2/2Pat : &lt;NOM&gt; &lt;Prenom&gt;|F |&lt;date&gt; | &lt;ipp&gt; |Intitul\u00e9 RCP\n\n\"\"\"\n\ndoc = nlp(text)\n\n# A table span\ntable = doc.spans[\"tables\"][0]\n# Leucocytes \u00a6x10*9/L \u00a64.97 \u00a64.09-11\n# H\u00e9maties \u00a6x10*12/L\u00a64.68 \u00a64.53-5.79\n# H\u00e9moglobine \u00a6g/dL \u00a614.8 \u00a613.4-16.7\n# H\u00e9matocrite \u00a6% \u00a644.2 \u00a639.2-48.6\n# VGM \u00a6fL \u00a694.4 + \u00a679.6-94\n# TCMH \u00a6pg \u00a631.6 \u00a627.3-32.8\n# CCMH \u00a6g/dL \u00a633.5 \u00a632.4-36.3\n# Plaquettes \u00a6x10*9/L \u00a6191 \u00a6172-398\n# VMP \u00a6fL \u00a611.5 + \u00a67.4-10.8\n\n# Convert span to Pandas table\ndf = table._.to_pd_table()\ntype(df)\n# &gt;&gt; pandas.core.frame.DataFrame\n</code></pre> The pd DataFrame: |      | 0           | 1        | 2      | 3         | | ---: | :---------- | :------- | :----- | :-------- | |    0 | Leucocytes  | x109/L  | 4.97   | 4.09-11   | |    1 | H\u00e9maties    | x1012/L | 4.68   | 4.53-5.79 | |    2 | H\u00e9moglobine | g/dL     | 14.8   | 13.4-16.7 | |    3 | H\u00e9matocrite | %        | 44.2   | 39.2-48.6 | |    4 | VGM         | fL       | 94.4 + | 79.6-94   | |    5 | TCMH        | pg       | 31.6   | 27.3-32.8 | |    6 | CCMH        | g/dL     | 33.5   | 32.4-36.3 | |    7 | Plaquettes  | x10*9/L  | 191    | 172-398   | |    8 | VMP         | fL       | 11.5 + | 7.4-10.8  |</p>"},{"location":"pipelines/misc/tables/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.tables</code> pipeline declares one spaCy extension on the <code>Span</code> object: the <code>to_pd_table()</code> method returns a parsed pandas version of the table.</p>"},{"location":"pipelines/misc/tables/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>tables_pattern</code> Pattern to identify table spans <code>rf\"(\\b.*{sep}.*\\n)+\"</code> <code>sep_pattern</code> Pattern to identify column separation <code>r\"\u00a6\"</code> <code>ignore_excluded</code> Ignore excluded tokens <code>True</code> <code>attr</code> spaCy attribute to match on, eg <code>NORM</code> or <code>TEXT</code> <code>\"TEXT\"</code>"},{"location":"pipelines/misc/tables/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.tables</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/ner/","title":"Named entity recognition","text":"<p>We provide a few Named Entity Recognition (NER) pipelines.</p>"},{"location":"pipelines/ner/adicap/","title":"ADICAP","text":"<p>The <code>eds.adicap</code> pipeline component matches the ADICAP codes. It was developped to work on anapathology reports.</p> <p>Document type</p> <p>It was developped to work on anapathology reports.</p> <p>We recommend also to use the <code>eds</code> language (<code>spacy.blank(\"eds\")</code>)</p> <p>The compulsory characters of the ADICAP code are identified and decoded. These characters represent the following attributes:</p> Field [en] Field [fr] Attribute Sampling mode Mode de prelevement sampling_mode Technic Type de technique technic Organ and regions Appareils, organes et r\u00e9gions organ Pathology Pathologie g\u00e9n\u00e9rale pathology Pathology type Type de la pathologie pathology_type Behaviour type Type de comportement behaviour_type <p>The pathology field takes 4 different values corresponding to the 4 possible interpretations of the ADICAP code, which are : \"PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE\", \"PATHOLOGIE TUMORALE\", \"PATHOLOGIE PARTICULIERE DES ORGANES\" and \"CYTOPATHOLOGIE\".</p> <p>Depending on the pathology value the behaviour type meaning changes, when the pathology is tumoral then it describes the malignancy of the tumor.</p> <p>For further details about the ADICAP code follow this link.</p>"},{"location":"pipelines/ner/adicap/#usage","title":"Usage","text":"<pre><code>import spacy\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.adicap\")\n\ntext = \"\"\"\"\nCOMPTE RENDU D\u2019EXAMEN\n\nAnt\u00e9riorit\u00e9(s) :  NEANT\n\n\nRenseignements cliniques :\nContexte d'exploration d'un carcinome canalaire infiltrant du quadrant sup\u00e9ro-externe du sein droit. La\nl\u00e9sion biopsi\u00e9e ce jour est situ\u00e9e \u00e0 5,5 cm de la l\u00e9sion du quadrant sup\u00e9ro-externe, \u00e0 l'union des\nquadrants inf\u00e9rieurs.\n\n\nMacrobiopsie 10G sur une zone de prise de contraste focale \u00e0 l'union des quadrants inf\u00e9rieurs du\nsein droit, mesurant 4 mm, class\u00e9e ACR4\n\n14 fragments ont \u00e9t\u00e9 communiqu\u00e9s fix\u00e9s en formol (lame n\u00b0 1a et lame n\u00b0 1b) . Il n'y a pas eu\nd'\u00e9chantillon congel\u00e9. Ces fragments ont \u00e9t\u00e9 inclus en paraffine en totalit\u00e9 et coup\u00e9s sur plusieurs\nniveaux.\nHistologiquement, il s'agit d'un parenchyme mammaire fibroadipeux parfois l\u00e9g\u00e8rement dystrophique\navec quelques petits kystes. Il n'y a pas d'hyperplasie \u00e9pith\u00e9liale, pas d'atypie, pas de prolif\u00e9ration\ntumorale. On note quelques suffusions h\u00e9morragiques focales.\n\nConclusion :\nL\u00e9gers remaniements dystrophiques \u00e0 l'union des quadrants inf\u00e9rieurs du sein droit.\nAbsence d'atypies ou de prolif\u00e9ration tumorale.\n\nCodification :   BHGS0040\n\"\"\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (BHGS0040,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: adicap\n\nent._.adicap.dict()\n# Out: {'code': 'BHGS0040',\n# 'sampling_mode': 'BIOPSIE CHIRURGICALE',\n# 'technic': 'HISTOLOGIE ET CYTOLOGIE PAR INCLUSION',\n# 'organ': \"SEIN (\u00c9GALEMENT UTILIS\u00c9 CHEZ L'HOMME)\",\n# 'pathology': 'PATHOLOGIE G\u00c9N\u00c9RALE NON TUMORALE',\n# 'pathology_type': 'ETAT SUBNORMAL - LESION MINEURE',\n# 'behaviour_type': 'CARACTERES GENERAUX'}\n</code></pre>"},{"location":"pipelines/ner/adicap/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>base_code</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>adicap_prefix</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p>"},{"location":"pipelines/ner/adicap/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.adicap</code> pipeline was developed by AP-HP's Data Science team. The codes were downloaded from the website of 'Agence du num\u00e9rique en sant\u00e9' 1 (\"Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions\")</p> <ol> <li> <p>Agence du num\u00e9rique en sant\u00e9. Th\u00e9saurus de la codification ADICAP - Index raisonn\u00e9 des l\u00e9sions. 05 2019. URL: http://esante.gouv.fr/terminologie-adicap.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipelines/ner/cim10/","title":"CIM10","text":"<p>The <code>eds.cim10</code> pipeline component matches the CIM10 (French-language ICD) terminology.</p> <p>Very low recall</p> <p>When using the <code>exact' matching mode, this component has a very poor recall performance. We can use the</code>simstring` mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"pipelines/ner/cim10/#usage","title":"Usage","text":"<pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.cim10\", config=dict(term_matcher=\"simstring\"))\n\ntext = \"Le patient est suivi pour fi\u00e8vres typho\u00efde et paratypho\u00efde.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (fi\u00e8vres typho\u00efde et paratypho\u00efde,)\n\nent = doc.ents[0]\n\nent.label_\n# Out: cim10\n\nent.kb_id_\n# Out: A01\n</code></pre>"},{"location":"pipelines/ner/cim10/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either <code>TerminologyTermMatcher.exact</code> or <code>TerminologyTermMatcher.simstring</code></p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p>"},{"location":"pipelines/ner/cim10/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.cim10</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/ner/covid/","title":"COVID","text":"<p>The <code>eds.covid</code> pipeline component detects mentions of COVID19 and adds them to <code>doc.ents</code>.</p>"},{"location":"pipelines/ner/covid/#usage","title":"Usage","text":"<pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.covid\")\n\ntext = \"Le patient est admis pour une infection au coronavirus.\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (infection au coronavirus,)\n</code></pre>"},{"location":"pipelines/ner/covid/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'LOWER'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"pipelines/ner/covid/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.covid</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/ner/drugs/","title":"Drugs","text":"<p>The <code>eds.drugs</code> pipeline component detects mentions of French drugs (brand names and active ingredients) and adds them to <code>doc.ents</code>. Each drug is mapped to an ATC code through the Romedi terminology 1. The ATC classifies drugs into groups.</p>"},{"location":"pipelines/ner/drugs/#usage","title":"Usage","text":"<p>In this example, we are looking for an oral antidiabetic medication (ATC code: A10B).</p> <pre><code>from edsnlp.pipelines.core.terminology import TerminologyTermMatcher\nimport spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.drugs\", config=dict(term_matcher=TerminologyTermMatcher.exact))\n\ntext = \"Traitement habituel: Kard\u00e9gic, cardensiel (bisoprolol), glucophage, lasilix\"\n\ndoc = nlp(text)\n\ndrugs_detected = [(x.text, x.kb_id_) for x in doc.ents]\n\ndrugs_detected\n# Out: [('Kard\u00e9gic', 'B01AC06'), ('cardensiel', 'C07AB07'), ('bisoprolol', 'C07AB07'), ('glucophage', 'A10BA02'), ('lasilix', 'C03CA01')]\n\noral_antidiabetics_detected = list(\n    filter(lambda x: (x[1].startswith(\"A10B\")), drugs_detected)\n)\noral_antidiabetics_detected\n# Out: [('glucophage', 'A10BA02')]\n</code></pre> <p>Glucophage is the brand name of a medication that contains metformine, the first-line medication for the treatment of type 2 diabetes.</p>"},{"location":"pipelines/ner/drugs/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either <code>TerminologyTermMatcher.exact</code> or <code>TerminologyTermMatcher.simstring</code></p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p>"},{"location":"pipelines/ner/drugs/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.drugs</code> pipeline was developed by the IAM team and CHU de Bordeaux's Data Science team.</p> <ol> <li> <p>S\u00e9bastien Cossin, Luc Lebrun, Gr\u00e9gory Lobre, Romain Loustau, Vianney Jouhet, Romain Griffier, Fleur Mougin, Gayo Diallo, and Frantz Thiessard. Romedi: An Open Data Source About French Drugs on the Semantic Web. Studies in Health Technology and Informatics, 264:79\u201382, August 2019. URL: https://hal.archives-ouvertes.fr/hal-02987843, doi:10.3233/SHTI190187.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipelines/ner/score/","title":"Score","text":"<p>The <code>eds.score</code> pipeline allows easy extraction of typical scores (Charlson, SOFA...) that can be found in clinical documents. The pipeline works by</p> <ul> <li>Extracting the score's name via the provided regular expressions</li> <li>Extracting the score's raw value via another set of RegEx</li> <li>Normalising the score's value via a normalising function</li> </ul>"},{"location":"pipelines/ner/score/#charlson-comorbidity-index","title":"Charlson Comorbidity Index","text":"<p>Implementing the <code>eds.score</code> pipeline, the <code>charlson</code> pipeline will extract the Charlson Comorbidity Index:</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.charlson\")\n\ntext = \"Charlson \u00e0 l'admission: 7.\\n\" \"Charlson: \\n\" \"OMS: \\n\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (Charlson \u00e0 l'admission: 7,)\n</code></pre> <p>We can see that only one occurrence was extracted. The second mention of Charlson in the text doesn't contain any numerical value, so it isn't extracted.</p> <p>Each extraction exposes 2 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'eds.charlson'\n\nent._.score_value\n# Out: 7\n</code></pre>"},{"location":"pipelines/ner/score/#sofa-score","title":"SOFA score","text":"<p>The <code>SOFA</code> pipe allows to extract Sequential Organ Failure Assessment (SOFA) scores, used to track a person's status during the stay in an intensive care unit to determine the extent of a person's organ function or rate failure.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.SOFA\")\n\ntext = \"SOFA (\u00e0 24H) : 12.\\n\" \"OMS: \\n\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (SOFA (\u00e0 24H) : 12,)\n</code></pre> <p>Each extraction exposes 3 extensions:</p> <pre><code>ent = doc.ents[0]\n\nent._.score_name\n# Out: 'eds.SOFA'\n\nent._.score_value\n# Out: 12\n\nent._.score_method\n# Out: '24H'\n</code></pre> <p>Score method can here be \"24H\", \"Maximum\", \"A l'admission\" or \"Non pr\u00e9cis\u00e9e\"</p>"},{"location":"pipelines/ner/score/#tnm-score","title":"TNM score","text":"<p>The <code>eds.TNM</code> pipe allows to extract TNM scores.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.TNM\")\n\ntext = \"TNM: pTx N1 M1\"\n\ndoc = nlp(text)\ndoc.ents\n# Out: (pTx N1 M1,)\n\nent = doc.ents[0]\nent._.value.dict()\n# {'modifier': 'p',\n#  'tumour': None,\n#  'tumour_specification': 'x',\n#  'node': '1',\n#  'node_specification': None,\n#  'metastasis': '1',\n#  'resection_completeness': None,\n#  'version': None,\n#  'version_year': None}\n</code></pre> <p>The TNM score is based on the developement of S. Priou, B. Rance and E. Kempf 1.</p>"},{"location":"pipelines/ner/score/#implementing-your-own-score","title":"Implementing your own score","text":"<p>Using the <code>eds.score</code> pipeline, you only have to change its configuration in order to implement a simple score extraction algorithm. As an example, let us see the configuration used for the <code>eds.charlson</code> pipe The configuration consists of 4 items:</p> <ul> <li><code>score_name</code>: The name of the score</li> <li><code>regex</code>: A list of regular expression to detect the score's mention</li> <li><code>value_extract</code>: A regular expression to extract the score's value in the context of the score's mention</li> <li><code>score_normalization</code>: A function name used to normalise the score's raw value</li> </ul> <p>Note</p> <p>spaCy doesn't allow to pass functions in the configuration of a pipeline. To circumvent this issue, functions need to be registered, which simply consists in decorating those functions</p> <p>The registration is done as follows:</p> <pre><code>@spacy.registry.misc(\"score_normalization.charlson\")\ndef my_normalization_score(raw_score: str):\n    # Implement some filtering here\n    # Return None if you want the score to be discarded\n    return normalized_score\n</code></pre> <p>The values used for the <code>eds.charlson</code> pipe are the following:</p> <pre><code>@spacy.registry.misc(\"score_normalization.charlson\")\ndef score_normalization(extracted_score):\n\"\"\"\n    Charlson score normalization.\n    If available, returns the integer value of the Charlson score.\n    \"\"\"\n    score_range = list(range(0, 30))\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n\n\ncharlson_config = dict(\n    score_name=\"charlson\",\n    regex=[r\"charlson\"],\n    value_extract=r\"charlson.*[\\n\\W]*(\\d+)\",\n    score_normalization=\"score_normalization.charlson\",\n)\n</code></pre> <ol> <li> <p>Emmanuelle Kempf, Sonia Priou, Guillaume Lam\u00e9, Christel Daniel, Ali Bellamine, Daniele Sommacale, yazid Belkacemi, Romain Bey, Gilles Galula, Namik Taright, Xavier Tannier, Bastien Rance, R\u00e9mi Flicoteaux, Fran\u00e7ois Hemery, Etienne Audureau, Gilles Chatellier, and Christophe Tournigand. Impact of two waves of Sars-Cov2 outbreak on the number, clinical presentation, care trajectories and survival of patients newly referred for a colorectal cancer: A French multicentric cohort study from a large group of University hospitals. International Journal of Cancer, 150(10):1609\u20131618, 2022. URL: https://hal.archives-ouvertes.fr/hal-03519085, doi:10.1002/ijc.33928.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipelines/ner/umls/","title":"UMLS","text":"<p>The <code>eds.umls</code> pipeline component matches the UMLS (Unified Medical Language System from NIH) terminology.</p> <p>Very low recall</p> <p>When using the <code>exact</code> matching mode, this component has a very poor recall performance. We can use the <code>simstring</code> mode to retrieve approximate matches, albeit at the cost of a significantly higher computation time.</p>"},{"location":"pipelines/ner/umls/#usage","title":"Usage","text":"<p><code>eds.umls</code> is an additional module that needs to be setup by:</p> <ol> <li><code>pip install -U umls_downloader</code></li> <li>Signing up for a UMLS Terminology Services Account. After filling a short form, you will receive your token API within a few days.</li> <li>Set <code>UMLS_API_KEY</code> locally: <code>export UMLS_API_KEY=your_api_key</code></li> </ol> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.umls\")\n\ntext = \"Grosse toux: le malade a \u00e9t\u00e9 mordu par des Amphibiens \" \"sous le genou\"\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (toux, a, par, Amphibiens, genou)\n\nent = doc.ents[0]\n\nent.label_\n# Out: umls\n\nent._.umls\n# Out: C0010200\n</code></pre> <p>You can easily change the default languages and sources with the <code>pattern_config</code> argument:</p> <pre><code>import spacy\n\n# Enable the french and english languages, through the french MeSH and LOINC\npattern_config = dict(languages=[\"FRE\", \"ENG\"], sources=[\"MSHFRE\", \"LNC\"])\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.umls\", config=dict(pattern_config=pattern_config))\n</code></pre> <p>See more options of languages and sources here.</p>"},{"location":"pipelines/ner/umls/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> PARAMETER DESCRIPTION <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either <code>TerminologyTermMatcher.exact</code> or <code>TerminologyTermMatcher.simstring</code></p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>pattern_config</code> <p>The pattern retriever configuration</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>dict(languages=['FRE'], sources=None)</code> </p>"},{"location":"pipelines/ner/umls/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.umls</code> pipeline was developed by AP-HP's Data Science team and INRIA SODA's team.</p>"},{"location":"pipelines/qualifiers/","title":"Qualifier overview","text":"<p>In EDS-NLP, we call qualifiers the suite of pipelines designed to qualify a pre-extracted entity for a linguistic modality.</p>"},{"location":"pipelines/qualifiers/#available-pipelines","title":"Available pipelines","text":"Name Description <code>eds.negation</code> Detect negated entities <code>eds.family</code> Detect entities that pertain to a patient's kin rather than themself <code>eds.hypothesis</code> Detect entities subject to speculation <code>eds.reported_speech</code> Detect entities that are quoted from the patient <code>eds.history</code> Detect entities that pertain to the patient's history"},{"location":"pipelines/qualifiers/#rationale","title":"Rationale","text":"<p>In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents.</p> <p>Now, consider the following example:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p> <p>To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort.</p>"},{"location":"pipelines/qualifiers/#under-the-hood","title":"Under the hood","text":"<p>Our qualifier pipelines all follow the same basic pattern:</p> <ol> <li> <p>The pipeline extracts cues. We define three (possibly overlapping) kinds :</p> <ul> <li><code>preceding</code>, ie cues that precede modulated entities ;</li> <li><code>following</code>, ie cues that follow modulated entities ;</li> <li>in some cases, <code>verbs</code>, ie verbs that convey a modulation (treated as preceding cues).</li> </ul> </li> <li> <p>The pipeline splits the text between sentences and propositions, using annotations from a sentencizer pipeline and <code>termination</code> patterns, which define syntagma/proposition terminations.</p> </li> <li> <p>For each pre-extracted entity, the pipeline checks whether there is a cue between the start of the syntagma and the start of the entity, or a following cue between the end of the entity and the end of the proposition.</p> </li> </ol> <p>Albeit simple, this algorithm can achieve very good performance depending on the modality. For instance, our <code>eds.negation</code> pipeline reaches 88% F1-score on our dataset.</p> <p>Dealing with pseudo-cues</p> <p>The pipeline can also detect pseudo-cues, ie phrases that contain cues but that are not cues themselves. For instance: <code>sans doute</code>/<code>without doubt</code> contains <code>sans/without</code>, but does not convey negation.</p> <p>Detecting pseudo-cues lets the pipeline filter out any cue that overlaps with a pseudo-cue.</p> <p>Sentence boundaries are required</p> <p>The rule-based algorithm detects cues, and propagate their modulation on the rest of the syntagma. For that reason, a qualifier pipeline needs a sentencizer component to be defined, and will fail otherwise.</p> <p>You may use EDS-NLP's:</p> <pre><code>nlp.add_pipe(\"eds.sentences\")\n</code></pre>"},{"location":"pipelines/qualifiers/#persisting-the-results","title":"Persisting the results","text":"<p>Our qualifier pipelines write their results to a custom spaCy extension, defined on both <code>Span</code> and <code>Token</code> objects. We follow the convention of naming said attribute after the pipeline itself, eg <code>Span._.negation</code>for the<code>eds.negation</code> pipeline. In most cases, that extension is a boolean.</p> <p>We also provide a string representation of the result, computed on the fly by declaring a getter that reads the boolean result of the pipeline. Following spaCy convention, we give this attribute the same name, followed by a <code>_</code>.</p>"},{"location":"pipelines/qualifiers/family/","title":"Family","text":"<p>The <code>eds.family</code> pipeline uses a simple rule-based algorithm to detect spans that describe a family member (or family history) of the patient rather than the patient themself.</p>"},{"location":"pipelines/qualifiers/family/#usage","title":"Usage","text":"<p>The following snippet matches a simple terminology, and checks the family context of the extracted entities. It is complete, and can be run as is.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\n# Dummy matcher\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(terms=dict(douleur=\"douleur\", osteoporose=\"ost\u00e9oporose\")),\n)\nnlp.add_pipe(\"eds.family\")\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents familiaux d'ost\u00e9oporose\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, ost\u00e9oporose)\n\ndoc.ents[0]._.family\n# Out: False\n\ndoc.ents[1]._.family\n# Out: True\n</code></pre>"},{"location":"pipelines/qualifiers/family/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>attr</code> spaCy attribute to match on (eg <code>NORM</code>, <code>TEXT</code>, <code>LOWER</code>) <code>\"NORM\"</code> <code>family</code> Family patterns <code>None</code> (use pre-defined patterns) <code>termination</code> Termination patterns (for syntagma/proposition extraction) <code>None</code> (use pre-defined patterns) <code>use_sections</code> Whether to use pre-annotated sections (requires the <code>sections</code> pipeline) <code>False</code> <code>on_ents_only</code> Whether to qualify pre-extracted entities only <code>True</code> <code>explain</code> Whether to keep track of the cues for each entity <code>False</code>"},{"location":"pipelines/qualifiers/family/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.family</code> pipeline declares two spaCy extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>family</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the span/token relates to a family member.</li> <li>The <code>family_</code> property is a human-readable string, computed from the <code>family</code> attribute. It implements a simple getter function that outputs <code>PATIENT</code> or <code>FAMILY</code>, depending on the value of <code>family</code>.</li> </ol>"},{"location":"pipelines/qualifiers/family/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.family</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/qualifiers/history/","title":"Medical History","text":"<p>The <code>eds.history</code> pipeline uses a simple rule-based algorithm to detect spans that describe medical history rather than the diagnostic of a given visit.</p> <p>The mere definition of an medical history is not straightforward. Hence, this component only tags entities that are explicitly described as part of the medical history, eg preceded by a synonym of \"medical history\".</p> <p>This component may also use the output of:</p> <ul> <li>the <code>eds.sections</code> pipeline. In that case, the entire <code>ant\u00e9c\u00e9dent</code> section is tagged as a medical history.</li> </ul> <p>Sections</p> <p>Be careful, the <code>eds.sections</code> component may oversize the <code>ant\u00e9c\u00e9dents</code> section. Indeed, it detects section titles and tags the entire text between a title and the next as a section. Hence, should a section title goes undetected after the <code>ant\u00e9c\u00e9dents</code> title, some parts of the document will erroneously be tagged as a medical history.</p> <p>To curb that possibility, using the output of the <code>eds.sections</code> component is deactivated by default.</p> <ul> <li>the <code>eds.dates</code> pipeline. In that case, it will take the dates into account to tag extracted entities as a medical history or not.</li> </ul> <p>Dates</p> <p>To take the most of the <code>eds.dates</code> component, you may add the <code>note_datetime</code> context (cf. Adding context). It allows the pipeline to compute the duration of absolute dates (eg le 28 ao\u00fbt 2022/August 28, 2022). The <code>birth_datetime</code> context allows the pipeline to exclude the birth date from the extracted dates.</p>"},{"location":"pipelines/qualifiers/history/#usage","title":"Usage","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are history or not. It is complete and can be run as is.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.sections\")\nnlp.add_pipe(\"eds.dates\")\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(terms=dict(douleur=\"douleur\", malaise=\"malaises\")),\n)\nnlp.add_pipe(\n    \"eds.history\",\n    config=dict(\n        use_sections=True,\n        use_dates=True,\n    ),\n)\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Il a des ant\u00e9c\u00e9dents de malaises.\"\n    \"ANT\u00c9C\u00c9DENTS : \"\n    \"- le patient a d\u00e9j\u00e0 eu des malaises. \"\n    \"- le patient a eu une douleur \u00e0 la jambe il y a 10 jours\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, malaises, malaises, douleur)\n\ndoc.ents[0]._.history\n# Out: False\n\ndoc.ents[1]._.history\n# Out: True\n\ndoc.ents[2]._.history  # (1)\n# Out: True\n\ndoc.ents[3]._.history  # (2)\n# Out: False\n</code></pre> <ol> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>.</li> <li>The entity is in the section <code>ant\u00e9c\u00e9dent</code>, however the extracted <code>relative_date</code> refers to an event that took place within 14 days.</li> </ol>"},{"location":"pipelines/qualifiers/history/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>attr</code> spaCy attribute to match on (eg <code>NORM</code>, <code>TEXT</code>, <code>LOWER</code>) <code>\"NORM\"</code> <code>history</code> History patterns <code>None</code> (use pre-defined patterns) <code>termination</code> Termination patterns (for syntagma/proposition extraction) <code>None</code> (use pre-defined patterns) <code>use_sections</code> Whether to use pre-annotated sections (requires the <code>sections</code> pipeline) <code>False</code> <code>use_dates</code> Whether to use dates pipeline (requires the <code>dates</code> pipeline and <code>note_datetime</code> context is recommended) <code>False</code> <code>history_limit</code> If <code>use_dates = True</code>. The number of days after which the event is considered as history. <code>14</code> (2 weeks) <code>exclude_birthdate</code> If <code>use_dates = True</code>. Whether to exclude the birth date from history dates. <code>True</code> <code>closest_dates_only</code> If <code>use_dates = True</code>. Whether to include the closest dates only. If <code>False</code>, it includes all dates in the sentence. <code>True</code> <code>on_ents_only</code> Whether to qualify pre-extracted entities only <code>True</code> <code>explain</code> Whether to keep track of the cues for each entity <code>False</code>"},{"location":"pipelines/qualifiers/history/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.history</code> pipeline declares two spaCy extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>history</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the span/token is a medical history.</li> <li>The <code>history_</code> property is a human-readable string, computed from the <code>history</code> attribute. It implements a simple getter function that outputs <code>CURRENT</code> or <code>ATCD</code>, depending on the value of <code>history</code>.</li> </ol>"},{"location":"pipelines/qualifiers/history/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.history</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/qualifiers/hypothesis/","title":"Hypothesis","text":"<p>The <code>eds.hypothesis</code> pipeline uses a simple rule-based algorithm to detect spans that are speculations rather than certain statements.</p>"},{"location":"pipelines/qualifiers/hypothesis/#usage","title":"Usage","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a speculation. It is complete and can be run as is.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\n# Dummy matcher\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(terms=dict(douleur=\"douleur\", fracture=\"fracture\")),\n)\nnlp.add_pipe(\"eds.hypothesis\")\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Possible fracture du radius.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (douleur, fracture)\n\ndoc.ents[0]._.hypothesis\n# Out: False\n\ndoc.ents[1]._.hypothesis\n# Out: True\n</code></pre>"},{"location":"pipelines/qualifiers/hypothesis/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>attr</code> spaCy attribute to match on (eg <code>NORM</code>, <code>TEXT</code>, <code>LOWER</code>) <code>\"NORM\"</code> <code>pseudo</code> Pseudo-hypothesis patterns <code>None</code> (use pre-defined patterns) <code>preceding</code> Preceding hypothesis patterns <code>None</code> (use pre-defined patterns) <code>following</code> Following hypothesis patterns <code>None</code> (use pre-defined patterns) <code>termination</code> Termination patterns (for syntagma/proposition extraction) <code>None</code> (use pre-defined patterns) <code>verbs_hyp</code> Patterns for verbs that imply a hypothesis <code>None</code> (use pre-defined patterns) <code>verbs_eds</code> Common verb patterns, checked for conditional mode <code>None</code> (use pre-defined patterns) <code>on_ents_only</code> Whether to qualify pre-extracted entities only <code>True</code> <code>within_ents</code> Whether to look for hypothesis within entities <code>False</code> <code>explain</code> Whether to keep track of the cues for each entity <code>False</code>"},{"location":"pipelines/qualifiers/hypothesis/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.hypothesis</code> pipeline declares two spaCy extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>hypothesis</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the span/token is a speculation.</li> <li>The <code>hypothesis_</code> property is a human-readable string, computed from the <code>hypothesis</code> attribute. It implements a simple getter function that outputs <code>HYP</code> or <code>CERT</code>, depending on the value of <code>hypothesis</code>.</li> </ol>"},{"location":"pipelines/qualifiers/hypothesis/#performance","title":"Performance","text":"<p>The pipeline's performance is measured on three datasets :</p> <ul> <li>The ESSAI1 and CAS2 datasets were developed at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at EDS to test the pipeline on actual clinical notes, using pseudonymised notes from the EDS.</li> </ul> Dataset Hypothesis F1 CAS/ESSAI 49% NegParHyp 52% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"pipelines/qualifiers/hypothesis/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.hypothesis</code> pipeline was developed by AP-HP's Data Science team.</p> <ol> <li> <p>Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale, 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637.\u00a0\u21a9</p> </li> <li> <p>Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis, Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipelines/qualifiers/negation/","title":"Negation","text":"<p>The <code>eds.negation</code> pipeline uses a simple rule-based algorithm to detect negated spans. It was designed at AP-HP's EDS, following the insights of the NegEx algorithm by Chapman et al1.</p>"},{"location":"pipelines/qualifiers/negation/#usage","title":"Usage","text":"<p>The following snippet matches a simple terminology, and checks the polarity of the extracted entities. It is complete and can be run as is.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\n# Dummy matcher\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(terms=dict(patient=\"patient\", fracture=\"fracture\")),\n)\nnlp.add_pipe(\"eds.negation\")\n\ntext = (\n    \"Le patient est admis le 23 ao\u00fbt 2021 pour une douleur au bras. \"\n    \"Le scanner ne d\u00e9tecte aucune fracture.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, fracture)\n\ndoc.ents[0]._.negation  # (1)\n# Out: False\n\ndoc.ents[1]._.negation\n# Out: True\n</code></pre> <ol> <li>The result of the pipeline is kept in the <code>negation</code> custom extension.</li> </ol>"},{"location":"pipelines/qualifiers/negation/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>attr</code> spaCy attribute to match on (eg <code>NORM</code>, <code>TEXT</code>, <code>LOWER</code>) <code>\"NORM\"</code> <code>pseudo</code> Pseudo-negation patterns <code>None</code> (use pre-defined patterns) <code>preceding</code> Preceding negation patterns <code>None</code> (use pre-defined patterns) <code>following</code> Following negation patterns <code>None</code> (use pre-defined patterns) <code>termination</code> Termination patterns (for syntagma/proposition extraction) <code>None</code> (use pre-defined patterns) <code>verbs</code> Patterns for verbs that imply a negation <code>None</code> (use pre-defined patterns) <code>on_ents_only</code> Whether to qualify pre-extracted entities only <code>True</code> <code>within_ents</code> Whether to look for negations within entities <code>False</code> <code>explain</code> Whether to keep track of the cues for each entity <code>False</code>"},{"location":"pipelines/qualifiers/negation/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.negation</code> pipeline declares two spaCy extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>negation</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the span/token is negated.</li> <li>The <code>negation_</code> property is a human-readable string, computed from the <code>negation</code> attribute. It implements a simple getter function that outputs <code>AFF</code> or <code>NEG</code>, depending on the value of <code>negation</code>.</li> </ol>"},{"location":"pipelines/qualifiers/negation/#performance","title":"Performance","text":"<p>The pipeline's performance is measured on three datasets :</p> <ul> <li>The ESSAI2 and CAS3 datasets were developed at the CNRS. The two are concatenated.</li> <li>The NegParHyp corpus was specifically developed at AP-HP to test the pipeline on actual clinical notes, using pseudonymised notes from the AP-HP.</li> </ul> Dataset Negation F1 CAS/ESSAI 71% NegParHyp 88% <p>NegParHyp corpus</p> <p>The NegParHyp corpus was built by matching a subset of the MeSH terminology with around 300 documents from AP-HP's clinical data warehouse. Matched entities were then labelled for negation, speculation and family context.</p>"},{"location":"pipelines/qualifiers/negation/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.negation</code> pipeline was developed by AP-HP's Data Science team.</p> <ol> <li> <p>Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. Journal of Biomedical Informatics, 34(5):301\u2013310, October 2001. URL: https://linkinghub.elsevier.com/retrieve/pii/S1532046401910299 (visited on 2020-12-31), doi:10.1006/jbin.2001.1029.\u00a0\u21a9</p> </li> <li> <p>Cl\u00e9ment Dalloux, Vincent Claveau, and Natalia Grabar. D\u00e9tection de la n\u00e9gation : corpus fran\u00e7ais et apprentissage supervis\u00e9. In SIIM 2017 - Symposium sur l'Ing\u00e9nierie de l'Information M\u00e9dicale, 1\u20138. Toulouse, France, November 2017. URL: https://hal.archives-ouvertes.fr/hal-01659637.\u00a0\u21a9</p> </li> <li> <p>Natalia Grabar, Vincent Claveau, and Cl\u00e9ment Dalloux. CAS: French Corpus with Clinical Cases. In LOUHI 2018 - The Ninth International Workshop on Health Text Mining and Information Analysis, Ninth International Workshop on Health Text Mining and Information Analysis (LOUHI) Proceedings of the Workshop, 1\u20137. Bruxelles, France, October 2018. URL: https://hal.archives-ouvertes.fr/hal-01937096.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipelines/qualifiers/reported-speech/","title":"Reported Speech","text":"<p>The <code>eds.reported_speech</code> pipeline uses a simple rule-based algorithm to detect spans that relate to reported speech (eg when the doctor quotes the patient). It was designed at AP-HP's EDS.</p>"},{"location":"pipelines/qualifiers/reported-speech/#usage","title":"Usage","text":"<p>The following snippet matches a simple terminology, and checks whether the extracted entities are part of a reported speech. It is complete and can be run as is.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\n# Dummy matcher\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(terms=dict(patient=\"patient\", alcool=\"alcoolis\u00e9\")),\n)\nnlp.add_pipe(\"eds.reported_speech\")\n\ntext = (\n    \"Le patient est admis aux urgences ce soir pour une douleur au bras. \"\n    \"Il nie \u00eatre alcoolis\u00e9.\"\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (patient, alcoolis\u00e9)\n\ndoc.ents[0]._.reported_speech\n# Out: False\n\ndoc.ents[1]._.reported_speech\n# Out: True\n</code></pre>"},{"location":"pipelines/qualifiers/reported-speech/#configuration","title":"Configuration","text":"<p>The pipeline can be configured using the following parameters :</p> Parameter Explanation Default <code>attr</code> spaCy attribute to match on (eg <code>NORM</code>, <code>TEXT</code>, <code>LOWER</code>) <code>\"NORM\"</code> <code>pseudo</code> Pseudo-reported speech patterns <code>None</code> (use pre-defined patterns) <code>preceding</code> Preceding reported speech patterns <code>None</code> (use pre-defined patterns) <code>following</code> Following reported speech patterns <code>None</code> (use pre-defined patterns) <code>termination</code> Termination patterns (for syntagma/proposition extraction) <code>None</code> (use pre-defined patterns) <code>verbs</code> Patterns for verbs that imply a reported speech <code>None</code> (use pre-defined patterns) <code>on_ents_only</code> Whether to qualify pre-extracted entities only <code>True</code> <code>within_ents</code> Whether to look for reported speech within entities <code>False</code> <code>explain</code> Whether to keep track of the cues for each entity <code>False</code>"},{"location":"pipelines/qualifiers/reported-speech/#declared-extensions","title":"Declared extensions","text":"<p>The <code>eds.reported_speech</code> pipeline declares two spaCy extensions, on both <code>Span</code> and <code>Token</code> objects :</p> <ol> <li>The <code>reported_speech</code> attribute is a boolean, set to <code>True</code> if the pipeline predicts that the span/token is reported.</li> <li>The <code>reported_speech_</code> property is a human-readable string, computed from the <code>reported_speech</code> attribute. It implements a simple getter function that outputs <code>DIRECT</code> or <code>REPORTED</code>, depending on the value of <code>reported_speech</code>.</li> </ol>"},{"location":"pipelines/qualifiers/reported-speech/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.reported_speech</code> pipeline was developed by AP-HP's Data Science team.</p>"},{"location":"pipelines/trainable/","title":"Trainable components overview","text":"<p>In addition to its rule-based pipeline components, EDS-NLP offers new trainable pipelines to fit and run machine learning models for classic biomedical information extraction tasks.</p>"},{"location":"pipelines/trainable/#available-components","title":"Available components :","text":"Name Description <code>eds.nested_ner</code> Recognize overlapping or nested entities (replaces spaCy's <code>ner</code> component) <p>Writing custom models</p> <p>spaCy models can be written with Thinc (spaCy's deep learning library), Tensorflow or Pytorch. As Pytorch is predominant in the NLP research field, we recommend writing models with the latter to facilitate interactions with the NLP community. To this end, we have written some Pytorch wrapping utilities like wrap_pytorch_model to allow loss and predictions to be computed directly in the Pytorch module.</p>"},{"location":"pipelines/trainable/#utils","title":"Utils","text":""},{"location":"pipelines/trainable/#training","title":"Training","text":"<p>In addition to the spaCy <code>train</code> CLI, EDS-NLP offers a <code>train</code> function that can be called in Python directly with an existing spaCy pipeline.</p> <p>Experimental</p> <p>This training API is an experimental feature of edsnlp and could change at any time.</p>"},{"location":"pipelines/trainable/#usage","title":"Usage","text":"<p>Let us define and train a full pipeline :</p> <pre><code>from pathlib import Path\n\nimport spacy\n\nfrom edsnlp.connectors.brat import BratConnector\nfrom edsnlp.utils.training import train, make_spacy_corpus_config\n\ntmp_path = Path(\"/tmp/test-train\")\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\"nested_ner\")  # (1)\n\n# Train the model, with additional training configuration\nnlp = train(\n    nlp,\n    output_path=tmp_path / \"model\",\n    config=dict(\n        **make_spacy_corpus_config(\n            train_data=\"/path/to/the/training/set/brat/files\",\n            dev_data=\"/path/to/the/dev/set/brat/files\",\n            nlp=nlp,\n            data_format=\"brat\",\n        ),\n        training=dict(\n            max_steps=4000,\n        ),\n    ),\n)\n\n# Finally, we can run the pipeline on a new document\ndoc = nlp(\"Arret du folfox si inefficace\")\ndoc.spans[\"drug\"]\n# Out: [folfox]\n\ndoc.spans[\"criteria\"]\n# Out: [si folfox inefficace]\n\n# And export new predictions as Brat annotations\npredicted_docs = BratConnector(\"/path/to/the/new/files\", run_pipe=True).brat2docs(nlp)\nBratConnector(\"/path/to/predictions\").docs2brat(predicted_docs)\n</code></pre> <ol> <li>you can configure the component using the <code>add_pipe(..., config=...)</code> parameter</li> </ol>"},{"location":"pipelines/trainable/ner/","title":"Nested Named Entity Recognition","text":"<p>The default spaCy Named Entity Recognizer (NER) pipeline only allows flat entity recognition, meaning that overlapping and nested entities cannot be extracted.</p> <p>The other spaCy component <code>SpanCategorizer</code> only supports assigning to a specific span group and both components are not well suited for extracting entities with ill-defined boundaries (this can occur if your training data contains difficult and long entities).</p> <p>We propose the new <code>eds.ner</code> component to extract almost any named entity:</p> <ul> <li>flat entities like spaCy's <code>EntityRecognizer</code> overlapping entities</li> <li>overlapping entities of different labels (much like spaCy's <code>SpanCategorizer</code>)</li> <li>entities will ill-defined boundaries</li> </ul> <p>However, the model cannot currently extract entities that are nested inside larger entities of the same label.</p> <p>The pipeline assigns both <code>doc.ents</code> (in which overlapping entities are filtered out) and <code>doc.spans</code>.</p>"},{"location":"pipelines/trainable/ner/#architecture","title":"Architecture","text":"<p>The model performs token classification using the BIOUL (Begin, Inside, Outside, Unary, Last) tagging scheme. To extract overlapping entities, each label has its own tag sequence, so the model predicts $n_{labels}$ sequences of O, I, B, L, U tags. The architecture is displayed in the figure below.</p> <p>To enforce the tagging scheme, (ex: I cannot follow O but only B, ...), we use a stack of CRF (Conditional Random Fields) layers, one per label during both training and prediction.</p> <p> </p> Nested NER architecture"},{"location":"pipelines/trainable/ner/#usage","title":"Usage","text":"<p>Let us define the pipeline and train it:</p> <pre><code>from pathlib import Path\n\nimport spacy\n\nfrom edsnlp.connectors.brat import BratConnector\nfrom edsnlp.utils.training import train, make_spacy_corpus_config\n\ntmp_path = Path(\"/tmp/test-nested-ner\")\n\nnlp = spacy.blank(\"eds\")\n# \u2193 below is the nested ner pipeline \u2193\n# you can configure it using the `add_pipe(..., config=...)` parameter\nnlp.add_pipe(\"nested_ner\")\n\n# Train the model, with additional training configuration\nnlp = train(\n    nlp,\n    output_path=tmp_path / \"model\",\n    config=dict(\n        **make_spacy_corpus_config(\n            train_data=\"/path/to/the/training/set/brat/files\",\n            dev_data=\"/path/to/the/dev/set/brat/files\",\n            nlp=nlp,\n            data_format=\"brat\",\n        ),\n        training=dict(\n            max_steps=4000,\n        ),\n    ),\n)\n\n# Finally, we can run the pipeline on a new document\ndoc = nlp(\"Arret du folfox si inefficace\")\ndoc.spans[\"drug\"]\n# Out: [folfox]\n\ndoc.spans[\"criteria\"]\n# Out: [si folfox inefficace]\n\n# And export new predictions as Brat annotations\npredicted_docs = BratConnector(\"/path/to/the/new/files\", run_pipe=True).brat2docs(nlp)\nBratConnector(\"/path/to/predictions\").docs2brat(predicted_docs)\n</code></pre>"},{"location":"pipelines/trainable/ner/#configuration","title":"Configuration","text":"<p>The pipeline component can be configured using the following parameters :</p> Parameter Explanation Default <code>ent_labels</code> Labels to search in and assign to <code>doc.ents</code>. Expects a list. <code>None</code> (inits to all labels in <code>doc.ents</code>) <code>spans_labels</code> Labels to search in and assign to <code>doc.spans</code>. Expects a dict of lists. <code>None</code> (inits to all span groups and their labels in <code>doc.spans</code>) <p>The default model <code>eds.nested_ner_model.v1</code> can be configured using the following parameters :</p> Parameter Explanation Default <code>loss_mode</code> How the CRF loss is computed <code>joint</code> \u2192<code>joint</code> Loss accounts for CRF transitions \u2192<code>independent</code> Loss does not account for CRF transitions (softmax loss) \u2192<code>marginal</code> Tag scores are smoothly updated with CRF transitions, and softmax loss is applied"},{"location":"pipelines/trainable/ner/#authors-and-citation","title":"Authors and citation","text":"<p>The <code>eds.nested_ner</code> pipeline was developed by AP-HP's Data Science team.</p> <p>The deep learning model was adapted from Wajsb\u00fcrt1</p> <ol> <li> <p>Perceval Wajsb\u00fcrt. Extraction and normalization of simple and structured entities in medical documents. Theses, Sorbonne Universit\u00e9, December 2021. URL: https://hal.archives-ouvertes.fr/tel-03624928.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"<code>edsnlp</code>","text":"<p>EDS-NLP</p>"},{"location":"reference/components/","title":"<code>edsnlp.components</code>","text":""},{"location":"reference/conjugator/","title":"<code>edsnlp.conjugator</code>","text":""},{"location":"reference/conjugator/#edsnlp.conjugator.conjugate_verb","title":"<code>conjugate_verb(verb, conjugator)</code>","text":"<p>Conjugates the verb using an instance of mlconjug3, and formats the results in a pandas <code>DataFrame</code>.</p> PARAMETER DESCRIPTION <code>verb</code> <p>Verb to conjugate.</p> <p> TYPE: <code>str</code> </p> <code>conjugator</code> <p>mlconjug3 instance for conjugating.</p> <p> TYPE: <code>mlconjug3.Conjugator</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>Normalized dataframe containing all conjugated forms for the verb.</p> Source code in <code>edsnlp/conjugator.py</code> <pre><code>def conjugate_verb(\n    verb: str,\n    conjugator: mlconjug3.Conjugator,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Conjugates the verb using an instance of mlconjug3,\n    and formats the results in a pandas `DataFrame`.\n\n    Parameters\n    ----------\n    verb : str\n        Verb to conjugate.\n    conjugator : mlconjug3.Conjugator\n        mlconjug3 instance for conjugating.\n\n    Returns\n    -------\n    pd.DataFrame\n        Normalized dataframe containing all conjugated forms\n        for the verb.\n    \"\"\"\n\n    df = pd.DataFrame(\n        conjugator.conjugate(verb).iterate(),\n        columns=[\"mode\", \"tense\", \"person\", \"term\"],\n    )\n\n    df.term = df.term.fillna(df.person)\n    df.loc[df.person == df.term, \"person\"] = None\n\n    df.insert(0, \"verb\", verb)\n\n    return df\n</code></pre>"},{"location":"reference/conjugator/#edsnlp.conjugator.conjugate","title":"<code>conjugate(verbs, language='fr')</code>","text":"<p>Conjugate a list of verbs.</p> PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to conjugate</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>language</code> <p>Language to conjugate. Defaults to French (<code>fr</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'fr'</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>Dataframe containing the conjugations for the provided verbs. Columns: <code>verb</code>, <code>mode</code>, <code>tense</code>, <code>person</code>, <code>term</code></p> Source code in <code>edsnlp/conjugator.py</code> <pre><code>def conjugate(\n    verbs: Union[str, List[str]],\n    language: str = \"fr\",\n) -&gt; pd.DataFrame:\n\"\"\"\n    Conjugate a list of verbs.\n\n    Parameters\n    ----------\n    verbs : Union[str, List[str]]\n        List of verbs to conjugate\n    language: str\n        Language to conjugate. Defaults to French (`fr`).\n\n    Returns\n    -------\n    pd.DataFrame\n        Dataframe containing the conjugations for the provided verbs.\n        Columns: `verb`, `mode`, `tense`, `person`, `term`\n    \"\"\"\n    if isinstance(verbs, str):\n        verbs = [verbs]\n\n    conjugator = mlconjug3.Conjugator(language=language)\n\n    df = pd.concat([conjugate_verb(verb, conjugator=conjugator) for verb in verbs])\n\n    df = df.reset_index(drop=True)\n\n    return df\n</code></pre>"},{"location":"reference/conjugator/#edsnlp.conjugator.get_conjugated_verbs","title":"<code>get_conjugated_verbs(verbs, matches, language='fr')</code>","text":"<p>Get a list of conjugated verbs.</p> PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to conjugate.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>matches</code> <p>List of dictionary describing the mode/tense/persons to keep.</p> <p> TYPE: <code>Union[List[Dict[str, str]], Dict[str, str]]</code> </p> <code>language</code> <p>[description], by default \"fr\" (French)</p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'fr'</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of terms to look for.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_conjugated_verbs(\n        \"aimer\",\n        dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"),\n    )\n['aimons']\n</code></pre> Source code in <code>edsnlp/conjugator.py</code> <pre><code>def get_conjugated_verbs(\n    verbs: Union[str, List[str]],\n    matches: Union[List[Dict[str, str]], Dict[str, str]],\n    language: str = \"fr\",\n) -&gt; List[str]:\n\"\"\"\n    Get a list of conjugated verbs.\n\n    Parameters\n    ----------\n    verbs : Union[str, List[str]]\n        List of verbs to conjugate.\n    matches : Union[List[Dict[str, str]], Dict[str, str]]\n        List of dictionary describing the mode/tense/persons to keep.\n    language : str, optional\n        [description], by default \"fr\" (French)\n\n    Returns\n    -------\n    List[str]\n        List of terms to look for.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_conjugated_verbs(\n            \"aimer\",\n            dict(mode=\"Indicatif\", tense=\"Pr\u00e9sent\", person=\"1p\"),\n        )\n    ['aimons']\n    \"\"\"\n\n    if isinstance(matches, dict):\n        matches = [matches]\n\n    terms = []\n\n    df = conjugate(\n        verbs=verbs,\n        language=language,\n    )\n\n    for match in matches:\n        q = \" &amp; \".join([f'{k} == \"{v}\"' for k, v in match.items()])\n        terms.extend(df.query(q).term.unique())\n\n    return list(set(terms))\n</code></pre>"},{"location":"reference/extensions/","title":"<code>edsnlp.extensions</code>","text":""},{"location":"reference/language/","title":"<code>edsnlp.language</code>","text":""},{"location":"reference/language/#edsnlp.language.EDSDefaults","title":"<code>EDSDefaults</code>","text":"<p>         Bases: <code>FrenchDefaults</code></p> <p>Defaults for the EDSLanguage class Mostly identical to the FrenchDefaults, but without tokenization info</p> Source code in <code>edsnlp/language.py</code> <pre><code>class EDSDefaults(FrenchDefaults):\n\"\"\"\n    Defaults for the EDSLanguage class\n    Mostly identical to the FrenchDefaults, but\n    without tokenization info\n    \"\"\"\n\n    tokenizer_exceptions = {}\n    infixes = []\n    lex_attr_getters = LEX_ATTRS\n    syntax_iterators = SYNTAX_ITERATORS\n    stop_words = STOP_WORDS\n    config = FrenchDefaults.config.merge(\n        {\n            \"nlp\": {\"tokenizer\": {\"@tokenizers\": \"eds.tokenizer\"}},\n        }\n    )\n</code></pre>"},{"location":"reference/language/#edsnlp.language.EDSLanguage","title":"<code>EDSLanguage</code>","text":"<p>         Bases: <code>French</code></p> <p>French clinical language. It is shipped with the <code>EDSTokenizer</code> tokenizer that better handles tokenization for French clinical documents</p> Source code in <code>edsnlp/language.py</code> <pre><code>@spacy.registry.languages(\"eds\")\nclass EDSLanguage(French):\n\"\"\"\n    French clinical language.\n    It is shipped with the `EDSTokenizer` tokenizer that better handles\n    tokenization for French clinical documents\n    \"\"\"\n\n    lang = \"eds\"\n    Defaults = EDSDefaults\n    default_config = Defaults\n</code></pre>"},{"location":"reference/language/#edsnlp.language.EDSTokenizer","title":"<code>EDSTokenizer</code>","text":"<p>         Bases: <code>DummyTokenizer</code></p> Source code in <code>edsnlp/language.py</code> <pre><code>class EDSTokenizer(DummyTokenizer):\n    def __init__(self, vocab: Vocab) -&gt; None:\n\"\"\"\n        Tokenizer class for French clinical documents.\n        It better handles tokenization around:\n        - numbers: \"ACR5\" -&gt; [\"ACR\", \"5\"] instead of [\"ACR5\"]\n        - newlines: \"\\n \\n \\n\" -&gt; [\"\\n\", \"\\n\", \"\\n\"] instead of [\"\\n \\n \\n\"]\n        and should be around 5-6 times faster than its standard French counterpart.\n        Parameters\n        ----------\n        vocab: Vocab\n            The spacy vocabulary\n        \"\"\"\n        self.vocab = vocab\n        punct = \"[:punct:]\" + \"\\\"'\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\"\n        num_like = r\"\\d+(?:[.,]\\d(?![.,]?[0-9])|(?![.,]?[0-9]))?\"\n        sep = rf\"\\d{punct}'\\n[:space:]\"\n        default = rf\"[^{sep}]+(?:['\u02ca](?=[[:alpha:]]|$))?\"\n        exceptions = \"|\".join(TOKENIZER_EXCEPTIONS)\n        acronym = r\"[A-Z][A-Z0-9]*[.](?=[A-Z0-9])\"\n        self.word_regex = regex.compile(\n            rf\"\"\"(?x)\n        (\n{exceptions}    # tokenizer exceptions like M., Dr., etc\n            |{acronym}      # acronyms\n            |{num_like}     # numbers\n            |[{punct}]      # punctuations\n            |[\\n\\r\\t]       # new lines or tabs\n            |[^\\S\\r\\n\\t]+   # multi-spaces\n            |{default}      # anything else: most often alpha-numerical words\n        )                   # followed by\n        ([^\\S\\r\\n\\t])?      # an optional space\n        \"\"\"\n        )\n\n    def __call__(self, text: str) -&gt; Doc:\n\"\"\"\n        Tokenizes the text using the EDSTokenizer\n\n        Parameters\n        ----------\n        text: str\n\n        Returns\n        -------\n        Doc\n\n        \"\"\"\n        last = 0\n        words = []\n        whitespaces = []\n        for match in self.word_regex.finditer(text):\n            begin, end = match.start(), match.end()\n            if last != begin:\n                logger.warning(\n                    \"Missed some characters during\"\n                    + f\" tokenization between {last} and {begin}: \"\n                    + text[last - 10 : last]\n                    + \"|\"\n                    + text[last:begin]\n                    + \"|\"\n                    + text[begin : begin + 10],\n                )\n            last = end\n            words.append(match.group(1))\n            whitespaces.append(bool(match.group(2)))\n        return Doc(self.vocab, words=words, spaces=whitespaces)\n</code></pre>"},{"location":"reference/language/#edsnlp.language.EDSTokenizer.__init__","title":"<code>__init__(vocab)</code>","text":"<pre><code>    Tokenizer class for French clinical documents.\n    It better handles tokenization around:\n    - numbers: \"ACR5\" -&gt; [\"ACR\", \"5\"] instead of [\"ACR5\"]\n    - newlines: \"\n</code></pre> <p>\" -&gt; [\" \", \" \", \" \"] instead of [\"</p> <p>\"]         and should be around 5-6 times faster than its standard French counterpart.         Parameters         ----------         vocab: Vocab             The spacy vocabulary</p> Source code in <code>edsnlp/language.py</code> <pre><code>def __init__(self, vocab: Vocab) -&gt; None:\n\"\"\"\n    Tokenizer class for French clinical documents.\n    It better handles tokenization around:\n    - numbers: \"ACR5\" -&gt; [\"ACR\", \"5\"] instead of [\"ACR5\"]\n    - newlines: \"\\n \\n \\n\" -&gt; [\"\\n\", \"\\n\", \"\\n\"] instead of [\"\\n \\n \\n\"]\n    and should be around 5-6 times faster than its standard French counterpart.\n    Parameters\n    ----------\n    vocab: Vocab\n        The spacy vocabulary\n    \"\"\"\n    self.vocab = vocab\n    punct = \"[:punct:]\" + \"\\\"'\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\"\n    num_like = r\"\\d+(?:[.,]\\d(?![.,]?[0-9])|(?![.,]?[0-9]))?\"\n    sep = rf\"\\d{punct}'\\n[:space:]\"\n    default = rf\"[^{sep}]+(?:['\u02ca](?=[[:alpha:]]|$))?\"\n    exceptions = \"|\".join(TOKENIZER_EXCEPTIONS)\n    acronym = r\"[A-Z][A-Z0-9]*[.](?=[A-Z0-9])\"\n    self.word_regex = regex.compile(\n        rf\"\"\"(?x)\n    (\n{exceptions}    # tokenizer exceptions like M., Dr., etc\n        |{acronym}      # acronyms\n        |{num_like}     # numbers\n        |[{punct}]      # punctuations\n        |[\\n\\r\\t]       # new lines or tabs\n        |[^\\S\\r\\n\\t]+   # multi-spaces\n        |{default}      # anything else: most often alpha-numerical words\n    )                   # followed by\n    ([^\\S\\r\\n\\t])?      # an optional space\n    \"\"\"\n    )\n</code></pre>"},{"location":"reference/language/#edsnlp.language.EDSTokenizer.__call__","title":"<code>__call__(text)</code>","text":"<p>Tokenizes the text using the EDSTokenizer</p> PARAMETER DESCRIPTION <code>text</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Doc</code> Source code in <code>edsnlp/language.py</code> <pre><code>def __call__(self, text: str) -&gt; Doc:\n\"\"\"\n    Tokenizes the text using the EDSTokenizer\n\n    Parameters\n    ----------\n    text: str\n\n    Returns\n    -------\n    Doc\n\n    \"\"\"\n    last = 0\n    words = []\n    whitespaces = []\n    for match in self.word_regex.finditer(text):\n        begin, end = match.start(), match.end()\n        if last != begin:\n            logger.warning(\n                \"Missed some characters during\"\n                + f\" tokenization between {last} and {begin}: \"\n                + text[last - 10 : last]\n                + \"|\"\n                + text[last:begin]\n                + \"|\"\n                + text[begin : begin + 10],\n            )\n        last = end\n        words.append(match.group(1))\n        whitespaces.append(bool(match.group(2)))\n    return Doc(self.vocab, words=words, spaces=whitespaces)\n</code></pre>"},{"location":"reference/language/#edsnlp.language.create_eds_tokenizer","title":"<code>create_eds_tokenizer()</code>","text":"<p>Creates a factory that returns new EDSTokenizer instances</p> RETURNS DESCRIPTION <code>EDSTokenizer</code> Source code in <code>edsnlp/language.py</code> <pre><code>@spacy.registry.tokenizers(\"eds.tokenizer\")\ndef create_eds_tokenizer():\n\"\"\"\n    Creates a factory that returns new EDSTokenizer instances\n\n    Returns\n    -------\n    EDSTokenizer\n    \"\"\"\n\n    def eds_tokenizer_factory(nlp):\n        return EDSTokenizer(nlp.vocab)\n\n    return eds_tokenizer_factory\n</code></pre>"},{"location":"reference/patch_spacy_dot_components/","title":"<code>edsnlp.patch_spacy_dot_components</code>","text":""},{"location":"reference/patch_spacy_dot_components/#edsnlp.patch_spacy_dot_components.factory","title":"<code>factory(cls, name, *, default_config=SimpleFrozenDict(), assigns=SimpleFrozenList(), requires=SimpleFrozenList(), retokenizes=False, default_score_weights=SimpleFrozenDict(), func=None)</code>  <code>classmethod</code>","text":"<p>Patched from spaCy to allow back dots in factory names (https://github.com/aphp/edsnlp/pull/152)</p> <p>Register a new pipeline component factory. Can be used as a decorator on a function or classmethod, or called as a function with the factory provided as the func keyword argument. To create a component and add it to the pipeline, you can use nlp.add_pipe(name).</p> <p>name (str): The name of the component factory. default_config (Dict[str, Any]): Default configuration, describing the     default values of the factory arguments. assigns (Iterable[str]): Doc/Token attributes assigned by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. requires (Iterable[str]): Doc/Token attributes required by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. retokenizes (bool): Whether the component changes the tokenization.     Used for pipeline analysis. default_score_weights (Dict[str, Optional[float]]): The scores to report during     training, and their default weight towards the final score used to     select the best model. Weights should sum to 1.0 per component and     will be combined and normalized for the whole pipeline. If None,     the score won't be shown in the logs or be weighted. func (Optional[Callable]): Factory function if not used as a decorator.</p> <p>DOCS: https://spacy.io/api/language#factory</p> Source code in <code>edsnlp/patch_spacy_dot_components.py</code> <pre><code>@classmethod\ndef factory(\n    cls,\n    name: str,\n    *,\n    default_config: Dict[str, Any] = SimpleFrozenDict(),\n    assigns: Iterable[str] = SimpleFrozenList(),\n    requires: Iterable[str] = SimpleFrozenList(),\n    retokenizes: bool = False,\n    default_score_weights: Dict[str, Optional[float]] = SimpleFrozenDict(),\n    func: Optional[Callable] = None,\n) -&gt; Callable:\n\"\"\"\n    Patched from spaCy to allow back dots in factory\n    names (https://github.com/aphp/edsnlp/pull/152)\n\n    Register a new pipeline component factory. Can be used as a decorator\n    on a function or classmethod, or called as a function with the factory\n    provided as the func keyword argument. To create a component and add\n    it to the pipeline, you can use nlp.add_pipe(name).\n\n    name (str): The name of the component factory.\n    default_config (Dict[str, Any]): Default configuration, describing the\n        default values of the factory arguments.\n    assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n        e.g. \"token.ent_id\". Used for pipeline analysis.\n    requires (Iterable[str]): Doc/Token attributes required by this component,\n        e.g. \"token.ent_id\". Used for pipeline analysis.\n    retokenizes (bool): Whether the component changes the tokenization.\n        Used for pipeline analysis.\n    default_score_weights (Dict[str, Optional[float]]): The scores to report during\n        training, and their default weight towards the final score used to\n        select the best model. Weights should sum to 1.0 per component and\n        will be combined and normalized for the whole pipeline. If None,\n        the score won't be shown in the logs or be weighted.\n    func (Optional[Callable]): Factory function if not used as a decorator.\n\n    DOCS: https://spacy.io/api/language#factory\n    \"\"\"\n    if not isinstance(name, str):\n        raise ValueError(Errors.E963.format(decorator=\"factory\"))\n    if not isinstance(default_config, dict):\n        err = Errors.E962.format(\n            style=\"default config\", name=name, cfg_type=type(default_config)\n        )\n        raise ValueError(err)\n\n    def add_factory(factory_func: Callable) -&gt; Callable:\n        internal_name = cls.get_factory_name(name)\n        if internal_name in registry.factories:\n            # We only check for the internal name here \u2013 it's okay if it's a\n            # subclass and the base class has a factory of the same name. We\n            # also only raise if the function is different to prevent raising\n            # if module is reloaded.\n            existing_func = registry.factories.get(internal_name)\n            if not util.is_same_func(factory_func, existing_func):\n                err = Errors.E004.format(\n                    name=name, func=existing_func, new_func=factory_func\n                )\n                raise ValueError(err)\n\n        arg_names = util.get_arg_names(factory_func)\n        if \"nlp\" not in arg_names or \"name\" not in arg_names:\n            raise ValueError(Errors.E964.format(name=name))\n        # Officially register the factory so we can later call\n        # registry.resolve and refer to it in the config as\n        # @factories = \"spacy.Language.xyz\". We use the class name here so\n        # different classes can have different factories.\n        registry.factories.register(internal_name, func=factory_func)\n        factory_meta = FactoryMeta(\n            factory=name,\n            default_config=default_config,\n            assigns=validate_attrs(assigns),\n            requires=validate_attrs(requires),\n            scores=list(default_score_weights.keys()),\n            default_score_weights=default_score_weights,\n            retokenizes=retokenizes,\n        )\n        cls.set_factory_meta(name, factory_meta)\n        # We're overwriting the class attr with a frozen dict to handle\n        # backwards-compat (writing to Language.factories directly). This\n        # wouldn't work with an instance property and just produce a\n        # confusing error \u2013 here we can show a custom error\n        cls.factories = SimpleFrozenDict(\n            registry.factories.get_all(), error=Errors.E957\n        )\n        return factory_func\n\n    if func is not None:  # Support non-decorator use cases\n        return add_factory(func)\n    return add_factory\n</code></pre>"},{"location":"reference/patch_spacy_dot_components/#edsnlp.patch_spacy_dot_components.component","title":"<code>component(cls, name, *, assigns=SimpleFrozenList(), requires=SimpleFrozenList(), retokenizes=False, func=None)</code>  <code>classmethod</code>","text":"<p>Patched from spaCy to allow back dots in factory names (https://github.com/aphp/edsnlp/pull/152)</p> <p>Register a new pipeline component. Can be used for stateless function components that don't require a separate factory. Can be used as a decorator on a function or classmethod, or called as a function with the factory provided as the func keyword argument. To create a component and add it to the pipeline, you can use nlp.add_pipe(name).</p> <p>name (str): The name of the component factory. assigns (Iterable[str]): Doc/Token attributes assigned by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. requires (Iterable[str]): Doc/Token attributes required by this component,     e.g. \"token.ent_id\". Used for pipeline analysis. retokenizes (bool): Whether the component changes the tokenization.     Used for pipeline analysis. func (Optional[Callable]): Factory function if not used as a decorator.</p> <p>DOCS: https://spacy.io/api/language#component</p> Source code in <code>edsnlp/patch_spacy_dot_components.py</code> <pre><code>@classmethod\ndef component(\n    cls,\n    name: str,\n    *,\n    assigns: Iterable[str] = SimpleFrozenList(),\n    requires: Iterable[str] = SimpleFrozenList(),\n    retokenizes: bool = False,\n    func: Optional[\"Pipe\"] = None,\n) -&gt; Callable[..., Any]:\n\"\"\"\n    Patched from spaCy to allow back dots in factory\n    names (https://github.com/aphp/edsnlp/pull/152)\n\n    Register a new pipeline component. Can be used for stateless function\n    components that don't require a separate factory. Can be used as a\n    decorator on a function or classmethod, or called as a function with the\n    factory provided as the func keyword argument. To create a component and\n    add it to the pipeline, you can use nlp.add_pipe(name).\n\n    name (str): The name of the component factory.\n    assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n        e.g. \"token.ent_id\". Used for pipeline analysis.\n    requires (Iterable[str]): Doc/Token attributes required by this component,\n        e.g. \"token.ent_id\". Used for pipeline analysis.\n    retokenizes (bool): Whether the component changes the tokenization.\n        Used for pipeline analysis.\n    func (Optional[Callable]): Factory function if not used as a decorator.\n\n    DOCS: https://spacy.io/api/language#component\n    \"\"\"\n    if name is not None:\n        if not isinstance(name, str):\n            raise ValueError(Errors.E963.format(decorator=\"component\"))\n    component_name = name if name is not None else util.get_object_name(func)\n\n    def add_component(component_func: \"Pipe\") -&gt; Callable:\n        if isinstance(func, type):  # function is a class\n            raise ValueError(Errors.E965.format(name=component_name))\n\n        def factory_func(nlp, name: str) -&gt; \"Pipe\":\n            return component_func\n\n        internal_name = cls.get_factory_name(name)\n        if internal_name in registry.factories:\n            # We only check for the internal name here \u2013 it's okay if it's a\n            # subclass and the base class has a factory of the same name. We\n            # also only raise if the function is different to prevent raising\n            # if module is reloaded. It's hacky, but we need to check the\n            # existing functure for a closure and whether that's identical\n            # to the component function (because factory_func created above\n            # will always be different, even for the same function)\n            existing_func = registry.factories.get(internal_name)\n            closure = existing_func.__closure__\n            wrapped = [c.cell_contents for c in closure][0] if closure else None\n            if util.is_same_func(wrapped, component_func):\n                factory_func = existing_func  # noqa: F811\n\n        cls.factory(\n            component_name,\n            assigns=assigns,\n            requires=requires,\n            retokenizes=retokenizes,\n            func=factory_func,\n        )\n        return component_func\n\n    if func is not None:  # Support non-decorator use cases\n        return add_component(func)\n    return add_component\n</code></pre>"},{"location":"reference/connectors/","title":"<code>edsnlp.connectors</code>","text":""},{"location":"reference/connectors/brat/","title":"<code>edsnlp.connectors.brat</code>","text":""},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector","title":"<code>BratConnector</code>","text":"<p>         Bases: <code>object</code></p> <p>Two-way connector with BRAT. Supports entities only.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Directory containing the BRAT files.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>n_jobs</code> <p>Number of jobs for multiprocessing, by default 1</p> <p> TYPE: <code>int, optional</code> DEFAULT: <code>1</code> </p> <code>attributes</code> <p>Mapping from BRAT attributes to spaCy Span extensions. Extensions / attributes that are not in the mapping are not imported or exported If left to None, the mapping is filled with all BRAT attributes.</p> <p> TYPE: <code>Optional[Union[Sequence[str], Mapping[str, str]]]</code> DEFAULT: <code>None</code> </p> <code>span_groups</code> <p>Additional span groups to look for entities in spaCy documents when exporting. Missing label (resp. span group) names are not imported (resp. exported) If left to None, the sequence is filled with all BRAT entity labels.</p> <p> TYPE: <code>Optional[Sequence[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>class BratConnector(object):\n\"\"\"\n    Two-way connector with BRAT. Supports entities only.\n\n    Parameters\n    ----------\n    directory : Union[str, Path]\n        Directory containing the BRAT files.\n    n_jobs : int, optional\n        Number of jobs for multiprocessing, by default 1\n    attributes: Optional[Union[Sequence[str], Mapping[str, str]]]\n        Mapping from BRAT attributes to spaCy Span extensions.\n        Extensions / attributes that are not in the mapping are not imported or exported\n        If left to None, the mapping is filled with all BRAT attributes.\n    span_groups: Optional[Sequence[str]]\n        Additional span groups to look for entities in spaCy documents when exporting.\n        Missing label (resp. span group) names are not imported (resp. exported)\n        If left to None, the sequence is filled with all BRAT entity labels.\n    \"\"\"\n\n    def __init__(\n        self,\n        directory: Union[str, Path],\n        n_jobs: int = 1,\n        attributes: Optional[Union[Sequence[str], Mapping[str, str]]] = None,\n        span_groups: Optional[Sequence[str]] = None,\n    ):\n        self.directory: Path = Path(directory)\n        self.n_jobs = n_jobs\n        if attributes is None:\n            self.attr_map = None\n        elif isinstance(attributes, (tuple, list)):\n            self.attr_map = {k: k for k in attributes}\n        elif isinstance(attributes, dict):\n            self.attr_map = attributes\n        else:\n            raise TypeError(\n                \"`attributes` should be a list, tuple or mapping of strings\"\n            )\n        self.span_groups = None if span_groups is None else tuple(span_groups)\n\n    def full_path(self, filename: str) -&gt; str:\n        return os.path.join(self.directory, filename)\n\n    def load_brat(self) -&gt; List[Dict]:\n\"\"\"\n        Transforms a BRAT folder to a list of spaCy documents.\n\n        Parameters\n        ----------\n        nlp:\n            A spaCy pipeline.\n\n        Returns\n        -------\n        docs:\n            List of spaCy documents, with annotations in the `ents` attribute.\n        \"\"\"\n        filenames = [\n            path.relative_to(self.directory) for path in self.directory.rglob(\"*.txt\")\n        ]\n\n        assert len(filenames), f\"BRAT directory {self.directory} is empty!\"\n\n        logger.info(\n            f\"The BRAT directory contains {len(filenames)} annotated documents.\"\n        )\n\n        def load_and_rename(filename):\n            res = load_from_brat(filename)\n            res[\"note_id\"] = str(Path(filename).relative_to(self.directory)).rsplit(\n                \".\", 1\n            )[0]\n            bar.update(1)\n            return res\n\n        bar = tqdm(\n            total=len(filenames), ascii=True, ncols=100, desc=\"Annotation extraction\"\n        )\n        with bar:\n            annotations = Parallel(n_jobs=self.n_jobs)(\n                delayed(load_and_rename)(self.full_path(filename))\n                for filename in filenames\n            )\n\n        return annotations\n\n    def brat2docs(self, nlp: Language, run_pipe=False) -&gt; List[Doc]:\n\"\"\"\n        Transforms a BRAT folder to a list of spaCy documents.\n\n        Parameters\n        ----------\n        nlp: Language\n            A spaCy pipeline.\n        run_pipe: bool\n            Should the full spaCy pipeline be run on the documents, or just the\n            tokenization (defaults to False ie only tokenization)\n\n        Returns\n        -------\n        docs:\n            List of spaCy documents, with annotations in the `ents` attribute.\n        \"\"\"\n\n        annotations = self.load_brat()\n\n        texts = [doc[\"text\"] for doc in annotations]\n\n        docs = []\n\n        if run_pipe:\n            gold_docs = nlp.pipe(texts, batch_size=50, n_process=self.n_jobs)\n        else:\n            gold_docs = (nlp.make_doc(t) for t in texts)\n\n        for doc, doc_annotations in tqdm(\n            zip(gold_docs, annotations),\n            ascii=True,\n            ncols=100,\n            desc=\"spaCy conversion\",\n            total=len(texts),\n        ):\n\n            doc._.note_id = doc_annotations[\"note_id\"]\n\n            spans = []\n            span_groups = defaultdict(lambda: [])\n\n            if self.attr_map is not None:\n                for dst in self.attr_map.values():\n                    if not Span.has_extension(dst):\n                        Span.set_extension(dst, default=None)\n\n            encountered_attributes = set()\n            for ent in doc_annotations[\"entities\"]:\n                if self.attr_map is None:\n                    for a in ent[\"attributes\"]:\n                        if not Span.has_extension(a[\"label\"]):\n                            Span.set_extension(a[\"label\"], default=None)\n                        encountered_attributes.add(a[\"label\"])\n\n                for fragment in ent[\"fragments\"]:\n                    span = doc.char_span(\n                        fragment[\"begin\"],\n                        fragment[\"end\"],\n                        label=ent[\"label\"],\n                        alignment_mode=\"expand\",\n                    )\n                    for a in ent[\"attributes\"]:\n                        if self.attr_map is None or a[\"label\"] in self.attr_map:\n                            new_name = (\n                                a[\"label\"]\n                                if self.attr_map is None\n                                else self.attr_map[a[\"label\"]]\n                            )\n                            span._.set(new_name, a[\"value\"] if a is not None else True)\n                    spans.append(span)\n\n                    if self.span_groups is None or ent[\"label\"] in self.span_groups:\n                        span_groups[ent[\"label\"]].append(span)\n\n            if self.attr_map is None:\n                self.attr_map = {k: k for k in encountered_attributes}\n\n            if self.span_groups is None:\n                self.span_groups = sorted(span_groups.keys())\n\n            doc.ents = filter_spans(spans)\n            for group_name, group in span_groups.items():\n                doc.spans[group_name] = group\n\n            docs.append(doc)\n\n        return docs\n\n    def doc2brat(self, doc: Doc) -&gt; None:\n\"\"\"\n        Writes a spaCy document to file in the BRAT directory.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file.\n        \"\"\"\n        filename = str(doc._.note_id)\n\n        if self.attr_map is None:\n            rattr_map = {}\n        else:\n            rattr_map = {v: k for k, v in self.attr_map.items()}\n\n        annotations = {\n            \"entities\": [\n                {\n                    \"entity_id\": i,\n                    \"fragments\": [\n                        {\n                            \"begin\": ent.start_char,\n                            \"end\": ent.end_char,\n                        }\n                    ],\n                    \"attributes\": [\n                        {\"label\": rattr_map[a], \"value\": getattr(ent._, a)}\n                        for a in rattr_map\n                        if getattr(ent._, a) is not None\n                    ],\n                    \"label\": ent.label_,\n                }\n                for i, ent in enumerate(\n                    sorted(\n                        {\n                            *doc.ents,\n                            *(\n                                span\n                                for name in doc.spans\n                                if self.span_groups is None or name in self.span_groups\n                                for span in doc.spans[name]\n                            ),\n                        }\n                    )\n                )\n            ],\n            \"text\": doc.text,\n        }\n        export_to_brat(\n            annotations,\n            self.full_path(f\"{filename}.txt\"),\n            overwrite_txt=False,\n            overwrite_ann=True,\n        )\n\n    def docs2brat(self, docs: List[Doc]) -&gt; None:\n\"\"\"\n        Writes a list of spaCy documents to file.\n\n        Parameters\n        ----------\n        docs:\n            List of spaCy documents.\n        \"\"\"\n        for doc in docs:\n            self.doc2brat(doc)\n\n    def get_brat(self) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n        Reads texts and annotations, and returns two DataFrame objects.\n        For backward compatibility\n\n        Returns\n        -------\n        texts:\n            A DataFrame containing two fields, `note_id` and `note_text`\n        annotations:\n            A DataFrame containing the annotations.\n        \"\"\"\n\n        brat = self.load_brat()\n\n        texts = pd.DataFrame(\n            [\n                {\n                    \"note_id\": doc[\"note_id\"],\n                    \"note_text\": doc[\"text\"],\n                }\n                for doc in brat\n            ]\n        )\n\n        annotations = pd.DataFrame(\n            [\n                {\n                    \"note_id\": doc[\"note_id\"],\n                    \"index\": i,\n                    \"begin\": f[\"begin\"],\n                    \"end\": f[\"end\"],\n                    \"label\": e[\"label\"],\n                    \"lexical_variant\": e[\"text\"],\n                }\n                for doc in brat\n                for i, e in enumerate(doc[\"entities\"])\n                for f in e[\"fragments\"]\n            ]\n        )\n\n        return texts, annotations\n</code></pre>"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.load_brat","title":"<code>load_brat()</code>","text":"<p>Transforms a BRAT folder to a list of spaCy documents.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>A spaCy pipeline.</p> <p> </p> RETURNS DESCRIPTION <code>docs</code> <p>List of spaCy documents, with annotations in the <code>ents</code> attribute.</p> <p> TYPE: <code>List[Dict]</code> </p> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>def load_brat(self) -&gt; List[Dict]:\n\"\"\"\n    Transforms a BRAT folder to a list of spaCy documents.\n\n    Parameters\n    ----------\n    nlp:\n        A spaCy pipeline.\n\n    Returns\n    -------\n    docs:\n        List of spaCy documents, with annotations in the `ents` attribute.\n    \"\"\"\n    filenames = [\n        path.relative_to(self.directory) for path in self.directory.rglob(\"*.txt\")\n    ]\n\n    assert len(filenames), f\"BRAT directory {self.directory} is empty!\"\n\n    logger.info(\n        f\"The BRAT directory contains {len(filenames)} annotated documents.\"\n    )\n\n    def load_and_rename(filename):\n        res = load_from_brat(filename)\n        res[\"note_id\"] = str(Path(filename).relative_to(self.directory)).rsplit(\n            \".\", 1\n        )[0]\n        bar.update(1)\n        return res\n\n    bar = tqdm(\n        total=len(filenames), ascii=True, ncols=100, desc=\"Annotation extraction\"\n    )\n    with bar:\n        annotations = Parallel(n_jobs=self.n_jobs)(\n            delayed(load_and_rename)(self.full_path(filename))\n            for filename in filenames\n        )\n\n    return annotations\n</code></pre>"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.brat2docs","title":"<code>brat2docs(nlp, run_pipe=False)</code>","text":"<p>Transforms a BRAT folder to a list of spaCy documents.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>A spaCy pipeline.</p> <p> TYPE: <code>Language</code> </p> <code>run_pipe</code> <p>Should the full spaCy pipeline be run on the documents, or just the tokenization (defaults to False ie only tokenization)</p> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>docs</code> <p>List of spaCy documents, with annotations in the <code>ents</code> attribute.</p> <p> TYPE: <code>List[Doc]</code> </p> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>def brat2docs(self, nlp: Language, run_pipe=False) -&gt; List[Doc]:\n\"\"\"\n    Transforms a BRAT folder to a list of spaCy documents.\n\n    Parameters\n    ----------\n    nlp: Language\n        A spaCy pipeline.\n    run_pipe: bool\n        Should the full spaCy pipeline be run on the documents, or just the\n        tokenization (defaults to False ie only tokenization)\n\n    Returns\n    -------\n    docs:\n        List of spaCy documents, with annotations in the `ents` attribute.\n    \"\"\"\n\n    annotations = self.load_brat()\n\n    texts = [doc[\"text\"] for doc in annotations]\n\n    docs = []\n\n    if run_pipe:\n        gold_docs = nlp.pipe(texts, batch_size=50, n_process=self.n_jobs)\n    else:\n        gold_docs = (nlp.make_doc(t) for t in texts)\n\n    for doc, doc_annotations in tqdm(\n        zip(gold_docs, annotations),\n        ascii=True,\n        ncols=100,\n        desc=\"spaCy conversion\",\n        total=len(texts),\n    ):\n\n        doc._.note_id = doc_annotations[\"note_id\"]\n\n        spans = []\n        span_groups = defaultdict(lambda: [])\n\n        if self.attr_map is not None:\n            for dst in self.attr_map.values():\n                if not Span.has_extension(dst):\n                    Span.set_extension(dst, default=None)\n\n        encountered_attributes = set()\n        for ent in doc_annotations[\"entities\"]:\n            if self.attr_map is None:\n                for a in ent[\"attributes\"]:\n                    if not Span.has_extension(a[\"label\"]):\n                        Span.set_extension(a[\"label\"], default=None)\n                    encountered_attributes.add(a[\"label\"])\n\n            for fragment in ent[\"fragments\"]:\n                span = doc.char_span(\n                    fragment[\"begin\"],\n                    fragment[\"end\"],\n                    label=ent[\"label\"],\n                    alignment_mode=\"expand\",\n                )\n                for a in ent[\"attributes\"]:\n                    if self.attr_map is None or a[\"label\"] in self.attr_map:\n                        new_name = (\n                            a[\"label\"]\n                            if self.attr_map is None\n                            else self.attr_map[a[\"label\"]]\n                        )\n                        span._.set(new_name, a[\"value\"] if a is not None else True)\n                spans.append(span)\n\n                if self.span_groups is None or ent[\"label\"] in self.span_groups:\n                    span_groups[ent[\"label\"]].append(span)\n\n        if self.attr_map is None:\n            self.attr_map = {k: k for k in encountered_attributes}\n\n        if self.span_groups is None:\n            self.span_groups = sorted(span_groups.keys())\n\n        doc.ents = filter_spans(spans)\n        for group_name, group in span_groups.items():\n            doc.spans[group_name] = group\n\n        docs.append(doc)\n\n    return docs\n</code></pre>"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.doc2brat","title":"<code>doc2brat(doc)</code>","text":"<p>Writes a spaCy document to file in the BRAT directory.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object. The spans in <code>ents</code> will populate the <code>note_id.ann</code> file.</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>def doc2brat(self, doc: Doc) -&gt; None:\n\"\"\"\n    Writes a spaCy document to file in the BRAT directory.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object. The spans in `ents` will populate the `note_id.ann` file.\n    \"\"\"\n    filename = str(doc._.note_id)\n\n    if self.attr_map is None:\n        rattr_map = {}\n    else:\n        rattr_map = {v: k for k, v in self.attr_map.items()}\n\n    annotations = {\n        \"entities\": [\n            {\n                \"entity_id\": i,\n                \"fragments\": [\n                    {\n                        \"begin\": ent.start_char,\n                        \"end\": ent.end_char,\n                    }\n                ],\n                \"attributes\": [\n                    {\"label\": rattr_map[a], \"value\": getattr(ent._, a)}\n                    for a in rattr_map\n                    if getattr(ent._, a) is not None\n                ],\n                \"label\": ent.label_,\n            }\n            for i, ent in enumerate(\n                sorted(\n                    {\n                        *doc.ents,\n                        *(\n                            span\n                            for name in doc.spans\n                            if self.span_groups is None or name in self.span_groups\n                            for span in doc.spans[name]\n                        ),\n                    }\n                )\n            )\n        ],\n        \"text\": doc.text,\n    }\n    export_to_brat(\n        annotations,\n        self.full_path(f\"{filename}.txt\"),\n        overwrite_txt=False,\n        overwrite_ann=True,\n    )\n</code></pre>"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.docs2brat","title":"<code>docs2brat(docs)</code>","text":"<p>Writes a list of spaCy documents to file.</p> PARAMETER DESCRIPTION <code>docs</code> <p>List of spaCy documents.</p> <p> TYPE: <code>List[Doc]</code> </p> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>def docs2brat(self, docs: List[Doc]) -&gt; None:\n\"\"\"\n    Writes a list of spaCy documents to file.\n\n    Parameters\n    ----------\n    docs:\n        List of spaCy documents.\n    \"\"\"\n    for doc in docs:\n        self.doc2brat(doc)\n</code></pre>"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.BratConnector.get_brat","title":"<code>get_brat()</code>","text":"<p>Reads texts and annotations, and returns two DataFrame objects. For backward compatibility</p> RETURNS DESCRIPTION <code>texts</code> <p>A DataFrame containing two fields, <code>note_id</code> and <code>note_text</code></p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>annotations</code> <p>A DataFrame containing the annotations.</p> <p> TYPE: <code>pd.DataFrame</code> </p> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>def get_brat(self) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Reads texts and annotations, and returns two DataFrame objects.\n    For backward compatibility\n\n    Returns\n    -------\n    texts:\n        A DataFrame containing two fields, `note_id` and `note_text`\n    annotations:\n        A DataFrame containing the annotations.\n    \"\"\"\n\n    brat = self.load_brat()\n\n    texts = pd.DataFrame(\n        [\n            {\n                \"note_id\": doc[\"note_id\"],\n                \"note_text\": doc[\"text\"],\n            }\n            for doc in brat\n        ]\n    )\n\n    annotations = pd.DataFrame(\n        [\n            {\n                \"note_id\": doc[\"note_id\"],\n                \"index\": i,\n                \"begin\": f[\"begin\"],\n                \"end\": f[\"end\"],\n                \"label\": e[\"label\"],\n                \"lexical_variant\": e[\"text\"],\n            }\n            for doc in brat\n            for i, e in enumerate(doc[\"entities\"])\n            for f in e[\"fragments\"]\n        ]\n    )\n\n    return texts, annotations\n</code></pre>"},{"location":"reference/connectors/brat/#edsnlp.connectors.brat.load_from_brat","title":"<code>load_from_brat(path, merge_spaced_fragments=True)</code>","text":"<p>Load a brat file</p> <p>Adapted from https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py</p> PARAMETER DESCRIPTION <code>path</code> <p>Path or glob path of the brat text file (.txt, not .ann)</p> <p> TYPE: <code>str</code> </p> <code>merge_spaced_fragments</code> <p>Merge fragments of a entity that was splitted by brat because it overlapped an end of line</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Iterator[Dict]</code> Source code in <code>edsnlp/connectors/brat.py</code> <pre><code>def load_from_brat(path: str, merge_spaced_fragments: bool = True) -&gt; Dict:\n\"\"\"\n    Load a brat file\n\n    Adapted from\n    https://github.com/percevalw/nlstruct/blob/master/nlstruct/datasets/brat.py\n\n    Parameters\n    ----------\n    path: str\n        Path or glob path of the brat text file (.txt, not .ann)\n    merge_spaced_fragments: bool\n        Merge fragments of a entity that was splitted by brat because it overlapped an\n        end of line\n\n    Returns\n    -------\n    Iterator[Dict]\n    \"\"\"\n    ann_filenames = []\n    for filename in glob.glob(path.replace(\".txt\", \".a*\"), recursive=True):\n        ann_filenames.append(filename)\n\n    entities = {}\n    relations = []\n    events = {}\n\n    # doc_id = filename.replace('.txt', '').split(\"/\")[-1]\n\n    with open(path) as f:\n        text = f.read()\n\n    note_id = path.split(\"/\")[-1].rsplit(\".\", 1)[0]\n\n    if not len(ann_filenames):\n        return {\n            \"note_id\": note_id,\n            \"text\": text,\n        }\n\n    for ann_file in ann_filenames:\n        with open(ann_file) as f:\n            for line_idx, line in enumerate(f):\n                try:\n                    if line.startswith(\"T\"):\n                        match = REGEX_ENTITY.match(line)\n                        if match is None:\n                            raise BratParsingError(ann_file, line)\n                        ann_id = match.group(1)\n                        entity = match.group(2)\n                        span = match.group(3)\n                        mention_text = match.group(4)\n                        entities[ann_id] = {\n                            \"text\": mention_text,\n                            \"entity_id\": ann_id,\n                            \"fragments\": [],\n                            \"attributes\": [],\n                            \"comments\": [],\n                            \"label\": entity,\n                        }\n                        last_end = None\n                        fragment_i = 0\n                        begins_ends = sorted(\n                            [\n                                (int(s.split()[0]), int(s.split()[1]))\n                                for s in span.split(\";\")\n                            ]\n                        )\n\n                        for begin, end in begins_ends:\n                            # If merge_spaced_fragments, merge two fragments that are\n                            # only separated by a newline (brat automatically creates\n                            # multiple fragments for a entity that spans over more than\n                            # one line)\n                            if (\n                                merge_spaced_fragments\n                                and last_end is not None\n                                and len(text[last_end:begin].strip()) == 0\n                            ):\n                                entities[ann_id][\"fragments\"][-1][\"end\"] = end\n                                last_end = end\n                                continue\n                            entities[ann_id][\"fragments\"].append(\n                                {\n                                    \"begin\": begin,\n                                    \"end\": end,\n                                }\n                            )\n                            fragment_i += 1\n                            last_end = end\n                    elif line.startswith(\"A\") or line.startswith(\"M\"):\n                        match = REGEX_ATTRIBUTE.match(line)\n                        if match is None:\n                            raise BratParsingError(ann_file, line)\n                        ann_id = match.group(1)\n                        parts = match.group(2).split(\" \")\n                        if len(parts) &gt;= 3:\n                            entity, entity_id, value = parts\n                        elif len(parts) == 2:\n                            entity, entity_id = parts\n                            value = None\n                        else:\n                            raise BratParsingError(ann_file, line)\n                        (\n                            entities[entity_id]\n                            if entity_id.startswith(\"T\")\n                            else events[entity_id]\n                        )[\"attributes\"].append(\n                            {\n                                \"attribute_id\": ann_id,\n                                \"label\": entity,\n                                \"value\": value,\n                            }\n                        )\n                    elif line.startswith(\"R\"):\n                        match = REGEX_RELATION.match(line)\n                        if match is None:\n                            raise BratParsingError(ann_file, line)\n                        ann_id = match.group(1)\n                        ann_name = match.group(2)\n                        arg1 = match.group(3)\n                        arg2 = match.group(4)\n                        relations.append(\n                            {\n                                \"relation_id\": ann_id,\n                                \"relation_label\": ann_name,\n                                \"from_entity_id\": arg1,\n                                \"to_entity_id\": arg2,\n                            }\n                        )\n                    elif line.startswith(\"E\"):\n                        match = REGEX_EVENT.match(line)\n                        if match is None:\n                            raise BratParsingError(ann_file, line)\n                        ann_id = match.group(1)\n                        arguments_txt = match.group(2)\n                        arguments = []\n                        for argument in REGEX_EVENT_PART.finditer(arguments_txt):\n                            arguments.append(\n                                {\n                                    \"entity_id\": argument.group(2),\n                                    \"label\": argument.group(1),\n                                }\n                            )\n                        events[ann_id] = {\n                            \"event_id\": ann_id,\n                            \"attributes\": [],\n                            \"arguments\": arguments,\n                        }\n                    elif line.startswith(\"#\"):\n                        match = REGEX_NOTE.match(line)\n                        if match is None:\n                            raise BratParsingError(ann_file, line)\n                        ann_id = match.group(1)\n                        entity_id = match.group(2)\n                        comment = match.group(3)\n                        entities[entity_id][\"comments\"].append(\n                            {\n                                \"comment_id\": ann_id,\n                                \"comment\": comment,\n                            }\n                        )\n                except Exception:\n                    raise Exception(\n                        \"Could not parse line {} from {}: {}\".format(\n                            line_idx, filename.replace(\".txt\", \".ann\"), repr(line)\n                        )\n                    )\n    return {\n        \"note_id\": note_id,\n        \"text\": text,\n        \"entities\": list(entities.values()),\n        \"relations\": relations,\n        \"events\": list(events.values()),\n    }\n</code></pre>"},{"location":"reference/connectors/labeltool/","title":"<code>edsnlp.connectors.labeltool</code>","text":""},{"location":"reference/connectors/labeltool/#edsnlp.connectors.labeltool.docs2labeltool","title":"<code>docs2labeltool(docs, extensions=None)</code>","text":"<p>Returns a labeltool-ready dataframe from a list of annotated document.</p> PARAMETER DESCRIPTION <code>docs</code> <p>List of annotated spacy docs.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>List of extensions to use by labeltool.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>df</code> <p>DataFrame tailored for labeltool.</p> <p> TYPE: <code>pd.DataFrame</code> </p> Source code in <code>edsnlp/connectors/labeltool.py</code> <pre><code>def docs2labeltool(\n    docs: List[Doc],\n    extensions: Optional[List[str]] = None,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Returns a labeltool-ready dataframe from a list of annotated document.\n\n    Parameters\n    ----------\n    docs: list of spaCy Doc\n        List of annotated spacy docs.\n    extensions: list of extensions\n        List of extensions to use by labeltool.\n\n    Returns\n    -------\n    df: pd.DataFrame\n        DataFrame tailored for labeltool.\n    \"\"\"\n\n    if extensions is None:\n        extensions = []\n\n    entities = []\n\n    for i, doc in enumerate(tqdm(docs, ascii=True, ncols=100)):\n        for ent in doc.ents:\n            d = dict(\n                note_text=doc.text,\n                offset_begin=ent.start_char,\n                offset_end=ent.end_char,\n                label_name=ent.label_,\n                label_value=ent.text,\n            )\n\n            d[\"note_id\"] = doc._.note_id or i\n\n            for ext in extensions:\n                d[ext] = getattr(ent._, ext)\n\n            entities.append(d)\n\n    df = pd.DataFrame.from_records(entities)\n\n    columns = [\n        \"note_id\",\n        \"note_text\",\n        \"offset_begin\",\n        \"offset_end\",\n        \"label_name\",\n        \"label_value\",\n    ]\n\n    df = df[columns + extensions]\n\n    return df\n</code></pre>"},{"location":"reference/connectors/omop/","title":"<code>edsnlp.connectors.omop</code>","text":""},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector","title":"<code>OmopConnector</code>","text":"<p>         Bases: <code>object</code></p> <p>[summary]</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy language object.</p> <p> TYPE: <code>Language</code> </p> <code>start_char</code> <p>Name of the column containing the start character index of the entity, by default \"start_char\"</p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'start_char'</code> </p> <code>end_char</code> <p>Name of the column containing the end character index of the entity, by default \"end_char\"</p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'end_char'</code> </p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>class OmopConnector(object):\n\"\"\"\n    [summary]\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy language object.\n    start_char : str, optional\n        Name of the column containing the start character index of the entity,\n        by default \"start_char\"\n    end_char : str, optional\n        Name of the column containing the end character index of the entity,\n        by default \"end_char\"\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        start_char: str = \"start_char\",\n        end_char: str = \"end_char\",\n    ):\n\n        self.start_char = start_char\n        self.end_char = end_char\n\n        self.nlp = nlp\n\n    def preprocess(\n        self, note: pd.DataFrame, note_nlp: pd.DataFrame\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n        Preprocess the input OMOP tables: modification of the column names.\n\n        Parameters\n        ----------\n        note : pd.DataFrame\n            OMOP `note` table.\n        note_nlp : pd.DataFrame\n            OMOP `note_nlp` table.\n\n        Returns\n        -------\n        note : pd.DataFrame\n            OMOP `note` table.\n        note_nlp : pd.DataFrame\n            OMOP `note_nlp` table.\n        \"\"\"\n\n        note_nlp = note_nlp.rename(\n            columns={\n                self.start_char: \"start_char\",\n                self.end_char: \"end_char\",\n            }\n        )\n\n        return note, note_nlp\n\n    def postprocess(\n        self, note: pd.DataFrame, note_nlp: pd.DataFrame\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n        Postprocess the input OMOP tables: modification of the column names.\n\n        Parameters\n        ----------\n        note : pd.DataFrame\n            OMOP `note` table.\n        note_nlp : pd.DataFrame\n            OMOP `note_nlp` table.\n\n        Returns\n        -------\n        note : pd.DataFrame\n            OMOP `note` table.\n        note_nlp : pd.DataFrame\n            OMOP `note_nlp` table.\n        \"\"\"\n\n        note_nlp = note_nlp.rename(\n            columns={\n                \"start_char\": self.start_char,\n                \"end_char\": self.end_char,\n            }\n        )\n\n        return note, note_nlp\n\n    def omop2docs(\n        self,\n        note: pd.DataFrame,\n        note_nlp: pd.DataFrame,\n        extensions: Optional[List[str]] = None,\n    ) -&gt; List[Doc]:\n\"\"\"\n        Transforms OMOP tables to a list of spaCy documents.\n\n        Parameters\n        ----------\n        note : pd.DataFrame\n            OMOP `note` table.\n        note_nlp : pd.DataFrame\n            OMOP `note_nlp` table.\n        extensions : Optional[List[str]], optional\n            Extensions to keep, by default None\n\n        Returns\n        -------\n        List[Doc]\n            List of spaCy documents.\n        \"\"\"\n        note, note_nlp = self.preprocess(note, note_nlp)\n        return omop2docs(note, note_nlp, self.nlp, extensions)\n\n    def docs2omop(\n        self,\n        docs: List[Doc],\n        extensions: Optional[List[str]] = None,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n        Transforms a list of spaCy documents to a pair of OMOP tables.\n\n        Parameters\n        ----------\n        docs : List[Doc]\n            List of spaCy documents.\n        extensions : Optional[List[str]], optional\n            Extensions to keep, by default None\n\n        Returns\n        -------\n        note : pd.DataFrame\n            OMOP `note` table.\n        note_nlp : pd.DataFrame\n            OMOP `note_nlp` table.\n        \"\"\"\n        note, note_nlp = docs2omop(docs, extensions=extensions)\n        note, note_nlp = self.postprocess(note, note_nlp)\n        return note, note_nlp\n</code></pre>"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.preprocess","title":"<code>preprocess(note, note_nlp)</code>","text":"<p>Preprocess the input OMOP tables: modification of the column names.</p> PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>def preprocess(\n    self, note: pd.DataFrame, note_nlp: pd.DataFrame\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Preprocess the input OMOP tables: modification of the column names.\n\n    Parameters\n    ----------\n    note : pd.DataFrame\n        OMOP `note` table.\n    note_nlp : pd.DataFrame\n        OMOP `note_nlp` table.\n\n    Returns\n    -------\n    note : pd.DataFrame\n        OMOP `note` table.\n    note_nlp : pd.DataFrame\n        OMOP `note_nlp` table.\n    \"\"\"\n\n    note_nlp = note_nlp.rename(\n        columns={\n            self.start_char: \"start_char\",\n            self.end_char: \"end_char\",\n        }\n    )\n\n    return note, note_nlp\n</code></pre>"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.postprocess","title":"<code>postprocess(note, note_nlp)</code>","text":"<p>Postprocess the input OMOP tables: modification of the column names.</p> PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>def postprocess(\n    self, note: pd.DataFrame, note_nlp: pd.DataFrame\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Postprocess the input OMOP tables: modification of the column names.\n\n    Parameters\n    ----------\n    note : pd.DataFrame\n        OMOP `note` table.\n    note_nlp : pd.DataFrame\n        OMOP `note_nlp` table.\n\n    Returns\n    -------\n    note : pd.DataFrame\n        OMOP `note` table.\n    note_nlp : pd.DataFrame\n        OMOP `note_nlp` table.\n    \"\"\"\n\n    note_nlp = note_nlp.rename(\n        columns={\n            \"start_char\": self.start_char,\n            \"end_char\": self.end_char,\n        }\n    )\n\n    return note, note_nlp\n</code></pre>"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.omop2docs","title":"<code>omop2docs(note, note_nlp, extensions=None)</code>","text":"<p>Transforms OMOP tables to a list of spaCy documents.</p> PARAMETER DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]], optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Doc]</code> <p>List of spaCy documents.</p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>def omop2docs(\n    self,\n    note: pd.DataFrame,\n    note_nlp: pd.DataFrame,\n    extensions: Optional[List[str]] = None,\n) -&gt; List[Doc]:\n\"\"\"\n    Transforms OMOP tables to a list of spaCy documents.\n\n    Parameters\n    ----------\n    note : pd.DataFrame\n        OMOP `note` table.\n    note_nlp : pd.DataFrame\n        OMOP `note_nlp` table.\n    extensions : Optional[List[str]], optional\n        Extensions to keep, by default None\n\n    Returns\n    -------\n    List[Doc]\n        List of spaCy documents.\n    \"\"\"\n    note, note_nlp = self.preprocess(note, note_nlp)\n    return omop2docs(note, note_nlp, self.nlp, extensions)\n</code></pre>"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.OmopConnector.docs2omop","title":"<code>docs2omop(docs, extensions=None)</code>","text":"<p>Transforms a list of spaCy documents to a pair of OMOP tables.</p> PARAMETER DESCRIPTION <code>docs</code> <p>List of spaCy documents.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]], optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>note</code> <p>OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>OMOP <code>note_nlp</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>def docs2omop(\n    self,\n    docs: List[Doc],\n    extensions: Optional[List[str]] = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Transforms a list of spaCy documents to a pair of OMOP tables.\n\n    Parameters\n    ----------\n    docs : List[Doc]\n        List of spaCy documents.\n    extensions : Optional[List[str]], optional\n        Extensions to keep, by default None\n\n    Returns\n    -------\n    note : pd.DataFrame\n        OMOP `note` table.\n    note_nlp : pd.DataFrame\n        OMOP `note_nlp` table.\n    \"\"\"\n    note, note_nlp = docs2omop(docs, extensions=extensions)\n    note, note_nlp = self.postprocess(note, note_nlp)\n    return note, note_nlp\n</code></pre>"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.omop2docs","title":"<code>omop2docs(note, note_nlp, nlp, extensions=None)</code>","text":"<p>Transforms an OMOP-formatted pair of dataframes into a list of documents.</p> PARAMETER DESCRIPTION <code>note</code> <p>The OMOP <code>note</code> table.</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>note_nlp</code> <p>The OMOP <code>note_nlp</code> table</p> <p> TYPE: <code>pd.DataFrame</code> </p> <code>nlp</code> <p>spaCy language object.</p> <p> TYPE: <code>Language</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]], optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Doc]</code> <p>List of spaCy documents</p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>def omop2docs(\n    note: pd.DataFrame,\n    note_nlp: pd.DataFrame,\n    nlp: Language,\n    extensions: Optional[List[str]] = None,\n) -&gt; List[Doc]:\n\"\"\"\n    Transforms an OMOP-formatted pair of dataframes into a list of documents.\n\n    Parameters\n    ----------\n    note : pd.DataFrame\n        The OMOP `note` table.\n    note_nlp : pd.DataFrame\n        The OMOP `note_nlp` table\n    nlp : Language\n        spaCy language object.\n    extensions : Optional[List[str]], optional\n        Extensions to keep, by default None\n\n    Returns\n    -------\n    List[Doc] :\n        List of spaCy documents\n    \"\"\"\n\n    note = note.copy()\n    note_nlp = note_nlp.copy()\n\n    extensions = extensions or []\n\n    def row2ent(row):\n        d = dict(\n            start_char=row.start_char,\n            end_char=row.end_char,\n            label=row.get(\"note_nlp_source_value\"),\n            extensions={ext: row.get(ext) for ext in extensions},\n        )\n\n        return d\n\n    # Create entities\n    note_nlp[\"ents\"] = note_nlp.apply(row2ent, axis=1)\n\n    note_nlp = note_nlp.groupby(\"note_id\", as_index=False)[\"ents\"].agg(list)\n\n    note = note.merge(note_nlp, on=[\"note_id\"], how=\"left\")\n\n    # Generate documents\n    note[\"doc\"] = note.note_text.apply(nlp)\n\n    # Process documents\n    for _, row in note.iterrows():\n\n        doc = row.doc\n        doc._.note_id = row.note_id\n        doc._.note_datetime = row.get(\"note_datetime\")\n\n        ents = []\n\n        if not isinstance(row.ents, list):\n            continue\n\n        for ent in row.ents:\n\n            span = doc.char_span(\n                ent[\"start_char\"],\n                ent[\"end_char\"],\n                ent[\"label\"],\n                alignment_mode=\"expand\",\n            )\n\n            for k, v in ent[\"extensions\"].items():\n                setattr(span._, k, v)\n\n            ents.append(span)\n\n            if span.label_ not in doc.spans:\n                doc.spans[span.label_] = [span]\n            else:\n                doc.spans[span.label_].append(span)\n\n        ents, discarded = filter_spans(ents, return_discarded=True)\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n    return list(note.doc)\n</code></pre>"},{"location":"reference/connectors/omop/#edsnlp.connectors.omop.docs2omop","title":"<code>docs2omop(docs, extensions=None)</code>","text":"<p>Transforms a list of spaCy docs to a pair of OMOP tables.</p> PARAMETER DESCRIPTION <code>docs</code> <p>List of documents to transform.</p> <p> TYPE: <code>List[Doc]</code> </p> <code>extensions</code> <p>Extensions to keep, by default None</p> <p> TYPE: <code>Optional[List[str]], optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[pd.DataFrame, pd.DataFrame]</code> <p>Pair of OMOP tables (<code>note</code> and <code>note_nlp</code>)</p> Source code in <code>edsnlp/connectors/omop.py</code> <pre><code>def docs2omop(\n    docs: List[Doc],\n    extensions: Optional[List[str]] = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n\"\"\"\n    Transforms a list of spaCy docs to a pair of OMOP tables.\n\n    Parameters\n    ----------\n    docs : List[Doc]\n        List of documents to transform.\n    extensions : Optional[List[str]], optional\n        Extensions to keep, by default None\n\n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        Pair of OMOP tables (`note` and `note_nlp`)\n    \"\"\"\n\n    df = pd.DataFrame(dict(doc=docs))\n\n    df[\"note_text\"] = df.doc.apply(lambda doc: doc.text)\n    df[\"note_id\"] = df.doc.apply(lambda doc: doc._.note_id)\n    df[\"note_datetime\"] = df.doc.apply(lambda doc: doc._.note_datetime)\n\n    if df.note_id.isna().any():\n        df[\"note_id\"] = range(len(df))\n\n    df[\"ents\"] = df.doc.apply(lambda doc: list(doc.ents))\n    df[\"ents\"] += df.doc.apply(lambda doc: list(doc.spans[\"discarded\"]))\n\n    note = df[[\"note_id\", \"note_text\", \"note_datetime\"]]\n\n    df = df[[\"note_id\", \"ents\"]].explode(\"ents\")\n\n    extensions = extensions or []\n\n    def ent2dict(\n        ent: Span,\n    ) -&gt; Dict[str, Any]:\n\n        d = dict(\n            start_char=ent.start_char,\n            end_char=ent.end_char,\n            note_nlp_source_value=ent.label_,\n            lexical_variant=ent.text,\n            # normalized_variant=ent._.normalized.text,\n        )\n\n        for ext in extensions:\n            d[ext] = getattr(ent._, ext)\n\n        return d\n\n    df[\"ents\"] = df.ents.apply(ent2dict)\n\n    columns = [\n        \"start_char\",\n        \"end_char\",\n        \"note_nlp_source_value\",\n        \"lexical_variant\",\n        # \"normalized_variant\",\n    ]\n    columns += extensions\n\n    df[columns] = df.ents.apply(pd.Series)\n\n    df[\"term_modifiers\"] = \"\"\n\n    for i, ext in enumerate(extensions):\n        if i &gt; 0:\n            df.term_modifiers += \";\"\n        df.term_modifiers += ext + \"=\" + df[ext].astype(str)\n\n    df[\"note_nlp_id\"] = range(len(df))\n\n    note_nlp = df[[\"note_nlp_id\", \"note_id\"] + columns]\n\n    return note, note_nlp\n</code></pre>"},{"location":"reference/matchers/","title":"<code>edsnlp.matchers</code>","text":""},{"location":"reference/matchers/regex/","title":"<code>edsnlp.matchers.regex</code>","text":""},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher","title":"<code>RegexMatcher</code>","text":"<p>         Bases: <code>object</code></p> <p>Simple RegExp matcher.</p> PARAMETER DESCRIPTION <code>alignment_mode</code> <p>How spans should be aligned with tokens. Possible values are <code>strict</code> (character indices must be aligned with token boundaries), \"contract\" (span of all tokens completely within the character span), \"expand\" (span of all tokens at least partially covered by the character span). Defaults to <code>expand</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'expand'</code> </p> <code>attr</code> <p>Default attribute to match on, by default \"TEXT\". Can be overiden in the <code>add</code> method.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>flags</code> <p>Additional flags provided to the <code>re</code> module. Can be overiden in the <code>add</code> method.</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>ignore_excluded</code> <p>Whether to skip exclusions</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>span_from_group</code> <p>If set to <code>False</code>, will create spans basede on the regex's full match. If set to <code>True</code>, will use the first matching capturing group as a span (and fall back to using the full match if no capturing group is matching)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>class RegexMatcher(object):\n\"\"\"\n    Simple RegExp matcher.\n\n    Parameters\n    ----------\n    alignment_mode : str\n        How spans should be aligned with tokens.\n        Possible values are `strict` (character indices must be aligned\n        with token boundaries), \"contract\" (span of all tokens completely\n        within the character span), \"expand\" (span of all tokens at least\n        partially covered by the character span).\n        Defaults to `expand`.\n    attr : str\n        Default attribute to match on, by default \"TEXT\".\n        Can be overiden in the `add` method.\n    flags : Union[re.RegexFlag, int]\n        Additional flags provided to the `re` module.\n        Can be overiden in the `add` method.\n    ignore_excluded : bool\n        Whether to skip exclusions\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n\n        You won't be able to match on newlines if this is enabled and\n        the \"spaces\"/\"newline\" option of `eds.normalizer` is enabled (by default).\n    span_from_group : bool\n        If set to `False`, will create spans basede on the regex's full match.\n        If set to `True`, will use the first matching capturing group as a span\n        (and fall back to using the full match if no capturing group is matching)\n    \"\"\"\n\n    def __init__(\n        self,\n        alignment_mode: str = \"expand\",\n        attr: str = \"TEXT\",\n        ignore_excluded: bool = False,\n        ignore_space_tokens: bool = False,\n        flags: Union[re.RegexFlag, int] = 0,  # No additional flags\n        span_from_group: bool = False,\n    ):\n        self.alignment_mode = alignment_mode\n        self.regex = []\n\n        self.default_attr = attr\n\n        self.flags = flags\n        self.span_from_group = span_from_group\n\n        self.ignore_excluded = ignore_excluded\n        self.ignore_space_tokens = ignore_space_tokens\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls):\n        if not Span.has_extension(\"normalized_variant\"):\n            Span.set_extension(\"normalized_variant\", getter=get_normalized_variant)\n\n    def build_patterns(self, regex: Patterns):\n\"\"\"\n        Build patterns and adds them for matching.\n        Helper function for pipelines using this matcher.\n\n        Parameters\n        ----------\n        regex : Patterns\n            Dictionary of label/terms, or label/dictionary of terms/attribute.\n        \"\"\"\n        if not regex:\n            regex = dict()\n\n        for key, patterns in regex.items():\n            if isinstance(patterns, dict):\n                attr = patterns.get(\"attr\")\n                alignment_mode = patterns.get(\"alignment_mode\")\n                flags = patterns.get(\"flags\")\n                patterns = patterns.get(\"regex\")\n            else:\n                attr = None\n                alignment_mode = None\n                flags = None\n\n            if isinstance(patterns, str):\n                patterns = [patterns]\n\n            self.add(\n                key=key,\n                patterns=patterns,\n                attr=attr,\n                alignment_mode=alignment_mode,\n                flags=flags,\n            )\n\n    def add(\n        self,\n        key: str,\n        patterns: List[str],\n        attr: Optional[str] = None,\n        ignore_excluded: Optional[bool] = None,\n        ignore_space_tokens: Optional[bool] = None,\n        alignment_mode: Optional[str] = None,\n        flags: Optional[re.RegexFlag] = None,\n    ):\n\"\"\"\n        Add a pattern to the registry.\n\n        Parameters\n        ----------\n        key : str\n            Key of the new/updated pattern.\n        patterns : List[str]\n            List of patterns to add.\n        attr : Optional[str]\n            Attribute to use for matching.\n            By default, uses the `default_attr` attribute\n        ignore_excluded : Optional[bool]\n            Whether to skip excluded tokens during matching.\n        ignore_space_tokens: Optional[bool]\n            Whether to skip space tokens during matching.\n\n            You won't be able to match on newlines if this is enabled and\n            the \"spaces\"/\"newline\" option of `eds.normalizer` is enabled (by default).\n\n        alignment_mode : Optional[str]\n            Overwrite alignment mode.\n        \"\"\"\n\n        if attr is None:\n            attr = self.default_attr\n\n        if ignore_excluded is None:\n            ignore_excluded = self.ignore_excluded\n\n        if ignore_space_tokens is None:\n            ignore_space_tokens = self.ignore_space_tokens\n\n        if alignment_mode is None:\n            alignment_mode = self.alignment_mode\n\n        if flags is None:\n            flags = self.flags\n\n        patterns = [compile_regex(pattern, flags) for pattern in patterns]\n\n        self.regex.append(\n            (\n                key,\n                patterns,\n                attr,\n                ignore_excluded,\n                ignore_space_tokens,\n                alignment_mode,\n            )\n        )\n\n    def remove(\n        self,\n        key: str,\n    ):\n\"\"\"\n        Remove a pattern for the registry.\n\n        Parameters\n        ----------\n        key : str\n            key of the pattern to remove.\n\n        Raises\n        ------\n        ValueError\n            If the key is not present in the registered patterns.\n        \"\"\"\n        n = len(self.regex)\n        self.regex = [pat for pat in self.regex if pat[0] != key]\n        if len(self.regex) == n:\n            raise ValueError(f\"`{key}` is not referenced in the matcher\")\n\n    def __len__(self):\n        return len(set([regex[0] for regex in self.regex]))\n\n    def match(\n        self,\n        doclike: Union[Doc, Span],\n    ) -&gt; Tuple[Span, re.Match]:\n\"\"\"\n        Iterates on the matches.\n\n        Parameters\n        ----------\n        doclike:\n            spaCy Doc or Span object to match on.\n\n        Yields\n        -------\n        span:\n            A match.\n        \"\"\"\n\n        for (\n            key,\n            patterns,\n            attr,\n            ignore_excluded,\n            ignore_space_tokens,\n            alignment_mode,\n        ) in self.regex:\n            text = get_text(doclike, attr, ignore_excluded, ignore_space_tokens)\n\n            for pattern in patterns:\n                for match in pattern.finditer(text):\n                    logger.trace(f\"Matched a regex from {key}: {repr(match.group())}\")\n\n                    start_char, end_char = span_from_match(\n                        match=match,\n                        span_from_group=self.span_from_group,\n                    )\n\n                    span = create_span(\n                        doclike=doclike,\n                        start_char=start_char,\n                        end_char=end_char,\n                        key=key,\n                        attr=attr,\n                        alignment_mode=alignment_mode,\n                        ignore_excluded=ignore_excluded,\n                        ignore_space_tokens=ignore_space_tokens,\n                    )\n\n                    if span is None:\n                        continue\n\n                    yield span, match\n\n    def match_with_groupdict_as_spans(\n        self,\n        doclike: Union[Doc, Span],\n    ) -&gt; Tuple[Span, Dict[str, Span]]:\n\"\"\"\n        Iterates on the matches.\n\n        Parameters\n        ----------\n        doclike:\n            spaCy Doc or Span object to match on.\n\n        Yields\n        -------\n        span:\n            A match.\n        \"\"\"\n\n        for (\n            key,\n            patterns,\n            attr,\n            ignore_excluded,\n            ignore_space_tokens,\n            alignment_mode,\n        ) in self.regex:\n            text = get_text(doclike, attr, ignore_excluded, ignore_space_tokens)\n\n            for pattern in patterns:\n                for match in pattern.finditer(text):\n                    logger.trace(f\"Matched a regex from {key}: {repr(match.group())}\")\n\n                    start_char, end_char = span_from_match(\n                        match=match,\n                        span_from_group=self.span_from_group,\n                    )\n\n                    span = create_span(\n                        doclike=doclike,\n                        start_char=start_char,\n                        end_char=end_char,\n                        key=key,\n                        attr=attr,\n                        alignment_mode=alignment_mode,\n                        ignore_excluded=ignore_excluded,\n                        ignore_space_tokens=ignore_space_tokens,\n                    )\n                    group_spans = {}\n                    for group_key, group_string in match.groupdict().items():\n                        if group_string:\n                            group_spans[group_key] = create_span(\n                                doclike=doclike,\n                                start_char=match.start(group_key),\n                                end_char=match.end(group_key),\n                                key=group_key,\n                                attr=attr,\n                                alignment_mode=alignment_mode,\n                                ignore_excluded=ignore_excluded,\n                                ignore_space_tokens=ignore_space_tokens,\n                            )\n\n                    yield span, group_spans\n\n    def __call__(\n        self,\n        doclike: Union[Doc, Span],\n        as_spans=False,\n        return_groupdict=False,\n    ) -&gt; Union[Span, Tuple[Span, Dict[str, Any]]]:\n\"\"\"\n        Performs matching. Yields matches.\n\n        Parameters\n        ----------\n        doclike:\n            spaCy Doc or Span object.\n        as_spans:\n            Returns matches as spans.\n\n        Yields\n        ------\n        span:\n            A match.\n        groupdict:\n            Additional information coming from the named patterns\n            in the regular expression.\n        \"\"\"\n        for span, match in self.match(doclike):\n            if not as_spans:\n                offset = doclike[0].i\n                span = (span.label, span.start - offset, span.end - offset)\n            if return_groupdict:\n                yield span, match.groupdict()\n            else:\n                yield span\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.build_patterns","title":"<code>build_patterns(regex)</code>","text":"<p>Build patterns and adds them for matching. Helper function for pipelines using this matcher.</p> PARAMETER DESCRIPTION <code>regex</code> <p>Dictionary of label/terms, or label/dictionary of terms/attribute.</p> <p> TYPE: <code>Patterns</code> </p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def build_patterns(self, regex: Patterns):\n\"\"\"\n    Build patterns and adds them for matching.\n    Helper function for pipelines using this matcher.\n\n    Parameters\n    ----------\n    regex : Patterns\n        Dictionary of label/terms, or label/dictionary of terms/attribute.\n    \"\"\"\n    if not regex:\n        regex = dict()\n\n    for key, patterns in regex.items():\n        if isinstance(patterns, dict):\n            attr = patterns.get(\"attr\")\n            alignment_mode = patterns.get(\"alignment_mode\")\n            flags = patterns.get(\"flags\")\n            patterns = patterns.get(\"regex\")\n        else:\n            attr = None\n            alignment_mode = None\n            flags = None\n\n        if isinstance(patterns, str):\n            patterns = [patterns]\n\n        self.add(\n            key=key,\n            patterns=patterns,\n            attr=attr,\n            alignment_mode=alignment_mode,\n            flags=flags,\n        )\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.add","title":"<code>add(key, patterns, attr=None, ignore_excluded=None, ignore_space_tokens=None, alignment_mode=None, flags=None)</code>","text":"<p>Add a pattern to the registry.</p> PARAMETER DESCRIPTION <code>key</code> <p>Key of the new/updated pattern.</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>List of patterns to add.</p> <p> TYPE: <code>List[str]</code> </p> <code>attr</code> <p>Attribute to use for matching. By default, uses the <code>default_attr</code> attribute</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> <p>alignment_mode : Optional[str]     Overwrite alignment mode.</p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def add(\n    self,\n    key: str,\n    patterns: List[str],\n    attr: Optional[str] = None,\n    ignore_excluded: Optional[bool] = None,\n    ignore_space_tokens: Optional[bool] = None,\n    alignment_mode: Optional[str] = None,\n    flags: Optional[re.RegexFlag] = None,\n):\n\"\"\"\n    Add a pattern to the registry.\n\n    Parameters\n    ----------\n    key : str\n        Key of the new/updated pattern.\n    patterns : List[str]\n        List of patterns to add.\n    attr : Optional[str]\n        Attribute to use for matching.\n        By default, uses the `default_attr` attribute\n    ignore_excluded : Optional[bool]\n        Whether to skip excluded tokens during matching.\n    ignore_space_tokens: Optional[bool]\n        Whether to skip space tokens during matching.\n\n        You won't be able to match on newlines if this is enabled and\n        the \"spaces\"/\"newline\" option of `eds.normalizer` is enabled (by default).\n\n    alignment_mode : Optional[str]\n        Overwrite alignment mode.\n    \"\"\"\n\n    if attr is None:\n        attr = self.default_attr\n\n    if ignore_excluded is None:\n        ignore_excluded = self.ignore_excluded\n\n    if ignore_space_tokens is None:\n        ignore_space_tokens = self.ignore_space_tokens\n\n    if alignment_mode is None:\n        alignment_mode = self.alignment_mode\n\n    if flags is None:\n        flags = self.flags\n\n    patterns = [compile_regex(pattern, flags) for pattern in patterns]\n\n    self.regex.append(\n        (\n            key,\n            patterns,\n            attr,\n            ignore_excluded,\n            ignore_space_tokens,\n            alignment_mode,\n        )\n    )\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.remove","title":"<code>remove(key)</code>","text":"<p>Remove a pattern for the registry.</p> PARAMETER DESCRIPTION <code>key</code> <p>key of the pattern to remove.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If the key is not present in the registered patterns.</p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def remove(\n    self,\n    key: str,\n):\n\"\"\"\n    Remove a pattern for the registry.\n\n    Parameters\n    ----------\n    key : str\n        key of the pattern to remove.\n\n    Raises\n    ------\n    ValueError\n        If the key is not present in the registered patterns.\n    \"\"\"\n    n = len(self.regex)\n    self.regex = [pat for pat in self.regex if pat[0] != key]\n    if len(self.regex) == n:\n        raise ValueError(f\"`{key}` is not referenced in the matcher\")\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match","title":"<code>match(doclike)</code>","text":"<p>Iterates on the matches.</p> PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object to match on.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE: <code>Tuple[Span, re.Match]</code> </p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def match(\n    self,\n    doclike: Union[Doc, Span],\n) -&gt; Tuple[Span, re.Match]:\n\"\"\"\n    Iterates on the matches.\n\n    Parameters\n    ----------\n    doclike:\n        spaCy Doc or Span object to match on.\n\n    Yields\n    -------\n    span:\n        A match.\n    \"\"\"\n\n    for (\n        key,\n        patterns,\n        attr,\n        ignore_excluded,\n        ignore_space_tokens,\n        alignment_mode,\n    ) in self.regex:\n        text = get_text(doclike, attr, ignore_excluded, ignore_space_tokens)\n\n        for pattern in patterns:\n            for match in pattern.finditer(text):\n                logger.trace(f\"Matched a regex from {key}: {repr(match.group())}\")\n\n                start_char, end_char = span_from_match(\n                    match=match,\n                    span_from_group=self.span_from_group,\n                )\n\n                span = create_span(\n                    doclike=doclike,\n                    start_char=start_char,\n                    end_char=end_char,\n                    key=key,\n                    attr=attr,\n                    alignment_mode=alignment_mode,\n                    ignore_excluded=ignore_excluded,\n                    ignore_space_tokens=ignore_space_tokens,\n                )\n\n                if span is None:\n                    continue\n\n                yield span, match\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.match_with_groupdict_as_spans","title":"<code>match_with_groupdict_as_spans(doclike)</code>","text":"<p>Iterates on the matches.</p> PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object to match on.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE: <code>Tuple[Span, Dict[str, Span]]</code> </p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def match_with_groupdict_as_spans(\n    self,\n    doclike: Union[Doc, Span],\n) -&gt; Tuple[Span, Dict[str, Span]]:\n\"\"\"\n    Iterates on the matches.\n\n    Parameters\n    ----------\n    doclike:\n        spaCy Doc or Span object to match on.\n\n    Yields\n    -------\n    span:\n        A match.\n    \"\"\"\n\n    for (\n        key,\n        patterns,\n        attr,\n        ignore_excluded,\n        ignore_space_tokens,\n        alignment_mode,\n    ) in self.regex:\n        text = get_text(doclike, attr, ignore_excluded, ignore_space_tokens)\n\n        for pattern in patterns:\n            for match in pattern.finditer(text):\n                logger.trace(f\"Matched a regex from {key}: {repr(match.group())}\")\n\n                start_char, end_char = span_from_match(\n                    match=match,\n                    span_from_group=self.span_from_group,\n                )\n\n                span = create_span(\n                    doclike=doclike,\n                    start_char=start_char,\n                    end_char=end_char,\n                    key=key,\n                    attr=attr,\n                    alignment_mode=alignment_mode,\n                    ignore_excluded=ignore_excluded,\n                    ignore_space_tokens=ignore_space_tokens,\n                )\n                group_spans = {}\n                for group_key, group_string in match.groupdict().items():\n                    if group_string:\n                        group_spans[group_key] = create_span(\n                            doclike=doclike,\n                            start_char=match.start(group_key),\n                            end_char=match.end(group_key),\n                            key=group_key,\n                            attr=attr,\n                            alignment_mode=alignment_mode,\n                            ignore_excluded=ignore_excluded,\n                            ignore_space_tokens=ignore_space_tokens,\n                        )\n\n                yield span, group_spans\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.RegexMatcher.__call__","title":"<code>__call__(doclike, as_spans=False, return_groupdict=False)</code>","text":"<p>Performs matching. Yields matches.</p> PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy Doc or Span object.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>as_spans</code> <p>Returns matches as spans.</p> <p> DEFAULT: <code>False</code> </p> YIELDS DESCRIPTION <code>span</code> <p>A match.</p> <p> TYPE: <code>Union[Span, Tuple[Span, Dict[str, Any]]]</code> </p> <code>groupdict</code> <p>Additional information coming from the named patterns in the regular expression.</p> <p> TYPE: <code>Union[Span, Tuple[Span, Dict[str, Any]]]</code> </p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def __call__(\n    self,\n    doclike: Union[Doc, Span],\n    as_spans=False,\n    return_groupdict=False,\n) -&gt; Union[Span, Tuple[Span, Dict[str, Any]]]:\n\"\"\"\n    Performs matching. Yields matches.\n\n    Parameters\n    ----------\n    doclike:\n        spaCy Doc or Span object.\n    as_spans:\n        Returns matches as spans.\n\n    Yields\n    ------\n    span:\n        A match.\n    groupdict:\n        Additional information coming from the named patterns\n        in the regular expression.\n    \"\"\"\n    for span, match in self.match(doclike):\n        if not as_spans:\n            offset = doclike[0].i\n            span = (span.label, span.start - offset, span.end - offset)\n        if return_groupdict:\n            yield span, match.groupdict()\n        else:\n            yield span\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.spans_generator","title":"<code>spans_generator(match)</code>","text":"<p>Iterates over every group, and then yields the full match</p> PARAMETER DESCRIPTION <code>match</code> <p>A match object</p> <p> TYPE: <code>re.Match</code> </p> YIELDS DESCRIPTION <code>Tuple[int, int]</code> <p>A tuple containing the start and end of the group or match</p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def spans_generator(match: re.Match) -&gt; Tuple[int, int]:\n\"\"\"\n    Iterates over every group, and then yields the full match\n\n    Parameters\n    ----------\n    match : re.Match\n        A match object\n\n    Yields\n    ------\n    Tuple[int, int]\n        A tuple containing the start and end of the group or match\n    \"\"\"\n    for idx in range(1, len(match.groups()) + 1):\n        yield match.start(idx), match.end(idx)\n    yield match.start(0), match.end(0)\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.span_from_match","title":"<code>span_from_match(match, span_from_group)</code>","text":"<p>Return the span (as a (start, end) tuple) of the first matching group. If <code>span_from_group=True</code>, returns the full match instead.</p> PARAMETER DESCRIPTION <code>match</code> <p>The Match object</p> <p> TYPE: <code>re.Match</code> </p> <code>span_from_group</code> <p>Whether to work on groups or on the full match</p> <p> TYPE: <code>bool</code> </p> RETURNS DESCRIPTION <code>Tuple[int, int]</code> <p>A tuple containing the start and end of the group or match</p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def span_from_match(\n    match: re.Match,\n    span_from_group: bool,\n) -&gt; Tuple[int, int]:\n\"\"\"\n    Return the span (as a (start, end) tuple) of the first matching group.\n    If `span_from_group=True`, returns the full match instead.\n\n    Parameters\n    ----------\n    match : re.Match\n        The Match object\n    span_from_group : bool\n        Whether to work on groups or on the full match\n\n    Returns\n    -------\n    Tuple[int, int]\n        A tuple containing the start and end of the group or match\n    \"\"\"\n    if not span_from_group:\n        start_char, end_char = match.start(), match.end()\n    else:\n        start_char, end_char = next(filter(lambda x: x[0] &gt;= 0, spans_generator(match)))\n    return start_char, end_char\n</code></pre>"},{"location":"reference/matchers/regex/#edsnlp.matchers.regex.create_span","title":"<code>create_span(doclike, start_char, end_char, key, attr, alignment_mode, ignore_excluded, ignore_space_tokens)</code>","text":"<p>spaCy only allows strict alignment mode for char_span on Spans. This method circumvents this.</p> PARAMETER DESCRIPTION <code>doclike</code> <p><code>Doc</code> or <code>Span</code>.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>start_char</code> <p>Character index within the Doc-like object.</p> <p> TYPE: <code>int</code> </p> <code>end_char</code> <p>Character index of the end, within the Doc-like object.</p> <p> TYPE: <code>int</code> </p> <code>key</code> <p>The key used to match.</p> <p> TYPE: <code>str</code> </p> <code>alignment_mode</code> <p>The alignment mode.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens.</p> <p> TYPE: <code>bool</code> </p> <code>Returns</code> <p> </p> <code>span</code> <p>A span matched on the Doc-like object.</p> <p> </p> Source code in <code>edsnlp/matchers/regex.py</code> <pre><code>def create_span(\n    doclike: Union[Doc, Span],\n    start_char: int,\n    end_char: int,\n    key: str,\n    attr: str,\n    alignment_mode: str,\n    ignore_excluded: bool,\n    ignore_space_tokens: bool,\n) -&gt; Span:\n\"\"\"\n    spaCy only allows strict alignment mode for char_span on Spans.\n    This method circumvents this.\n    Parameters\n    ----------\n    doclike : Union[Doc, Span]\n        `Doc` or `Span`.\n    start_char : int\n        Character index within the Doc-like object.\n    end_char : int\n        Character index of the end, within the Doc-like object.\n    key : str\n        The key used to match.\n    alignment_mode : str\n        The alignment mode.\n    ignore_excluded : bool\n        Whether to skip excluded tokens.\n    ignore_space_tokens : bool\n        Whether to skip space tokens.\n    Returns\n    -------\n    span:\n        A span matched on the Doc-like object.\n    \"\"\"\n\n    doc = doclike if isinstance(doclike, Doc) else doclike.doc\n\n    # Handle the simple case immediately\n    if attr in {\"TEXT\", \"LOWER\"} and not ignore_excluded and not ignore_space_tokens:\n        off = doclike[0].idx\n        return doc.char_span(\n            start_char + off,\n            end_char + off,\n            label=key,\n            alignment_mode=alignment_mode,\n        )\n\n    # If doclike is a Span, we need to get the clean\n    # index of the first included token\n    if ignore_excluded or ignore_space_tokens:\n        original, clean = alignment(\n            doc=doc,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n        )\n\n        first_included = get_first_included(doclike)\n        i = bisect_left(original, first_included.idx)\n        first = clean[i]\n\n    else:\n        first = doclike[0].idx\n\n    start_char = (\n        first\n        + start_char\n        + offset(\n            doc,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n            index=first + start_char,\n        )\n    )\n\n    end_char = (\n        first\n        + end_char\n        + offset(\n            doc,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n            index=first + end_char,\n        )\n    )\n\n    span = doc.char_span(\n        start_char,\n        end_char,\n        label=key,\n        alignment_mode=alignment_mode,\n    )\n\n    return span\n</code></pre>"},{"location":"reference/matchers/simstring/","title":"<code>edsnlp.matchers.simstring</code>","text":""},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter","title":"<code>SimstringWriter</code>","text":"Source code in <code>edsnlp/matchers/simstring.py</code> <pre><code>class SimstringWriter:\n    def __init__(self, path: Union[str, Path]):\n\"\"\"\n        A context class to write a simstring database\n\n        Parameters\n        ----------\n        path: Union[str, Path]\n            Path to database\n        \"\"\"\n        os.makedirs(path, exist_ok=True)\n        self.path = path\n\n    def __enter__(self):\n        path = os.path.join(self.path, \"terms.simstring\")\n        self.db = simstring.writer(path, 3, False, True)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.db.close()\n\n    def insert(self, term):\n        self.db.insert(term)\n</code></pre>"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringWriter.__init__","title":"<code>__init__(path)</code>","text":"<p>A context class to write a simstring database</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to database</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>edsnlp/matchers/simstring.py</code> <pre><code>def __init__(self, path: Union[str, Path]):\n\"\"\"\n    A context class to write a simstring database\n\n    Parameters\n    ----------\n    path: Union[str, Path]\n        Path to database\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    self.path = path\n</code></pre>"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher","title":"<code>SimstringMatcher</code>","text":"Source code in <code>edsnlp/matchers/simstring.py</code> <pre><code>class SimstringMatcher:\n    def __init__(\n        self,\n        vocab: Vocab,\n        path: Optional[Union[Path, str]] = None,\n        measure: SimilarityMeasure = SimilarityMeasure.dice,\n        threshold: float = 0.75,\n        windows: int = 5,\n        ignore_excluded: bool = False,\n        ignore_space_tokens: bool = False,\n        attr: str = \"NORM\",\n    ):\n\"\"\"\n        PhraseMatcher that allows to skip excluded tokens.\n        Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS\n\n        Parameters\n        ----------\n        vocab : Vocab\n            spaCy vocabulary to match on.\n        path: Optional[Union[Path, str]]\n            Path where we will store the precomputed patterns\n        measure: SimilarityMeasure\n            Name of the similarity measure.\n            One of [jaccard, dice, overlap, cosine]\n        windows: int\n            Maximum number of words in a candidate span\n        threshold: float\n            Minimum similarity value to match a concept's synonym\n        ignore_excluded : Optional[bool]\n            Whether to exclude tokens that have an EXCLUDED tag, by default False\n        ignore_space_tokens : Optional[bool]\n            Whether to exclude tokens that have a \"SPACE\" tag, by default False\n        attr : str\n            Default attribute to match on, by default \"TEXT\".\n            Can be overridden in the `add` method.\n            To match on a custom attribute, prepend the attribute name with `_`.\n        \"\"\"\n\n        assert measure in (\n            SimilarityMeasure.jaccard,\n            SimilarityMeasure.dice,\n            SimilarityMeasure.overlap,\n            SimilarityMeasure.cosine,\n        )\n\n        self.vocab = vocab\n        self.windows = windows\n        self.measure = measure\n        self.threshold = threshold\n        self.ignore_excluded = ignore_excluded\n        self.ignore_space_tokens = ignore_space_tokens\n        self.attr = attr\n\n        if path is None:\n            path = tempfile.mkdtemp()\n        self.path = Path(path)\n\n        self.ss_reader = None\n        self.syn2cuis = None\n\n    def build_patterns(\n        self, nlp: Language, terms: Dict[str, Iterable[str]], progress: bool = False\n    ):\n\"\"\"\n        Build patterns and adds them for matching.\n\n        Parameters\n        ----------\n        nlp : Language\n            The instance of the spaCy language class.\n        terms : Patterns\n            Dictionary of label/terms, or label/dictionary of terms/attribute.\n        progress: bool\n            Whether to track progress when preprocessing terms\n        \"\"\"\n\n        self.ss_reader = None\n        self.syn2cuis = None\n\n        syn2cuis = defaultdict(lambda: [])\n        token_pipelines = [\n            name\n            for name, pipe in nlp.pipeline\n            if any(\n                \"token\" in assign and not assign == \"token.is_sent_start\"\n                for assign in nlp.get_pipe_meta(name).assigns\n            )\n        ]\n        with nlp.select_pipes(enable=token_pipelines):\n            with SimstringWriter(self.path) as ss_db:\n                for cui, synset in tqdm(terms.items()) if progress else terms.items():\n                    for term in nlp.pipe(synset):\n                        norm_text = get_text(\n                            term,\n                            self.attr,\n                            ignore_excluded=self.ignore_excluded,\n                            ignore_space_tokens=self.ignore_space_tokens,\n                        )\n                        term = \"##\" + norm_text + \"##\"\n                        ss_db.insert(term)\n                        syn2cuis[term].append(cui)\n        syn2cuis = {term: tuple(sorted(set(cuis))) for term, cuis in syn2cuis.items()}\n        with open(self.path / \"cui-db.pkl\", \"wb\") as f:\n            pickle.dump(syn2cuis, f)\n\n    def load(self):\n        if self.ss_reader is None:\n            self.ss_reader = simstring.reader(\n                os.path.join(self.path, \"terms.simstring\")\n            )\n            self.ss_reader.measure = getattr(simstring, self.measure)\n            self.ss_reader.threshold = self.threshold\n\n            with open(os.path.join(self.path, \"cui-db.pkl\"), \"rb\") as f:\n                self.syn2cuis = pickle.load(f)\n\n    def __call__(self, doc, as_spans=False):\n        self.load()\n\n        root = getattr(doc, \"doc\", doc)\n        if root.has_annotation(\"IS_SENT_START\"):\n            sents = tuple(doc.sents)\n        else:\n            sents = (doc,)\n\n        ents: List[Tuple[str, int, int, float]] = []\n\n        for sent in sents:\n            text, offsets = get_text_and_offsets(\n                doclike=sent,\n                attr=self.attr,\n                ignore_excluded=self.ignore_excluded,\n            )\n            sent_start = getattr(sent, \"start\", 0)\n            for size in range(1, self.windows):\n                for i in range(0, len(offsets) - size):\n                    begin_char, _, begin_i, _ = offsets[i]\n                    _, end_char, _, end_i = offsets[i + size]\n                    span_text = \"##\" + text[begin_char:end_char] + \"##\"\n                    matches = self.ss_reader.retrieve(span_text)\n                    for res in matches:\n                        sim = _similarity(span_text, res, measure=self.measure)\n                        for cui in self.syn2cuis[res]:\n                            ents.append(\n                                (cui, begin_i + sent_start, end_i + sent_start, sim)\n                            )\n\n        sorted_spans = sorted(ents, key=simstring_sort_key, reverse=True)\n        results = []\n        seen_tokens = set()\n        for span in sorted_spans:\n            # Check for end - 1 here because boundaries are inclusive\n            span_tokens = set(range(span[1], span[2]))\n            if not (span_tokens &amp; seen_tokens):\n                results.append(span)\n                seen_tokens.update(span_tokens)\n        results = sorted(results, key=lambda span: span[1])\n        if as_spans:\n            spans = [\n                Span(root, span_data[1], span_data[2], span_data[0])\n                for span_data in results\n            ]\n            return spans\n        else:\n            return [(self.vocab.strings[span[0]], span[1], span[2]) for span in results]\n</code></pre>"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.__init__","title":"<code>__init__(vocab, path=None, measure=SimilarityMeasure.dice, threshold=0.75, windows=5, ignore_excluded=False, ignore_space_tokens=False, attr='NORM')</code>","text":"<p>PhraseMatcher that allows to skip excluded tokens. Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS</p> PARAMETER DESCRIPTION <code>vocab</code> <p>spaCy vocabulary to match on.</p> <p> TYPE: <code>Vocab</code> </p> <code>path</code> <p>Path where we will store the precomputed patterns</p> <p> TYPE: <code>Optional[Union[Path, str]]</code> DEFAULT: <code>None</code> </p> <code>measure</code> <p>Name of the similarity measure. One of [jaccard, dice, overlap, cosine]</p> <p> TYPE: <code>SimilarityMeasure</code> DEFAULT: <code>SimilarityMeasure.dice</code> </p> <code>windows</code> <p>Maximum number of words in a candidate span</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>threshold</code> <p>Minimum similarity value to match a concept's synonym</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.75</code> </p> <code>ignore_excluded</code> <p>Whether to exclude tokens that have an EXCLUDED tag, by default False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to exclude tokens that have a \"SPACE\" tag, by default False</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Default attribute to match on, by default \"TEXT\". Can be overridden in the <code>add</code> method. To match on a custom attribute, prepend the attribute name with <code>_</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> Source code in <code>edsnlp/matchers/simstring.py</code> <pre><code>def __init__(\n    self,\n    vocab: Vocab,\n    path: Optional[Union[Path, str]] = None,\n    measure: SimilarityMeasure = SimilarityMeasure.dice,\n    threshold: float = 0.75,\n    windows: int = 5,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    attr: str = \"NORM\",\n):\n\"\"\"\n    PhraseMatcher that allows to skip excluded tokens.\n    Heavily inspired by https://github.com/Georgetown-IR-Lab/QuickUMLS\n\n    Parameters\n    ----------\n    vocab : Vocab\n        spaCy vocabulary to match on.\n    path: Optional[Union[Path, str]]\n        Path where we will store the precomputed patterns\n    measure: SimilarityMeasure\n        Name of the similarity measure.\n        One of [jaccard, dice, overlap, cosine]\n    windows: int\n        Maximum number of words in a candidate span\n    threshold: float\n        Minimum similarity value to match a concept's synonym\n    ignore_excluded : Optional[bool]\n        Whether to exclude tokens that have an EXCLUDED tag, by default False\n    ignore_space_tokens : Optional[bool]\n        Whether to exclude tokens that have a \"SPACE\" tag, by default False\n    attr : str\n        Default attribute to match on, by default \"TEXT\".\n        Can be overridden in the `add` method.\n        To match on a custom attribute, prepend the attribute name with `_`.\n    \"\"\"\n\n    assert measure in (\n        SimilarityMeasure.jaccard,\n        SimilarityMeasure.dice,\n        SimilarityMeasure.overlap,\n        SimilarityMeasure.cosine,\n    )\n\n    self.vocab = vocab\n    self.windows = windows\n    self.measure = measure\n    self.threshold = threshold\n    self.ignore_excluded = ignore_excluded\n    self.ignore_space_tokens = ignore_space_tokens\n    self.attr = attr\n\n    if path is None:\n        path = tempfile.mkdtemp()\n    self.path = Path(path)\n\n    self.ss_reader = None\n    self.syn2cuis = None\n</code></pre>"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.SimstringMatcher.build_patterns","title":"<code>build_patterns(nlp, terms, progress=False)</code>","text":"<p>Build patterns and adds them for matching.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The instance of the spaCy language class.</p> <p> TYPE: <code>Language</code> </p> <code>terms</code> <p>Dictionary of label/terms, or label/dictionary of terms/attribute.</p> <p> TYPE: <code>Patterns</code> </p> <code>progress</code> <p>Whether to track progress when preprocessing terms</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>edsnlp/matchers/simstring.py</code> <pre><code>def build_patterns(\n    self, nlp: Language, terms: Dict[str, Iterable[str]], progress: bool = False\n):\n\"\"\"\n    Build patterns and adds them for matching.\n\n    Parameters\n    ----------\n    nlp : Language\n        The instance of the spaCy language class.\n    terms : Patterns\n        Dictionary of label/terms, or label/dictionary of terms/attribute.\n    progress: bool\n        Whether to track progress when preprocessing terms\n    \"\"\"\n\n    self.ss_reader = None\n    self.syn2cuis = None\n\n    syn2cuis = defaultdict(lambda: [])\n    token_pipelines = [\n        name\n        for name, pipe in nlp.pipeline\n        if any(\n            \"token\" in assign and not assign == \"token.is_sent_start\"\n            for assign in nlp.get_pipe_meta(name).assigns\n        )\n    ]\n    with nlp.select_pipes(enable=token_pipelines):\n        with SimstringWriter(self.path) as ss_db:\n            for cui, synset in tqdm(terms.items()) if progress else terms.items():\n                for term in nlp.pipe(synset):\n                    norm_text = get_text(\n                        term,\n                        self.attr,\n                        ignore_excluded=self.ignore_excluded,\n                        ignore_space_tokens=self.ignore_space_tokens,\n                    )\n                    term = \"##\" + norm_text + \"##\"\n                    ss_db.insert(term)\n                    syn2cuis[term].append(cui)\n    syn2cuis = {term: tuple(sorted(set(cuis))) for term, cuis in syn2cuis.items()}\n    with open(self.path / \"cui-db.pkl\", \"wb\") as f:\n        pickle.dump(syn2cuis, f)\n</code></pre>"},{"location":"reference/matchers/simstring/#edsnlp.matchers.simstring.get_text_and_offsets","title":"<code>get_text_and_offsets(doclike, attr='TEXT', ignore_excluded=True, ignore_space_tokens=True)</code>  <code>cached</code>","text":"<p>Align different representations of a <code>Doc</code> or <code>Span</code> object.</p> PARAMETER DESCRIPTION <code>doclike</code> <p>spaCy <code>Doc</code> or <code>Span</code> object</p> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p>Attribute to use, by default <code>\"TEXT\"</code></p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to remove excluded tokens, by default True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>ignore_space_tokens</code> <p>Whether to remove space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[Tuple[int, int, int, int]]]</code> <p>The new clean text and offset tuples for each word giving the begin char indice of the word in the new text, the end char indice of its preceding word and the begin / end indices of the word in the original document</p> Source code in <code>edsnlp/matchers/simstring.py</code> <pre><code>@lru_cache(maxsize=128)\ndef get_text_and_offsets(\n    doclike: Union[Span, Doc],\n    attr: str = \"TEXT\",\n    ignore_excluded: bool = True,\n    ignore_space_tokens: bool = True,\n) -&gt; Tuple[str, List[Tuple[int, int, int]]]:\n\"\"\"\n    Align different representations of a `Doc` or `Span` object.\n\n    Parameters\n    ----------\n    doclike : Doc\n        spaCy `Doc` or `Span` object\n    attr : str, optional\n        Attribute to use, by default `\"TEXT\"`\n    ignore_excluded : bool\n        Whether to remove excluded tokens, by default True\n    ignore_space_tokens : bool\n        Whether to remove space tokens, by default False\n\n\n    Returns\n    -------\n    Tuple[str, List[Tuple[int, int, int, int]]]\n        The new clean text and offset tuples for each word giving the begin char indice\n        of the word in the new text, the end char indice of its preceding word and the\n        begin / end indices of the word in the original document\n    \"\"\"\n    attr = attr.upper()\n    attr = ATTRIBUTES.get(attr, attr)\n\n    custom = attr.startswith(\"_\")\n\n    if custom:\n        attr = attr[1:].lower()\n\n    offsets = []\n\n    cursor = 0\n\n    text = []\n\n    last = cursor\n    last_i = 0\n    for i, token in enumerate(doclike):\n\n        if (not ignore_excluded or token.tag_ != \"EXCLUDED\") and (\n            not ignore_space_tokens or token.tag_ != \"SPACE\"\n        ):\n            if custom:\n                token_text = getattr(token._, attr)\n            else:\n                token_text = getattr(token, attr)\n\n            # We add the cursor\n            end = cursor + len(token_text)\n            offsets.append((cursor, last, i, last_i + 1))\n\n            cursor = end\n            last = end\n            last_i = i\n\n            text.append(token_text)\n\n            if token.whitespace_:\n                cursor += 1\n                text.append(\" \")\n\n    offsets.append((cursor, last, len(doclike), last_i + 1))\n\n    return \"\".join(text), offsets\n</code></pre>"},{"location":"reference/matchers/utils/","title":"<code>edsnlp.matchers.utils</code>","text":""},{"location":"reference/matchers/utils/offset/","title":"<code>edsnlp.matchers.utils.offset</code>","text":""},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.alignment","title":"<code>alignment(doc, attr='TEXT', ignore_excluded=True, ignore_space_tokens=True)</code>  <code>cached</code>","text":"<p>Align different representations of a <code>Doc</code> or <code>Span</code> object.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy <code>Doc</code> or <code>Span</code> object</p> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p>Attribute to use, by default <code>\"TEXT\"</code></p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to remove excluded tokens, by default True</p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>True</code> </p> <code>ignore_space_tokens</code> <p>Whether to remove space tokens, by default True</p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Tuple[List[int], List[int]]</code> <p>An alignment tuple: original and clean lists.</p> Source code in <code>edsnlp/matchers/utils/offset.py</code> <pre><code>@lru_cache(maxsize=32)\ndef alignment(\n    doc: Doc,\n    attr: str = \"TEXT\",\n    ignore_excluded: bool = True,\n    ignore_space_tokens: bool = True,\n) -&gt; Tuple[List[int], List[int]]:\n\"\"\"\n    Align different representations of a `Doc` or `Span` object.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy `Doc` or `Span` object\n    attr : str, optional\n        Attribute to use, by default `\"TEXT\"`\n    ignore_excluded : bool, optional\n        Whether to remove excluded tokens, by default True\n    ignore_space_tokens : bool, optional\n        Whether to remove space tokens, by default True\n\n    Returns\n    -------\n    Tuple[List[int], List[int]]\n        An alignment tuple: original and clean lists.\n    \"\"\"\n    assert isinstance(doc, Doc)\n\n    attr = attr.upper()\n    attr = ATTRIBUTES.get(attr, attr)\n\n    custom = attr.startswith(\"_\")\n\n    if custom:\n        attr = attr[1:].lower()\n\n    # Define the length function\n    length = partial(token_length, custom=custom, attr=attr)\n\n    original = []\n    clean = []\n\n    cursor = 0\n\n    for token in doc:\n\n        if (not ignore_excluded or token.tag_ != \"EXCLUDED\") and (\n            not ignore_space_tokens or not token.tag_ == \"SPACE\"\n        ):\n\n            # The token is not excluded, we add its extremities to the list\n            original.append(token.idx)\n\n            # We add the cursor\n            clean.append(cursor)\n            cursor += length(token)\n\n            if token.whitespace_:\n                cursor += 1\n\n    return original, clean\n</code></pre>"},{"location":"reference/matchers/utils/offset/#edsnlp.matchers.utils.offset.offset","title":"<code>offset(doc, attr, ignore_excluded, ignore_space_tokens, index)</code>","text":"<p>Compute offset between the original text and a given representation (defined by the couple <code>attr</code>, <code>ignore_excluded</code>).</p> <p>The alignment itself is computed with <code>alignment</code>.</p> PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object</p> <p> TYPE: <code>Doc</code> </p> <code>attr</code> <p>The attribute used by the <code>RegexMatcher</code> (eg <code>NORM</code>)</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens.</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore spaces tokens.</p> <p> TYPE: <code>bool</code> </p> <code>index</code> <p>The index in the pre-processed text.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The offset. To get the character index in the original document, just do: <code>original = index + offset(doc, attr, ignore_excluded, index)</code></p> Source code in <code>edsnlp/matchers/utils/offset.py</code> <pre><code>def offset(\n    doc: Doc,\n    attr: str,\n    ignore_excluded: bool,\n    ignore_space_tokens: bool,\n    index: int,\n) -&gt; int:\n\"\"\"\n    Compute offset between the original text and a given representation\n    (defined by the couple `attr`, `ignore_excluded`).\n\n    The alignment itself is computed with\n    [`alignment`][edsnlp.matchers.utils.offset.alignment].\n\n    Parameters\n    ----------\n    doc : Doc\n        The spaCy `Doc` object\n    attr : str\n        The attribute used by the [`RegexMatcher`][edsnlp.matchers.regex.RegexMatcher]\n        (eg `NORM`)\n    ignore_excluded : bool\n        Whether to ignore excluded tokens.\n    ignore_space_tokens : bool\n        Whether to ignore spaces tokens.\n    index : int\n        The index in the pre-processed text.\n\n    Returns\n    -------\n    int\n        The offset. To get the character index in the original document,\n        just do: `#!python original = index + offset(doc, attr, ignore_excluded, index)`\n    \"\"\"\n    original, clean = alignment(\n        doc=doc,\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n    )\n\n    # We use bisect to efficiently find the correct rightmost-lower index\n    i = bisect_left(clean, index)\n    i = min(i, len(original) - 1)\n\n    return original[i] - clean[i]\n</code></pre>"},{"location":"reference/matchers/utils/text/","title":"<code>edsnlp.matchers.utils.text</code>","text":""},{"location":"reference/matchers/utils/text/#edsnlp.matchers.utils.text.get_text","title":"<code>get_text(doclike, attr, ignore_excluded, ignore_space_tokens=False)</code>  <code>cached</code>","text":"<p>Get text using a custom attribute, possibly ignoring excluded tokens.</p> PARAMETER DESCRIPTION <code>doclike</code> <p>Doc or Span to get text from.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>attr</code> <p>Attribute to use.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens, by default False</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens, by default False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Extracted text.</p> Source code in <code>edsnlp/matchers/utils/text.py</code> <pre><code>@lru_cache(32)\ndef get_text(\n    doclike: Union[Doc, Span],\n    attr: str,\n    ignore_excluded: bool,\n    ignore_space_tokens: bool = False,\n) -&gt; str:\n\"\"\"\n    Get text using a custom attribute, possibly ignoring excluded tokens.\n\n    Parameters\n    ----------\n    doclike : Union[Doc, Span]\n        Doc or Span to get text from.\n    attr : str\n        Attribute to use.\n    ignore_excluded : bool\n        Whether to skip excluded tokens, by default False\n    ignore_space_tokens : bool\n        Whether to skip space tokens, by default False\n\n    Returns\n    -------\n    str\n        Extracted text.\n    \"\"\"\n\n    attr = attr.upper()\n\n    if not ignore_excluded and not ignore_space_tokens:\n        if attr == \"TEXT\":\n            return doclike.text\n        elif attr == \"LOWER\":\n            return doclike.text.lower()\n        else:\n            tokens = doclike\n    else:\n        if not ignore_space_tokens:\n            tokens = [t for t in doclike if t.tag_ != \"EXCLUDED\"]\n        else:\n            tokens = [t for t in doclike if t.tag_ not in (\"EXCLUDED\", \"SPACE\")]\n\n    if not tokens:\n        return \"\"\n\n    attr = ATTRIBUTES.get(attr, attr)\n\n    if attr.startswith(\"_\"):\n        attr = attr[1:].lower()\n        return \"\".join(\n            [getattr(t._, attr) + t.whitespace_ for t in tokens[:-1]]\n        ) + getattr(tokens[-1], attr)\n    else:\n        return \"\".join(\n            [getattr(t, attr) + t.whitespace_ for t in tokens[:-1]]\n        ) + getattr(tokens[-1], attr)\n</code></pre>"},{"location":"reference/models/","title":"<code>edsnlp.models</code>","text":""},{"location":"reference/models/pytorch_wrapper/","title":"<code>edsnlp.models.pytorch_wrapper</code>","text":""},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule","title":"<code>PytorchWrapperModule</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>class PytorchWrapperModule(torch.nn.Module):\n    def __init__(\n        self,\n        input_size: Optional[int] = None,\n        n_labels: Optional[int] = None,\n    ):\n\"\"\"\n        Pytorch wrapping module for Spacy.\n        Models that expect to be wrapped with\n        [wrap_pytorch_model][edsnlp.models.pytorch_wrapper.wrap_pytorch_model]\n        should inherit from this module.\n\n        Parameters\n        ----------\n        input_size: int\n            Size of the input embeddings\n        n_labels: int\n            Number of labels predicted by the module\n        \"\"\"\n        super().__init__()\n\n        self.cfg = {\"n_labels\": n_labels, \"input_size\": input_size}\n\n    @property\n    def n_labels(self):\n        return self.cfg[\"n_labels\"]\n\n    @property\n    def input_size(self):\n        return self.cfg[\"input_size\"]\n\n    def load_state_dict(\n        self, state_dict: OrderedDict[str, torch.Tensor], strict: bool = True\n    ):\n\"\"\"\n        Loads the model inplace from a dumped `state_dict` object\n\n        Parameters\n        ----------\n        state_dict: OrderedDict[str, torch.Tensor]\n        strict: bool\n        \"\"\"\n        self.cfg = state_dict.pop(\"cfg\")\n        self.initialize()\n        super().load_state_dict(state_dict, strict)\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n\"\"\"\n        Loads the model inplace from a dumped `state_dict` object\n\n        Parameters\n        ----------\n        destination: Any\n        prefix: str\n        keep_vars: bool\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        state = super().state_dict(destination, prefix, keep_vars)\n        state[\"cfg\"] = self.cfg\n        return state\n\n    def set_n_labels(self, n_labels):\n\"\"\"\n        Sets the number of labels. To instantiate the linear layer, we need to\n        call the `initialize` method.\n\n        Parameters\n        ----------\n        n_labels: int\n            Number of different labels predicted by this module\n        \"\"\"\n        self.cfg[\"n_labels\"] = n_labels\n\n    def initialize(self):\n\"\"\"\n        Once the number of labels n_labels are known, this method\n        initializes the torch linear layer.\n        \"\"\"\n        raise NotImplementedError()\n\n    def forward(\n        self,\n        embeds: torch.FloatTensor,\n        mask: torch.BoolTensor,\n        *,\n        additional_outputs: typing.Dict[str, Any] = None,\n        is_train: bool = False,\n        is_predict: bool = False,\n    ) -&gt; Optional[torch.FloatTensor]:\n\"\"\"\n        Apply the nested pytorch module to:\n        - compute the loss\n        - predict the outputs\n        non exclusively.\n        If outputs are predicted, they are assigned to the `additional_outputs`\n        list.\n\n        Parameters\n        ----------\n        embeds: torch.FloatTensor\n            Input embeddings\n        mask: torch.BoolTensor\n            Input embeddings mask\n        additional_outputs: List\n            Additional outputs that should not / cannot be back-propped through\n            (Thinc treats Pytorch models solely as derivable functions, but the CRF\n            that we employ performs the best tag decoding function with Pytorch)\n            This list will contain the predicted outputs\n        is_train: bool=False\n            Are we training the model (defaults to True)\n        is_predict: bool=False\n            Are we predicting the model (defaults to False)\n\n        Returns\n        -------\n        Optional[torch.FloatTensor]\n            Optional 0d loss (shape = [1]) to train the model\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.__init__","title":"<code>__init__(input_size=None, n_labels=None)</code>","text":"<p>Pytorch wrapping module for Spacy. Models that expect to be wrapped with wrap_pytorch_model should inherit from this module.</p> PARAMETER DESCRIPTION <code>input_size</code> <p>Size of the input embeddings</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>n_labels</code> <p>Number of labels predicted by the module</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def __init__(\n    self,\n    input_size: Optional[int] = None,\n    n_labels: Optional[int] = None,\n):\n\"\"\"\n    Pytorch wrapping module for Spacy.\n    Models that expect to be wrapped with\n    [wrap_pytorch_model][edsnlp.models.pytorch_wrapper.wrap_pytorch_model]\n    should inherit from this module.\n\n    Parameters\n    ----------\n    input_size: int\n        Size of the input embeddings\n    n_labels: int\n        Number of labels predicted by the module\n    \"\"\"\n    super().__init__()\n\n    self.cfg = {\"n_labels\": n_labels, \"input_size\": input_size}\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.load_state_dict","title":"<code>load_state_dict(state_dict, strict=True)</code>","text":"<p>Loads the model inplace from a dumped <code>state_dict</code> object</p> PARAMETER DESCRIPTION <code>state_dict</code> <p> TYPE: <code>OrderedDict[str, torch.Tensor]</code> </p> <code>strict</code> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def load_state_dict(\n    self, state_dict: OrderedDict[str, torch.Tensor], strict: bool = True\n):\n\"\"\"\n    Loads the model inplace from a dumped `state_dict` object\n\n    Parameters\n    ----------\n    state_dict: OrderedDict[str, torch.Tensor]\n    strict: bool\n    \"\"\"\n    self.cfg = state_dict.pop(\"cfg\")\n    self.initialize()\n    super().load_state_dict(state_dict, strict)\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.state_dict","title":"<code>state_dict(destination=None, prefix='', keep_vars=False)</code>","text":"<p>Loads the model inplace from a dumped <code>state_dict</code> object</p> PARAMETER DESCRIPTION <code>destination</code> <p> DEFAULT: <code>None</code> </p> <code>prefix</code> <p> DEFAULT: <code>''</code> </p> <code>keep_vars</code> <p> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dict</code> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n\"\"\"\n    Loads the model inplace from a dumped `state_dict` object\n\n    Parameters\n    ----------\n    destination: Any\n    prefix: str\n    keep_vars: bool\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    state = super().state_dict(destination, prefix, keep_vars)\n    state[\"cfg\"] = self.cfg\n    return state\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.set_n_labels","title":"<code>set_n_labels(n_labels)</code>","text":"<p>Sets the number of labels. To instantiate the linear layer, we need to call the <code>initialize</code> method.</p> PARAMETER DESCRIPTION <code>n_labels</code> <p>Number of different labels predicted by this module</p> <p> </p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def set_n_labels(self, n_labels):\n\"\"\"\n    Sets the number of labels. To instantiate the linear layer, we need to\n    call the `initialize` method.\n\n    Parameters\n    ----------\n    n_labels: int\n        Number of different labels predicted by this module\n    \"\"\"\n    self.cfg[\"n_labels\"] = n_labels\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.initialize","title":"<code>initialize()</code>","text":"<p>Once the number of labels n_labels are known, this method initializes the torch linear layer.</p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def initialize(self):\n\"\"\"\n    Once the number of labels n_labels are known, this method\n    initializes the torch linear layer.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.PytorchWrapperModule.forward","title":"<code>forward(embeds, mask, *, additional_outputs=None, is_train=False, is_predict=False)</code>","text":"<p>Apply the nested pytorch module to: - compute the loss - predict the outputs non exclusively. If outputs are predicted, they are assigned to the <code>additional_outputs</code> list.</p> PARAMETER DESCRIPTION <code>embeds</code> <p>Input embeddings</p> <p> TYPE: <code>torch.FloatTensor</code> </p> <code>mask</code> <p>Input embeddings mask</p> <p> TYPE: <code>torch.BoolTensor</code> </p> <code>additional_outputs</code> <p>Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This list will contain the predicted outputs</p> <p> TYPE: <code>typing.Dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>is_train</code> <p>Are we training the model (defaults to True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_predict</code> <p>Are we predicting the model (defaults to False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Optional[torch.FloatTensor]</code> <p>Optional 0d loss (shape = [1]) to train the model</p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def forward(\n    self,\n    embeds: torch.FloatTensor,\n    mask: torch.BoolTensor,\n    *,\n    additional_outputs: typing.Dict[str, Any] = None,\n    is_train: bool = False,\n    is_predict: bool = False,\n) -&gt; Optional[torch.FloatTensor]:\n\"\"\"\n    Apply the nested pytorch module to:\n    - compute the loss\n    - predict the outputs\n    non exclusively.\n    If outputs are predicted, they are assigned to the `additional_outputs`\n    list.\n\n    Parameters\n    ----------\n    embeds: torch.FloatTensor\n        Input embeddings\n    mask: torch.BoolTensor\n        Input embeddings mask\n    additional_outputs: List\n        Additional outputs that should not / cannot be back-propped through\n        (Thinc treats Pytorch models solely as derivable functions, but the CRF\n        that we employ performs the best tag decoding function with Pytorch)\n        This list will contain the predicted outputs\n    is_train: bool=False\n        Are we training the model (defaults to True)\n    is_predict: bool=False\n        Are we predicting the model (defaults to False)\n\n    Returns\n    -------\n    Optional[torch.FloatTensor]\n        Optional 0d loss (shape = [1]) to train the model\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.pytorch_forward","title":"<code>pytorch_forward(model, X, is_train=False)</code>","text":"<p>Run the stacked CRF pytorch model to train / run a nested NER model</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Model</code> </p> <code>X</code> <p> TYPE: <code>Tuple[Iterable[Doc], PredT, bool]</code> </p> <code>is_train</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tuple[Tuple[Floats1d, PredictionT], Callable[Floats1d, Any]]</code> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def pytorch_forward(\n    model: Model,\n    X: Tuple[Iterable[Doc], PredT, bool],\n    is_train: bool = False,\n) -&gt; Tuple[Tuple[Floats1d, PredT], Callable[[Floats1d], Any]]:\n\"\"\"\n    Run the stacked CRF pytorch model to train / run a nested NER model\n\n    Parameters\n    ----------\n    model: Model\n    X: Tuple[Iterable[Doc], PredictionT, bool]\n    is_train: bool\n\n    Returns\n    -------\n    Tuple[Tuple[Floats1d, PredictionT], Callable[Floats1d, Any]]\n    \"\"\"\n    [docs, *rest_X, is_predict] = X\n    encoder: Model[List[Doc], List[Floats2d]] = model.get_ref(\"encoder\")\n    embeds_list, bp_embeds = encoder(docs, is_train=is_train)\n    embeds = model.ops.pad(embeds_list)  # pad embeds\n\n    ##################################################\n    # Prepare the torch nested ner crf module inputs #\n    ##################################################\n    additional_outputs = {}\n    # Convert input from numpy/cupy to torch\n    (torch_embeds, *torch_rest), get_d_embeds = custom_xp2torch(\n        model, (embeds, *rest_X)\n    )\n    # Prepare token mask from docs' lengths\n    torch_mask = (\n        torch.arange(embeds.shape[1], device=torch_embeds.device)\n        &lt; torch.tensor([d.shape[0] for d in embeds_list], device=torch_embeds.device)[\n            :, None\n        ]\n    )\n\n    #################\n    # Run the model #\n    #################\n    loss_torch, torch_backprop = model.shims[0](\n        ArgsKwargs(\n            (torch_embeds, torch_mask, *torch_rest),\n            {\n                \"additional_outputs\": additional_outputs,\n                \"is_train\": is_train,\n                \"is_predict\": is_predict,\n            },\n        ),\n        is_train,\n    )\n\n    ####################################\n    # Postprocess the module's outputs #\n    ####################################\n    loss = torch2xp(loss_torch) if loss_torch is not None else None\n    additional_outputs = convert_recursive(is_torch_array, torch2xp, additional_outputs)\n\n    def backprop(d_loss: Floats1d) -&gt; Any:\n        d_loss_torch = ArgsKwargs(\n            args=((loss_torch,),), kwargs={\"grad_tensors\": xp2torch(d_loss)}\n        )\n        d_embeds_torch = torch_backprop(d_loss_torch)\n        d_embeds = get_d_embeds(d_embeds_torch)\n        d_embeds_list = [\n            d_padded_row[: len(d_item)]\n            for d_item, d_padded_row in zip(embeds_list, d_embeds)\n        ]\n        d_docs = bp_embeds(d_embeds_list)\n        return d_docs\n\n    return (loss, additional_outputs), backprop\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.instance_init","title":"<code>instance_init(model, X=None, Y=None)</code>","text":"<p>Initializes the model by setting the input size of the model layers and the number of predicted labels</p> PARAMETER DESCRIPTION <code>model</code> <p>Nested NER thinc model</p> <p> TYPE: <code>Model</code> </p> <code>X</code> <p>list of documents on which we apply the encoder layer</p> <p> TYPE: <code>List[Doc]</code> DEFAULT: <code>None</code> </p> <code>Y</code> <p>Unused gold spans</p> <p> TYPE: <code>Ints2d</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def instance_init(model: Model, X: List[Doc] = None, Y: Ints2d = None) -&gt; Model:\n\"\"\"\n    Initializes the model by setting the input size of the model layers and the number\n    of predicted labels\n\n    Parameters\n    ----------\n    model: Model\n        Nested NER thinc model\n    X: List[Doc]\n        list of documents on which we apply the encoder layer\n    Y: Ints2d\n        Unused gold spans\n\n    Returns\n    -------\n\n    \"\"\"\n    encoder = model.get_ref(\"encoder\")\n    if X is not None:\n        encoder.initialize(X)\n\n    pt_model = model.attrs[\"pt_model\"]\n    pt_model.cfg[\"input_size\"] = encoder.get_dim(\"nO\")\n    pt_model.initialize()\n    pt_model.to(get_torch_default_device())\n    model.set_dim(\"nI\", pt_model.input_size)\n\n    return model\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.wrap_pytorch_model","title":"<code>wrap_pytorch_model(encoder, pt_model)</code>","text":"<p>Chain and wraps a spaCy/Thinc encoder model (like a tok2vec) and a pytorch model. The loss should be computed directly in the Pytorch module and Categorical predictions are supported</p> PARAMETER DESCRIPTION <code>encoder</code> <p>The Thinc document token embedding layer</p> <p> TYPE: <code>Model[List[Doc], List[Floats2d]]</code> </p> <code>pt_model</code> <p>The Pytorch model</p> <p> TYPE: <code>PytorchWrapperModule</code> </p> RETURNS DESCRIPTION <code>    Tuple[Iterable[Doc], Optional[PredT], Optional[bool]],</code> Source code in <code>edsnlp/models/pytorch_wrapper.py</code> <pre><code>def wrap_pytorch_model(\n    encoder: Model[List[Doc], List[Floats2d]],\n    pt_model: PytorchWrapperModule,\n) -&gt; Model[\n    Tuple[Iterable[Doc], Optional[PredT], Optional[bool]],\n    Tuple[Floats1d, PredT],\n]:\n\"\"\"\n    Chain and wraps a spaCy/Thinc encoder model (like a tok2vec) and a pytorch model.\n    The loss should be computed directly in the Pytorch module and Categorical\n    predictions are supported\n\n    Parameters\n    ----------\n    encoder: Model[List[Doc], List[Floats2d]]\n        The Thinc document token embedding layer\n    pt_model: PytorchWrapperModule\n        The Pytorch model\n\n    Returns\n    -------\n        Tuple[Iterable[Doc], Optional[PredT], Optional[bool]],\n        # inputs (docs, gold, *rest, is_predict)\n        Tuple[Floats1d, PredT],\n        # outputs (loss, *additional_outputs)\n    \"\"\"\n    return Model(\n        \"pytorch\",\n        pytorch_forward,\n        attrs={\n            \"set_n_labels\": pt_model.set_n_labels,\n            \"pt_model\": pt_model,\n        },\n        layers=[encoder],\n        shims=[PyTorchShim(pt_model)],\n        refs={\"encoder\": encoder},\n        dims={\"nI\": None, \"nO\": None},\n        init=instance_init,\n    )\n</code></pre>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.wrap_pytorch_model--inputs-docs-gold-rest-is_predict","title":"inputs (docs, gold, *rest, is_predict)","text":"<p>Tuple[Floats1d, PredT],</p>"},{"location":"reference/models/pytorch_wrapper/#edsnlp.models.pytorch_wrapper.wrap_pytorch_model--outputs-loss-additional_outputs","title":"outputs (loss, *additional_outputs)","text":""},{"location":"reference/models/stack_crf_ner/","title":"<code>edsnlp.models.stack_crf_ner</code>","text":""},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule","title":"<code>StackedCRFNERModule</code>","text":"<p>         Bases: <code>PytorchWrapperModule</code></p> Source code in <code>edsnlp/models/stack_crf_ner.py</code> <pre><code>class StackedCRFNERModule(PytorchWrapperModule):\n    def __init__(\n        self,\n        input_size: Optional[int] = None,\n        n_labels: Optional[int] = None,\n        mode: CRFMode = CRFMode.joint,\n    ):\n\"\"\"\n        Nested NER CRF module\n\n        Parameters\n        ----------\n        input_size: int\n            Size of the input embeddings\n        n_labels: int\n            Number of labels predicted by the module\n        mode: CRFMode\n            Loss mode of the CRF\n        \"\"\"\n        super().__init__(input_size, n_labels)\n\n        self.cfg[\"mode\"] = mode\n\n        assert mode in (CRFMode.independent, CRFMode.joint, CRFMode.marginal)\n        self.crf = MultiLabelBIOULDecoder(1, learnable_transitions=False)\n\n        self.classifier = None\n\n    def initialize(self):\n\"\"\"\n        Once the number of labels n_labels are known, this method\n        initializes the torch linear layer.\n        \"\"\"\n        num_tags = self.n_labels * self.crf.num_tags\n        self.classifier = torch.nn.Linear(self.input_size, num_tags)\n\n    def forward(\n        self,\n        embeds: torch.FloatTensor,\n        mask: torch.BoolTensor,\n        spans: Optional[torch.LongTensor] = None,\n        additional_outputs: Dict[str, Any] = None,\n        is_train: bool = False,\n        is_predict: bool = False,\n    ) -&gt; Optional[torch.FloatTensor]:\n\"\"\"\n        Apply the nested ner module to the document embeddings to:\n        - compute the loss\n        - predict the spans\n        non exclusively.\n        If spans are predicted, they are assigned to the `additional_outputs`\n        dictionary.\n\n        Parameters\n        ----------\n        embeds: torch.FloatTensor\n            Token embeddings to predict the tags from\n        mask: torch.BoolTensor\n            Mask of the sequences\n        spans: Optional[torch.LongTensor]\n            2d tensor of n_spans * (doc_idx, label_idx, begin, end)\n        additional_outputs: Dict[str, Any]\n            Additional outputs that should not / cannot be back-propped through\n            (Thinc treats Pytorch models solely as derivable functions, but the CRF\n            that we employ performs the best tag decoding function with Pytorch)\n            This dict will contain the predicted 2d tensor of spans\n        is_train: bool=False\n            Are we training the model (defaults to True)\n        is_predict: bool=False\n            Are we predicting the model (defaults to False)\n\n        Returns\n        -------\n        Optional[torch.FloatTensor]\n            Optional 0d loss (shape = [1]) to train the model\n        \"\"\"\n        n_samples, n_tokens = embeds.shape[:2]\n        logits = self.classifier(embeds)\n        crf_logits = flatten_dim(\n            logits.view(n_samples, n_tokens, self.n_labels, self.crf.num_tags).permute(\n                0, 2, 1, 3\n            ),\n            dim=0,\n        )\n        crf_mask = repeat(mask, self.n_labels, 0)\n        loss = None\n        if is_train:\n            tags = self.crf.spans_to_tags(\n                spans, n_samples=n_samples, n_tokens=n_tokens, n_labels=self.n_labels\n            )\n            crf_target = flatten_dim(\n                torch.nn.functional.one_hot(tags, 5).bool()\n                if len(tags.shape) == 3\n                else tags,\n                dim=0,\n            )\n            if self.cfg[\"mode\"] == CRFMode.joint:\n                loss = self.crf(\n                    crf_logits,\n                    crf_mask,\n                    crf_target,\n                )\n            elif self.cfg[\"mode\"] == CRFMode.independent:\n                loss = (\n                    -crf_logits.log_softmax(-1)\n                    .masked_fill(~crf_target, IMPOSSIBLE)\n                    .logsumexp(-1)[crf_mask]\n                    .sum()\n                )\n            elif self.cfg[\"mode\"] == CRFMode.marginal:\n                crf_logits = self.crf.marginal(\n                    crf_logits,\n                    crf_mask,\n                )\n                loss = (\n                    -crf_logits.log_softmax(-1)\n                    .masked_fill(~crf_target, IMPOSSIBLE)\n                    .logsumexp(-1)[crf_mask]\n                    .sum()\n                )\n            if (loss &gt; -IMPOSSIBLE).any():\n                logger.warning(\n                    \"You likely have an impossible transition in your \"\n                    \"training data NER tags, skipping this batch.\"\n                )\n                loss = torch.zeros(1, dtype=torch.float, device=embeds.device)\n            loss = loss.sum().unsqueeze(0) / 100.0\n        if is_predict:\n            pred_tags = self.crf.decode(crf_logits, crf_mask).reshape(\n                n_samples, self.n_labels, n_tokens\n            )\n            pred_spans = self.crf.tags_to_spans(pred_tags)\n            additional_outputs[\"spans\"] = pred_spans\n        return loss\n</code></pre>"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.__init__","title":"<code>__init__(input_size=None, n_labels=None, mode=CRFMode.joint)</code>","text":"<p>Nested NER CRF module</p> PARAMETER DESCRIPTION <code>input_size</code> <p>Size of the input embeddings</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>n_labels</code> <p>Number of labels predicted by the module</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>mode</code> <p>Loss mode of the CRF</p> <p> TYPE: <code>CRFMode</code> DEFAULT: <code>CRFMode.joint</code> </p> Source code in <code>edsnlp/models/stack_crf_ner.py</code> <pre><code>def __init__(\n    self,\n    input_size: Optional[int] = None,\n    n_labels: Optional[int] = None,\n    mode: CRFMode = CRFMode.joint,\n):\n\"\"\"\n    Nested NER CRF module\n\n    Parameters\n    ----------\n    input_size: int\n        Size of the input embeddings\n    n_labels: int\n        Number of labels predicted by the module\n    mode: CRFMode\n        Loss mode of the CRF\n    \"\"\"\n    super().__init__(input_size, n_labels)\n\n    self.cfg[\"mode\"] = mode\n\n    assert mode in (CRFMode.independent, CRFMode.joint, CRFMode.marginal)\n    self.crf = MultiLabelBIOULDecoder(1, learnable_transitions=False)\n\n    self.classifier = None\n</code></pre>"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.initialize","title":"<code>initialize()</code>","text":"<p>Once the number of labels n_labels are known, this method initializes the torch linear layer.</p> Source code in <code>edsnlp/models/stack_crf_ner.py</code> <pre><code>def initialize(self):\n\"\"\"\n    Once the number of labels n_labels are known, this method\n    initializes the torch linear layer.\n    \"\"\"\n    num_tags = self.n_labels * self.crf.num_tags\n    self.classifier = torch.nn.Linear(self.input_size, num_tags)\n</code></pre>"},{"location":"reference/models/stack_crf_ner/#edsnlp.models.stack_crf_ner.StackedCRFNERModule.forward","title":"<code>forward(embeds, mask, spans=None, additional_outputs=None, is_train=False, is_predict=False)</code>","text":"<p>Apply the nested ner module to the document embeddings to: - compute the loss - predict the spans non exclusively. If spans are predicted, they are assigned to the <code>additional_outputs</code> dictionary.</p> PARAMETER DESCRIPTION <code>embeds</code> <p>Token embeddings to predict the tags from</p> <p> TYPE: <code>torch.FloatTensor</code> </p> <code>mask</code> <p>Mask of the sequences</p> <p> TYPE: <code>torch.BoolTensor</code> </p> <code>spans</code> <p>2d tensor of n_spans * (doc_idx, label_idx, begin, end)</p> <p> TYPE: <code>Optional[torch.LongTensor]</code> DEFAULT: <code>None</code> </p> <code>additional_outputs</code> <p>Additional outputs that should not / cannot be back-propped through (Thinc treats Pytorch models solely as derivable functions, but the CRF that we employ performs the best tag decoding function with Pytorch) This dict will contain the predicted 2d tensor of spans</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> <code>is_train</code> <p>Are we training the model (defaults to True)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>is_predict</code> <p>Are we predicting the model (defaults to False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Optional[torch.FloatTensor]</code> <p>Optional 0d loss (shape = [1]) to train the model</p> Source code in <code>edsnlp/models/stack_crf_ner.py</code> <pre><code>def forward(\n    self,\n    embeds: torch.FloatTensor,\n    mask: torch.BoolTensor,\n    spans: Optional[torch.LongTensor] = None,\n    additional_outputs: Dict[str, Any] = None,\n    is_train: bool = False,\n    is_predict: bool = False,\n) -&gt; Optional[torch.FloatTensor]:\n\"\"\"\n    Apply the nested ner module to the document embeddings to:\n    - compute the loss\n    - predict the spans\n    non exclusively.\n    If spans are predicted, they are assigned to the `additional_outputs`\n    dictionary.\n\n    Parameters\n    ----------\n    embeds: torch.FloatTensor\n        Token embeddings to predict the tags from\n    mask: torch.BoolTensor\n        Mask of the sequences\n    spans: Optional[torch.LongTensor]\n        2d tensor of n_spans * (doc_idx, label_idx, begin, end)\n    additional_outputs: Dict[str, Any]\n        Additional outputs that should not / cannot be back-propped through\n        (Thinc treats Pytorch models solely as derivable functions, but the CRF\n        that we employ performs the best tag decoding function with Pytorch)\n        This dict will contain the predicted 2d tensor of spans\n    is_train: bool=False\n        Are we training the model (defaults to True)\n    is_predict: bool=False\n        Are we predicting the model (defaults to False)\n\n    Returns\n    -------\n    Optional[torch.FloatTensor]\n        Optional 0d loss (shape = [1]) to train the model\n    \"\"\"\n    n_samples, n_tokens = embeds.shape[:2]\n    logits = self.classifier(embeds)\n    crf_logits = flatten_dim(\n        logits.view(n_samples, n_tokens, self.n_labels, self.crf.num_tags).permute(\n            0, 2, 1, 3\n        ),\n        dim=0,\n    )\n    crf_mask = repeat(mask, self.n_labels, 0)\n    loss = None\n    if is_train:\n        tags = self.crf.spans_to_tags(\n            spans, n_samples=n_samples, n_tokens=n_tokens, n_labels=self.n_labels\n        )\n        crf_target = flatten_dim(\n            torch.nn.functional.one_hot(tags, 5).bool()\n            if len(tags.shape) == 3\n            else tags,\n            dim=0,\n        )\n        if self.cfg[\"mode\"] == CRFMode.joint:\n            loss = self.crf(\n                crf_logits,\n                crf_mask,\n                crf_target,\n            )\n        elif self.cfg[\"mode\"] == CRFMode.independent:\n            loss = (\n                -crf_logits.log_softmax(-1)\n                .masked_fill(~crf_target, IMPOSSIBLE)\n                .logsumexp(-1)[crf_mask]\n                .sum()\n            )\n        elif self.cfg[\"mode\"] == CRFMode.marginal:\n            crf_logits = self.crf.marginal(\n                crf_logits,\n                crf_mask,\n            )\n            loss = (\n                -crf_logits.log_softmax(-1)\n                .masked_fill(~crf_target, IMPOSSIBLE)\n                .logsumexp(-1)[crf_mask]\n                .sum()\n            )\n        if (loss &gt; -IMPOSSIBLE).any():\n            logger.warning(\n                \"You likely have an impossible transition in your \"\n                \"training data NER tags, skipping this batch.\"\n            )\n            loss = torch.zeros(1, dtype=torch.float, device=embeds.device)\n        loss = loss.sum().unsqueeze(0) / 100.0\n    if is_predict:\n        pred_tags = self.crf.decode(crf_logits, crf_mask).reshape(\n            n_samples, self.n_labels, n_tokens\n        )\n        pred_spans = self.crf.tags_to_spans(pred_tags)\n        additional_outputs[\"spans\"] = pred_spans\n    return loss\n</code></pre>"},{"location":"reference/models/torch/","title":"<code>edsnlp.models.torch</code>","text":""},{"location":"reference/models/torch/crf/","title":"<code>edsnlp.models.torch.crf</code>","text":""},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF","title":"<code>LinearChainCRF</code>","text":"<p>         Bases: <code>torch.nn.Module</code></p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>class LinearChainCRF(torch.nn.Module):\n    def __init__(\n        self,\n        forbidden_transitions,\n        start_forbidden_transitions=None,\n        end_forbidden_transitions=None,\n        learnable_transitions=True,\n        with_start_end_transitions=True,\n    ):\n\"\"\"\n        A linear chain CRF in Pytorch\n\n        Parameters\n        ----------\n        forbidden_transitions: torch.BoolTensor\n            Shape: n_tags * n_tags\n            Impossible transitions (1 means impossible) from position n to position n+1\n        start_forbidden_transitions: Optional[torch.BoolTensor]\n            Shape: n_tags\n            Impossible transitions at the start of a sequence\n        end_forbidden_transitions Optional[torch.BoolTensor]\n            Shape: n_tags\n            Impossible transitions at the end of a sequence\n        learnable_transitions: bool\n            Should we learn transition scores to complete the\n            constraints ?\n        with_start_end_transitions:\n            Should we apply start-end transitions.\n            If learnable_transitions is True, learn start/end transition scores\n        \"\"\"\n        super().__init__()\n\n        num_tags = forbidden_transitions.shape[0]\n\n        self.register_buffer(\"forbidden_transitions\", forbidden_transitions.bool())\n        if start_forbidden_transitions is not None:\n            self.register_buffer(\n                \"start_forbidden_transitions\", start_forbidden_transitions.bool()\n            )\n        else:\n            self.register_buffer(\n                \"start_forbidden_transitions\", torch.zeros(num_tags, dtype=torch.bool)\n            )\n        if end_forbidden_transitions is not None:\n            self.register_buffer(\n                \"end_forbidden_transitions\", end_forbidden_transitions.bool()\n            )\n        else:\n            self.register_buffer(\n                \"end_forbidden_transitions\", torch.zeros(num_tags, dtype=torch.bool)\n            )\n\n        if learnable_transitions:\n            self.transitions = torch.nn.Parameter(\n                torch.zeros_like(forbidden_transitions, dtype=torch.float)\n            )\n        else:\n            self.register_buffer(\n                \"transitions\",\n                torch.zeros_like(forbidden_transitions, dtype=torch.float),\n            )\n\n        if learnable_transitions and with_start_end_transitions:\n            self.start_transitions = torch.nn.Parameter(\n                torch.zeros(num_tags, dtype=torch.float)\n            )\n        else:\n            self.register_buffer(\n                \"start_transitions\", torch.zeros(num_tags, dtype=torch.float)\n            )\n\n        if learnable_transitions and with_start_end_transitions:\n            self.end_transitions = torch.nn.Parameter(\n                torch.zeros(num_tags, dtype=torch.float)\n            )\n        else:\n            self.register_buffer(\n                \"end_transitions\", torch.zeros(num_tags, dtype=torch.float)\n            )\n\n    def decode(self, emissions, mask):\n\"\"\"\n        Decodes a sequence of tag scores using the Viterbi algorithm\n\n        Parameters\n        ----------\n        emissions: torch.FloatTensor\n            Shape: ... * n_tokens * n_tags\n        mask: torch.BoolTensor\n            Shape: ... * n_tokens\n\n        Returns\n        -------\n        torch.LongTensor\n            Backtrack indices (= argmax), ie best tag sequence\n        \"\"\"\n        transitions = self.transitions.masked_fill(\n            self.forbidden_transitions, IMPOSSIBLE\n        )\n        start_transitions = self.start_transitions.masked_fill(\n            self.start_forbidden_transitions, IMPOSSIBLE\n        )\n        end_transitions = self.end_transitions.masked_fill(\n            self.end_forbidden_transitions, IMPOSSIBLE\n        )\n        n_samples, n_tokens = mask.shape\n\n        emissions[..., 1:][~mask] = IMPOSSIBLE\n        emissions = emissions.transpose(0, 1)\n\n        # emissions: n_tokens * n_samples * n_tags\n        out = [emissions[0] + start_transitions]\n        backtrack = []\n\n        for k in range(1, len(emissions)):\n            res, indices = max_reduce(out[-1], transitions)\n            backtrack.append(indices)\n            out.append(res + emissions[k])\n\n        res, indices = max_reduce(out[-1], end_transitions.unsqueeze(-1))\n        path = torch.zeros(n_samples, n_tokens, dtype=torch.long)\n        path[:, -1] = indices.squeeze(-1)\n\n        path_range = torch.arange(n_samples, device=path.device)\n        if len(backtrack) &gt; 1:\n            # Backward max path following\n            for k, b in enumerate(backtrack[::-1]):\n                path[:, -k - 2] = b[path_range, path[:, -k - 1]]\n\n        return path\n\n    def marginal(self, emissions, mask):\n\"\"\"\n        Compute the marginal log-probabilities of the tags\n        given the emissions and the transition probabilities and\n        constraints of the CRF\n\n        We could use the `propagate` method but this implementation\n        is faster.\n\n        Parameters\n        ----------\n        emissions: torch.FloatTensor\n            Shape: ... * n_tokens * n_tags\n        mask: torch.BoolTensor\n            Shape: ... * n_tokens\n\n        Returns\n        -------\n        torch.FloatTensor\n            Shape: ... * n_tokens * n_tags\n        \"\"\"\n        device = emissions.device\n\n        transitions = self.transitions.masked_fill(\n            self.forbidden_transitions, IMPOSSIBLE\n        )\n        start_transitions = self.start_transitions.masked_fill(\n            self.start_forbidden_transitions, IMPOSSIBLE\n        )\n        end_transitions = self.end_transitions.masked_fill(\n            self.end_forbidden_transitions, IMPOSSIBLE\n        )\n\n        bi_transitions = torch.stack([transitions, transitions.t()], dim=0)\n\n        # add start transitions (ie cannot start with ...)\n        emissions[:, 0] = emissions[:, 0] + start_transitions\n\n        # add end transitions (ie cannot end with ...): flip the emissions along the\n        # token axis, and add the end transitions\n        # emissions = masked_flip(emissions, mask, dim_x=1)\n        emissions[\n            torch.arange(mask.shape[0], device=device), mask.long().sum(1) - 1\n        ] = (\n            emissions[\n                torch.arange(mask.shape[0], device=device),\n                mask.long().sum(1) - 1,\n            ]\n            + end_transitions\n        )\n\n        # stack start -&gt; end emissions (needs to flip the previously flipped emissions),\n        # and end -&gt; start emissions\n        bi_emissions = torch.stack(\n            [emissions, masked_flip(emissions, mask, dim_x=1)], 1\n        )\n        bi_emissions = bi_emissions.transpose(0, 2)\n\n        out = [bi_emissions[0]]\n        for k in range(1, len(bi_emissions)):\n            res = logsumexp_reduce(out[-1], bi_transitions)\n            out.append(res + bi_emissions[k])\n        out = torch.stack(out, dim=0).transpose(0, 2)\n\n        forward = out[:, 0]\n        backward = masked_flip(out[:, 1], mask, dim_x=1)\n        backward_z = backward[:, 0].logsumexp(-1)\n\n        return forward + backward - emissions - backward_z[:, None, None]\n\n    def forward(self, emissions, mask, target):\n\"\"\"\n        Compute the posterior reduced log-probabilities of the tags\n        given the emissions and the transition probabilities and\n        constraints of the CRF, ie the loss.\n\n\n        We could use the `propagate` method but this implementation\n        is faster.\n\n        Parameters\n        ----------\n        emissions: torch.FloatTensor\n            Shape: ... * n_tokens * n_tags\n        mask: torch.BoolTensor\n            Shape: ... * n_tokens\n        target: torch.BoolTensor\n            Shape: ... * n_tokens * n_tags\n            The target tags represented with 1-hot encoding\n            We use 1-hot instead of long format to handle\n            cases when multiple tags at a given position are\n            allowed during training.\n\n        Returns\n        -------\n        torch.FloatTensor\n            Shape: ...\n            The loss\n        \"\"\"\n        transitions = self.transitions.masked_fill(\n            self.forbidden_transitions, IMPOSSIBLE\n        )\n        start_transitions = self.start_transitions.masked_fill(\n            self.start_forbidden_transitions, IMPOSSIBLE\n        )\n        end_transitions = self.end_transitions.masked_fill(\n            self.end_forbidden_transitions, IMPOSSIBLE\n        )\n\n        bi_emissions = torch.stack(\n            [emissions.masked_fill(~target, IMPOSSIBLE), emissions], 1\n        ).transpose(0, 2)\n\n        # emissions: n_samples * n_tokens * n_tags\n        # bi_emissions: n_tokens * 2 * n_samples * n_tags\n        out = [bi_emissions[0] + start_transitions]\n\n        for k in range(1, len(bi_emissions)):\n            res = logsumexp_reduce(out[-1], transitions)\n            out.append(res + bi_emissions[k])\n        out = torch.stack(out, dim=0).transpose(0, 2)\n        # n_samples * 2 * n_tokens * n_tags\n        z = (\n            masked_flip(out, mask.unsqueeze(1).repeat(1, 2, 1), dim_x=2)[:, :, 0]\n            + end_transitions\n        )\n        supervised_z = z[:, 0].logsumexp(-1)\n        unsupervised_z = z[:, 1].logsumexp(-1)\n        return -(supervised_z - unsupervised_z)\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.__init__","title":"<code>__init__(forbidden_transitions, start_forbidden_transitions=None, end_forbidden_transitions=None, learnable_transitions=True, with_start_end_transitions=True)</code>","text":"<p>A linear chain CRF in Pytorch</p> PARAMETER DESCRIPTION <code>forbidden_transitions</code> <p>Shape: n_tags * n_tags Impossible transitions (1 means impossible) from position n to position n+1</p> <p> </p> <code>start_forbidden_transitions</code> <p>Shape: n_tags Impossible transitions at the start of a sequence</p> <p> DEFAULT: <code>None</code> </p> <code>end_forbidden_transitions</code> <p>Shape: n_tags Impossible transitions at the end of a sequence</p> <p> DEFAULT: <code>None</code> </p> <code>learnable_transitions</code> <p>Should we learn transition scores to complete the constraints ?</p> <p> DEFAULT: <code>True</code> </p> <code>with_start_end_transitions</code> <p>Should we apply start-end transitions. If learnable_transitions is True, learn start/end transition scores</p> <p> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>def __init__(\n    self,\n    forbidden_transitions,\n    start_forbidden_transitions=None,\n    end_forbidden_transitions=None,\n    learnable_transitions=True,\n    with_start_end_transitions=True,\n):\n\"\"\"\n    A linear chain CRF in Pytorch\n\n    Parameters\n    ----------\n    forbidden_transitions: torch.BoolTensor\n        Shape: n_tags * n_tags\n        Impossible transitions (1 means impossible) from position n to position n+1\n    start_forbidden_transitions: Optional[torch.BoolTensor]\n        Shape: n_tags\n        Impossible transitions at the start of a sequence\n    end_forbidden_transitions Optional[torch.BoolTensor]\n        Shape: n_tags\n        Impossible transitions at the end of a sequence\n    learnable_transitions: bool\n        Should we learn transition scores to complete the\n        constraints ?\n    with_start_end_transitions:\n        Should we apply start-end transitions.\n        If learnable_transitions is True, learn start/end transition scores\n    \"\"\"\n    super().__init__()\n\n    num_tags = forbidden_transitions.shape[0]\n\n    self.register_buffer(\"forbidden_transitions\", forbidden_transitions.bool())\n    if start_forbidden_transitions is not None:\n        self.register_buffer(\n            \"start_forbidden_transitions\", start_forbidden_transitions.bool()\n        )\n    else:\n        self.register_buffer(\n            \"start_forbidden_transitions\", torch.zeros(num_tags, dtype=torch.bool)\n        )\n    if end_forbidden_transitions is not None:\n        self.register_buffer(\n            \"end_forbidden_transitions\", end_forbidden_transitions.bool()\n        )\n    else:\n        self.register_buffer(\n            \"end_forbidden_transitions\", torch.zeros(num_tags, dtype=torch.bool)\n        )\n\n    if learnable_transitions:\n        self.transitions = torch.nn.Parameter(\n            torch.zeros_like(forbidden_transitions, dtype=torch.float)\n        )\n    else:\n        self.register_buffer(\n            \"transitions\",\n            torch.zeros_like(forbidden_transitions, dtype=torch.float),\n        )\n\n    if learnable_transitions and with_start_end_transitions:\n        self.start_transitions = torch.nn.Parameter(\n            torch.zeros(num_tags, dtype=torch.float)\n        )\n    else:\n        self.register_buffer(\n            \"start_transitions\", torch.zeros(num_tags, dtype=torch.float)\n        )\n\n    if learnable_transitions and with_start_end_transitions:\n        self.end_transitions = torch.nn.Parameter(\n            torch.zeros(num_tags, dtype=torch.float)\n        )\n    else:\n        self.register_buffer(\n            \"end_transitions\", torch.zeros(num_tags, dtype=torch.float)\n        )\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.decode","title":"<code>decode(emissions, mask)</code>","text":"<p>Decodes a sequence of tag scores using the Viterbi algorithm</p> PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>torch.LongTensor</code> <p>Backtrack indices (= argmax), ie best tag sequence</p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>def decode(self, emissions, mask):\n\"\"\"\n    Decodes a sequence of tag scores using the Viterbi algorithm\n\n    Parameters\n    ----------\n    emissions: torch.FloatTensor\n        Shape: ... * n_tokens * n_tags\n    mask: torch.BoolTensor\n        Shape: ... * n_tokens\n\n    Returns\n    -------\n    torch.LongTensor\n        Backtrack indices (= argmax), ie best tag sequence\n    \"\"\"\n    transitions = self.transitions.masked_fill(\n        self.forbidden_transitions, IMPOSSIBLE\n    )\n    start_transitions = self.start_transitions.masked_fill(\n        self.start_forbidden_transitions, IMPOSSIBLE\n    )\n    end_transitions = self.end_transitions.masked_fill(\n        self.end_forbidden_transitions, IMPOSSIBLE\n    )\n    n_samples, n_tokens = mask.shape\n\n    emissions[..., 1:][~mask] = IMPOSSIBLE\n    emissions = emissions.transpose(0, 1)\n\n    # emissions: n_tokens * n_samples * n_tags\n    out = [emissions[0] + start_transitions]\n    backtrack = []\n\n    for k in range(1, len(emissions)):\n        res, indices = max_reduce(out[-1], transitions)\n        backtrack.append(indices)\n        out.append(res + emissions[k])\n\n    res, indices = max_reduce(out[-1], end_transitions.unsqueeze(-1))\n    path = torch.zeros(n_samples, n_tokens, dtype=torch.long)\n    path[:, -1] = indices.squeeze(-1)\n\n    path_range = torch.arange(n_samples, device=path.device)\n    if len(backtrack) &gt; 1:\n        # Backward max path following\n        for k, b in enumerate(backtrack[::-1]):\n            path[:, -k - 2] = b[path_range, path[:, -k - 1]]\n\n    return path\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.marginal","title":"<code>marginal(emissions, mask)</code>","text":"<p>Compute the marginal log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF</p> <p>We could use the <code>propagate</code> method but this implementation is faster.</p> PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>torch.FloatTensor</code> <p>Shape: ... * n_tokens * n_tags</p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>def marginal(self, emissions, mask):\n\"\"\"\n    Compute the marginal log-probabilities of the tags\n    given the emissions and the transition probabilities and\n    constraints of the CRF\n\n    We could use the `propagate` method but this implementation\n    is faster.\n\n    Parameters\n    ----------\n    emissions: torch.FloatTensor\n        Shape: ... * n_tokens * n_tags\n    mask: torch.BoolTensor\n        Shape: ... * n_tokens\n\n    Returns\n    -------\n    torch.FloatTensor\n        Shape: ... * n_tokens * n_tags\n    \"\"\"\n    device = emissions.device\n\n    transitions = self.transitions.masked_fill(\n        self.forbidden_transitions, IMPOSSIBLE\n    )\n    start_transitions = self.start_transitions.masked_fill(\n        self.start_forbidden_transitions, IMPOSSIBLE\n    )\n    end_transitions = self.end_transitions.masked_fill(\n        self.end_forbidden_transitions, IMPOSSIBLE\n    )\n\n    bi_transitions = torch.stack([transitions, transitions.t()], dim=0)\n\n    # add start transitions (ie cannot start with ...)\n    emissions[:, 0] = emissions[:, 0] + start_transitions\n\n    # add end transitions (ie cannot end with ...): flip the emissions along the\n    # token axis, and add the end transitions\n    # emissions = masked_flip(emissions, mask, dim_x=1)\n    emissions[\n        torch.arange(mask.shape[0], device=device), mask.long().sum(1) - 1\n    ] = (\n        emissions[\n            torch.arange(mask.shape[0], device=device),\n            mask.long().sum(1) - 1,\n        ]\n        + end_transitions\n    )\n\n    # stack start -&gt; end emissions (needs to flip the previously flipped emissions),\n    # and end -&gt; start emissions\n    bi_emissions = torch.stack(\n        [emissions, masked_flip(emissions, mask, dim_x=1)], 1\n    )\n    bi_emissions = bi_emissions.transpose(0, 2)\n\n    out = [bi_emissions[0]]\n    for k in range(1, len(bi_emissions)):\n        res = logsumexp_reduce(out[-1], bi_transitions)\n        out.append(res + bi_emissions[k])\n    out = torch.stack(out, dim=0).transpose(0, 2)\n\n    forward = out[:, 0]\n    backward = masked_flip(out[:, 1], mask, dim_x=1)\n    backward_z = backward[:, 0].logsumexp(-1)\n\n    return forward + backward - emissions - backward_z[:, None, None]\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.LinearChainCRF.forward","title":"<code>forward(emissions, mask, target)</code>","text":"<p>Compute the posterior reduced log-probabilities of the tags given the emissions and the transition probabilities and constraints of the CRF, ie the loss.</p> <p>We could use the <code>propagate</code> method but this implementation is faster.</p> PARAMETER DESCRIPTION <code>emissions</code> <p>Shape: ... * n_tokens * n_tags</p> <p> </p> <code>mask</code> <p>Shape: ... * n_tokens</p> <p> </p> <code>target</code> <p>Shape: ... * n_tokens * n_tags The target tags represented with 1-hot encoding We use 1-hot instead of long format to handle cases when multiple tags at a given position are allowed during training.</p> <p> </p> RETURNS DESCRIPTION <code>torch.FloatTensor</code> <p>Shape: ... The loss</p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>def forward(self, emissions, mask, target):\n\"\"\"\n    Compute the posterior reduced log-probabilities of the tags\n    given the emissions and the transition probabilities and\n    constraints of the CRF, ie the loss.\n\n\n    We could use the `propagate` method but this implementation\n    is faster.\n\n    Parameters\n    ----------\n    emissions: torch.FloatTensor\n        Shape: ... * n_tokens * n_tags\n    mask: torch.BoolTensor\n        Shape: ... * n_tokens\n    target: torch.BoolTensor\n        Shape: ... * n_tokens * n_tags\n        The target tags represented with 1-hot encoding\n        We use 1-hot instead of long format to handle\n        cases when multiple tags at a given position are\n        allowed during training.\n\n    Returns\n    -------\n    torch.FloatTensor\n        Shape: ...\n        The loss\n    \"\"\"\n    transitions = self.transitions.masked_fill(\n        self.forbidden_transitions, IMPOSSIBLE\n    )\n    start_transitions = self.start_transitions.masked_fill(\n        self.start_forbidden_transitions, IMPOSSIBLE\n    )\n    end_transitions = self.end_transitions.masked_fill(\n        self.end_forbidden_transitions, IMPOSSIBLE\n    )\n\n    bi_emissions = torch.stack(\n        [emissions.masked_fill(~target, IMPOSSIBLE), emissions], 1\n    ).transpose(0, 2)\n\n    # emissions: n_samples * n_tokens * n_tags\n    # bi_emissions: n_tokens * 2 * n_samples * n_tags\n    out = [bi_emissions[0] + start_transitions]\n\n    for k in range(1, len(bi_emissions)):\n        res = logsumexp_reduce(out[-1], transitions)\n        out.append(res + bi_emissions[k])\n    out = torch.stack(out, dim=0).transpose(0, 2)\n    # n_samples * 2 * n_tokens * n_tags\n    z = (\n        masked_flip(out, mask.unsqueeze(1).repeat(1, 2, 1), dim_x=2)[:, :, 0]\n        + end_transitions\n    )\n    supervised_z = z[:, 0].logsumexp(-1)\n    unsupervised_z = z[:, 1].logsumexp(-1)\n    return -(supervised_z - unsupervised_z)\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder","title":"<code>MultiLabelBIOULDecoder</code>","text":"<p>         Bases: <code>LinearChainCRF</code></p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>class MultiLabelBIOULDecoder(LinearChainCRF):\n    def __init__(\n        self,\n        num_labels,\n        with_start_end_transitions=True,\n        learnable_transitions=True,\n    ):\n\"\"\"\n        Create a linear chain CRF with hard constraints to enforce the BIOUL tagging\n        scheme\n\n        Parameters\n        ----------\n        num_labels: int\n        with_start_end_transitions: bool\n        learnable_transitions: bool\n        \"\"\"\n        O, I, B, L, U = 0, 1, 2, 3, 4\n\n        num_tags = 1 + num_labels * 4\n        self.num_tags = num_tags\n        forbidden_transitions = torch.ones(num_tags, num_tags, dtype=torch.bool)\n        forbidden_transitions[O, O] = 0  # O to O\n        for i in range(num_labels):\n            STRIDE = 4 * i\n            for j in range(num_labels):\n                STRIDE_J = j * 4\n                forbidden_transitions[L + STRIDE, B + STRIDE_J] = 0  # L-i to B-j\n                forbidden_transitions[L + STRIDE, U + STRIDE_J] = 0  # L-i to U-j\n                forbidden_transitions[U + STRIDE, B + STRIDE_J] = 0  # U-i to B-j\n                forbidden_transitions[U + STRIDE, U + STRIDE_J] = 0  # U-i to U-j\n\n            forbidden_transitions[O, B + STRIDE] = 0  # O to B-i\n            forbidden_transitions[B + STRIDE, I + STRIDE] = 0  # B-i to I-i\n            forbidden_transitions[I + STRIDE, I + STRIDE] = 0  # I-i to I-i\n            forbidden_transitions[I + STRIDE, L + STRIDE] = 0  # I-i to L-i\n            forbidden_transitions[B + STRIDE, L + STRIDE] = 0  # B-i to L-i\n\n            forbidden_transitions[L + STRIDE, O] = 0  # L-i to O\n            forbidden_transitions[O, U + STRIDE] = 0  # O to U-i\n            forbidden_transitions[U + STRIDE, O] = 0  # U-i to O\n\n        start_forbidden_transitions = torch.zeros(num_tags, dtype=torch.bool)\n        if with_start_end_transitions:\n            for i in range(num_labels):\n                STRIDE = 4 * i\n                start_forbidden_transitions[I + STRIDE] = 1  # forbidden to start by I-i\n                start_forbidden_transitions[L + STRIDE] = 1  # forbidden to start by L-i\n\n        end_forbidden_transitions = torch.zeros(num_tags, dtype=torch.bool)\n        if with_start_end_transitions:\n            for i in range(num_labels):\n                STRIDE = 4 * i\n                end_forbidden_transitions[I + STRIDE] = 1  # forbidden to end by I-i\n                end_forbidden_transitions[B + STRIDE] = 1  # forbidden to end by B-i\n\n        super().__init__(\n            forbidden_transitions,\n            start_forbidden_transitions,\n            end_forbidden_transitions,\n            with_start_end_transitions=with_start_end_transitions,\n            learnable_transitions=learnable_transitions,\n        )\n\n    @staticmethod\n    def spans_to_tags(\n        spans: torch.Tensor, n_samples: int, n_labels: int, n_tokens: int\n    ):\n\"\"\"\n        Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end)\n        to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens\n\n        Parameters\n        ----------\n        spans: torch.Tensor\n        n_samples: int\n        n_labels: int\n        n_tokens: int\n\n        Returns\n        -------\n        torch.Tensor\n        \"\"\"\n        device = spans.device\n        cpu = torch.device(\"cpu\")\n        if not len(spans):\n            return torch.zeros(\n                n_samples, n_labels, n_tokens, dtype=torch.long, device=device\n            )\n        doc_indices, label_indices, begins, ends = spans.cpu().unbind(-1)\n        ends = ends - 1\n\n        pos = torch.arange(n_tokens, device=cpu)\n        b_tags, l_tags, u_tags, i_tags = torch.zeros(\n            4, n_samples, n_labels, n_tokens, dtype=torch.bool, device=cpu\n        ).unbind(0)\n        tags = torch.zeros(n_samples, n_labels, n_tokens, dtype=torch.long, device=cpu)\n        where_u = begins == ends\n        u_tags[doc_indices[where_u], label_indices[where_u], begins[where_u]] = True\n        b_tags[doc_indices[~where_u], label_indices[~where_u], begins[~where_u]] = True\n        l_tags[doc_indices[~where_u], label_indices[~where_u], ends[~where_u]] = True\n        i_tags.view(-1, n_tokens).index_add_(\n            0,\n            doc_indices * n_labels + label_indices,\n            (begins.unsqueeze(-1) &lt; pos) &amp; (pos &lt; ends.unsqueeze(-1)),\n        )\n        tags[u_tags] = 4\n        tags[b_tags] = 2\n        tags[l_tags] = 3\n        tags[i_tags.bool()] = 1\n        return tags.to(device)\n\n    @staticmethod\n    def tags_to_spans(tags):\n\"\"\"\n        Convert a sequence of multiple label BIOUL tags to a sequence of spans\n\n        Parameters\n        ----------\n        tags: torch.LongTensor\n            Shape: n_samples * n_labels * n_tokens\n        mask: torch.BoolTensor\n            Shape: n_samples * n_labels * n_tokens\n\n        Returns\n        -------\n        torch.LongTensor\n            Shape: n_spans *  4\n            (doc_idx, label_idx, begin, end)\n        \"\"\"\n        return torch.cat(\n            [\n                torch.nonzero((tags == 4) | (tags == 2)),\n                torch.nonzero((tags == 4) | (tags == 3))[..., [-1]] + 1,\n            ],\n            dim=-1,\n        )\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.__init__","title":"<code>__init__(num_labels, with_start_end_transitions=True, learnable_transitions=True)</code>","text":"<p>Create a linear chain CRF with hard constraints to enforce the BIOUL tagging scheme</p> PARAMETER DESCRIPTION <code>num_labels</code> <p> </p> <code>with_start_end_transitions</code> <p> DEFAULT: <code>True</code> </p> <code>learnable_transitions</code> <p> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>def __init__(\n    self,\n    num_labels,\n    with_start_end_transitions=True,\n    learnable_transitions=True,\n):\n\"\"\"\n    Create a linear chain CRF with hard constraints to enforce the BIOUL tagging\n    scheme\n\n    Parameters\n    ----------\n    num_labels: int\n    with_start_end_transitions: bool\n    learnable_transitions: bool\n    \"\"\"\n    O, I, B, L, U = 0, 1, 2, 3, 4\n\n    num_tags = 1 + num_labels * 4\n    self.num_tags = num_tags\n    forbidden_transitions = torch.ones(num_tags, num_tags, dtype=torch.bool)\n    forbidden_transitions[O, O] = 0  # O to O\n    for i in range(num_labels):\n        STRIDE = 4 * i\n        for j in range(num_labels):\n            STRIDE_J = j * 4\n            forbidden_transitions[L + STRIDE, B + STRIDE_J] = 0  # L-i to B-j\n            forbidden_transitions[L + STRIDE, U + STRIDE_J] = 0  # L-i to U-j\n            forbidden_transitions[U + STRIDE, B + STRIDE_J] = 0  # U-i to B-j\n            forbidden_transitions[U + STRIDE, U + STRIDE_J] = 0  # U-i to U-j\n\n        forbidden_transitions[O, B + STRIDE] = 0  # O to B-i\n        forbidden_transitions[B + STRIDE, I + STRIDE] = 0  # B-i to I-i\n        forbidden_transitions[I + STRIDE, I + STRIDE] = 0  # I-i to I-i\n        forbidden_transitions[I + STRIDE, L + STRIDE] = 0  # I-i to L-i\n        forbidden_transitions[B + STRIDE, L + STRIDE] = 0  # B-i to L-i\n\n        forbidden_transitions[L + STRIDE, O] = 0  # L-i to O\n        forbidden_transitions[O, U + STRIDE] = 0  # O to U-i\n        forbidden_transitions[U + STRIDE, O] = 0  # U-i to O\n\n    start_forbidden_transitions = torch.zeros(num_tags, dtype=torch.bool)\n    if with_start_end_transitions:\n        for i in range(num_labels):\n            STRIDE = 4 * i\n            start_forbidden_transitions[I + STRIDE] = 1  # forbidden to start by I-i\n            start_forbidden_transitions[L + STRIDE] = 1  # forbidden to start by L-i\n\n    end_forbidden_transitions = torch.zeros(num_tags, dtype=torch.bool)\n    if with_start_end_transitions:\n        for i in range(num_labels):\n            STRIDE = 4 * i\n            end_forbidden_transitions[I + STRIDE] = 1  # forbidden to end by I-i\n            end_forbidden_transitions[B + STRIDE] = 1  # forbidden to end by B-i\n\n    super().__init__(\n        forbidden_transitions,\n        start_forbidden_transitions,\n        end_forbidden_transitions,\n        with_start_end_transitions=with_start_end_transitions,\n        learnable_transitions=learnable_transitions,\n    )\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.spans_to_tags","title":"<code>spans_to_tags(spans, n_samples, n_labels, n_tokens)</code>  <code>staticmethod</code>","text":"<p>Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end) to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens</p> PARAMETER DESCRIPTION <code>spans</code> <p> TYPE: <code>torch.Tensor</code> </p> <code>n_samples</code> <p> TYPE: <code>int</code> </p> <code>n_labels</code> <p> TYPE: <code>int</code> </p> <code>n_tokens</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>torch.Tensor</code> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>@staticmethod\ndef spans_to_tags(\n    spans: torch.Tensor, n_samples: int, n_labels: int, n_tokens: int\n):\n\"\"\"\n    Convert a tensor of spans of shape n_spans * (doc_idx, label, begin, end)\n    to a matrix of BIOUL tags of shape n_samples * n_labels * n_tokens\n\n    Parameters\n    ----------\n    spans: torch.Tensor\n    n_samples: int\n    n_labels: int\n    n_tokens: int\n\n    Returns\n    -------\n    torch.Tensor\n    \"\"\"\n    device = spans.device\n    cpu = torch.device(\"cpu\")\n    if not len(spans):\n        return torch.zeros(\n            n_samples, n_labels, n_tokens, dtype=torch.long, device=device\n        )\n    doc_indices, label_indices, begins, ends = spans.cpu().unbind(-1)\n    ends = ends - 1\n\n    pos = torch.arange(n_tokens, device=cpu)\n    b_tags, l_tags, u_tags, i_tags = torch.zeros(\n        4, n_samples, n_labels, n_tokens, dtype=torch.bool, device=cpu\n    ).unbind(0)\n    tags = torch.zeros(n_samples, n_labels, n_tokens, dtype=torch.long, device=cpu)\n    where_u = begins == ends\n    u_tags[doc_indices[where_u], label_indices[where_u], begins[where_u]] = True\n    b_tags[doc_indices[~where_u], label_indices[~where_u], begins[~where_u]] = True\n    l_tags[doc_indices[~where_u], label_indices[~where_u], ends[~where_u]] = True\n    i_tags.view(-1, n_tokens).index_add_(\n        0,\n        doc_indices * n_labels + label_indices,\n        (begins.unsqueeze(-1) &lt; pos) &amp; (pos &lt; ends.unsqueeze(-1)),\n    )\n    tags[u_tags] = 4\n    tags[b_tags] = 2\n    tags[l_tags] = 3\n    tags[i_tags.bool()] = 1\n    return tags.to(device)\n</code></pre>"},{"location":"reference/models/torch/crf/#edsnlp.models.torch.crf.MultiLabelBIOULDecoder.tags_to_spans","title":"<code>tags_to_spans(tags)</code>  <code>staticmethod</code>","text":"<p>Convert a sequence of multiple label BIOUL tags to a sequence of spans</p> PARAMETER DESCRIPTION <code>tags</code> <p>Shape: n_samples * n_labels * n_tokens</p> <p> </p> <code>mask</code> <p>Shape: n_samples * n_labels * n_tokens</p> <p> </p> RETURNS DESCRIPTION <code>torch.LongTensor</code> <p>Shape: n_spans *  4 (doc_idx, label_idx, begin, end)</p> Source code in <code>edsnlp/models/torch/crf.py</code> <pre><code>@staticmethod\ndef tags_to_spans(tags):\n\"\"\"\n    Convert a sequence of multiple label BIOUL tags to a sequence of spans\n\n    Parameters\n    ----------\n    tags: torch.LongTensor\n        Shape: n_samples * n_labels * n_tokens\n    mask: torch.BoolTensor\n        Shape: n_samples * n_labels * n_tokens\n\n    Returns\n    -------\n    torch.LongTensor\n        Shape: n_spans *  4\n        (doc_idx, label_idx, begin, end)\n    \"\"\"\n    return torch.cat(\n        [\n            torch.nonzero((tags == 4) | (tags == 2)),\n            torch.nonzero((tags == 4) | (tags == 3))[..., [-1]] + 1,\n        ],\n        dim=-1,\n    )\n</code></pre>"},{"location":"reference/pipelines/","title":"<code>edsnlp.pipelines</code>","text":""},{"location":"reference/pipelines/base/","title":"<code>edsnlp.pipelines.base</code>","text":""},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent","title":"<code>BaseComponent</code>","text":"<p>         Bases: <code>object</code></p> <p>The <code>BaseComponent</code> adds a <code>set_extensions</code> method, called at the creation of the object.</p> <p>It helps decouple the initialisation of the pipeline from the creation of extensions, and is particularly usefull when distributing EDSNLP on a cluster, since the serialisation mechanism imposes that the extensions be reset.</p> Source code in <code>edsnlp/pipelines/base.py</code> <pre><code>class BaseComponent(object):\n\"\"\"\n    The `BaseComponent` adds a `set_extensions` method,\n    called at the creation of the object.\n\n    It helps decouple the initialisation of the pipeline from\n    the creation of extensions, and is particularly usefull when\n    distributing EDSNLP on a cluster, since the serialisation mechanism\n    imposes that the extensions be reset.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\"\"\"\n        Set `Doc`, `Span` and `Token` extensions.\n        \"\"\"\n        pass\n\n    def _boundaries(\n        self, doc: Doc, terminations: Optional[List[Span]] = None\n    ) -&gt; List[Tuple[int, int]]:\n\"\"\"\n        Create sub sentences based sentences and terminations found in text.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n        terminations:\n            List of tuples with (match_id, start, end)\n\n        Returns\n        -------\n        boundaries:\n            List of tuples with (start, end) of spans\n        \"\"\"\n\n        if terminations is None:\n            terminations = []\n\n        sent_starts = [sent.start for sent in doc.sents]\n        termination_starts = [t.start for t in terminations]\n\n        starts = sent_starts + termination_starts + [len(doc)]\n\n        # Remove duplicates\n        starts = list(set(starts))\n\n        # Sort starts\n        starts.sort()\n\n        boundaries = [(start, end) for start, end in zip(starts[:-1], starts[1:])]\n\n        return boundaries\n</code></pre>"},{"location":"reference/pipelines/base/#edsnlp.pipelines.base.BaseComponent.set_extensions","title":"<code>set_extensions()</code>  <code>classmethod</code>","text":"<p>Set <code>Doc</code>, <code>Span</code> and <code>Token</code> extensions.</p> Source code in <code>edsnlp/pipelines/base.py</code> <pre><code>@classmethod\ndef set_extensions(cls) -&gt; None:\n\"\"\"\n    Set `Doc`, `Span` and `Token` extensions.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/pipelines/factories/","title":"<code>edsnlp.pipelines.factories</code>","text":""},{"location":"reference/pipelines/terminations/","title":"<code>edsnlp.pipelines.terminations</code>","text":""},{"location":"reference/pipelines/core/","title":"<code>edsnlp.pipelines.core</code>","text":""},{"location":"reference/pipelines/core/context/","title":"<code>edsnlp.pipelines.core.context</code>","text":""},{"location":"reference/pipelines/core/context/context/","title":"<code>edsnlp.pipelines.core.context.context</code>","text":""},{"location":"reference/pipelines/core/context/context/#edsnlp.pipelines.core.context.context.ContextAdder","title":"<code>ContextAdder</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Provides a generic context adder component.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>context</code> <p>The list of extensions to add to the <code>Doc</code></p> <p> TYPE: <code>List[str]</code> </p> Source code in <code>edsnlp/pipelines/core/context/context.py</code> <pre><code>class ContextAdder(BaseComponent):\n\"\"\"\n    Provides a generic context adder component.\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    context : List[str]\n        The list of extensions to add to the `Doc`\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        context: List[str],\n    ):\n\n        self.nlp = nlp\n        self.context = context\n        self.set_extensions()\n\n    def set_extensions(self):\n        for col in self.context:\n            if not Doc.has_extension(col):\n                Doc.set_extension(col, default=None)\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/context/factory/","title":"<code>edsnlp.pipelines.core.context.factory</code>","text":""},{"location":"reference/pipelines/core/contextual_matcher/","title":"<code>edsnlp.pipelines.core.contextual_matcher</code>","text":""},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/","title":"<code>edsnlp.pipelines.core.contextual_matcher.contextual_matcher</code>","text":""},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher","title":"<code>ContextualMatcher</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Allows additional matching in the surrounding context of the main match group, for qualification/filtering.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'expand'</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py</code> <pre><code>class ContextualMatcher(BaseComponent):\n\"\"\"\n    Allows additional matching in the surrounding context of the main match group,\n    for qualification/filtering.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy `Language` object.\n    name : str\n        The name of the pipe\n    patterns: Union[Dict[str, Any], List[Dict[str, Any]]]\n        The configuration dictionary\n    assign_as_span : bool\n        Whether to store eventual extractions defined via the `assign` key as Spans\n        or as string\n    attr : str\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n    ignore_excluded : bool\n        Whether to skip excluded tokens during matching.\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n    alignment_mode : str\n        Overwrite alignment mode.\n    regex_flags : Union[re.RegexFlag, int]\n        RegExp flags to use when matching, filtering and assigning (See\n        [here](https://docs.python.org/3/library/re.html#flags))\n    include_assigned : bool\n        Whether to include (eventual) assign matches to the final entity\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        name: str,\n        patterns: Union[Dict[str, Any], List[Dict[str, Any]]],\n        assign_as_span: bool = False,\n        alignment_mode: str = \"expand\",\n        attr: str = \"NORM\",\n        regex_flags: Union[re.RegexFlag, int] = 0,\n        ignore_excluded: bool = False,\n        ignore_space_tokens: bool = False,\n        include_assigned: bool = False,\n    ):\n        self.name = name\n        self.nlp = nlp\n        self.attr = attr\n        self.assign_as_span = assign_as_span\n        self.ignore_excluded = ignore_excluded\n        self.alignment_mode = alignment_mode\n        self.regex_flags = regex_flags\n        self.include_assigned = include_assigned\n\n        # Configuration parsing\n        patterns = models.FullConfig.parse_obj(patterns).__root__\n        self.patterns = {pattern.source: pattern for pattern in patterns}\n\n        # Matchers for the anchors\n        self.phrase_matcher = EDSPhraseMatcher(\n            self.nlp.vocab,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n        )\n        self.regex_matcher = RegexMatcher(\n            attr=attr,\n            flags=regex_flags,\n            ignore_excluded=ignore_excluded,\n            alignment_mode=alignment_mode,\n        )\n\n        self.phrase_matcher.build_patterns(\n            nlp=nlp,\n            terms={\n                source: {\n                    \"patterns\": p.terms,\n                }\n                for source, p in self.patterns.items()\n            },\n        )\n        self.regex_matcher.build_patterns(\n            regex={\n                source: {\n                    \"regex\": p.regex,\n                    \"attr\": p.regex_attr,\n                    \"flags\": p.regex_flags,\n                }\n                for source, p in self.patterns.items()\n            }\n        )\n\n        self.exclude_matchers = defaultdict(\n            list\n        )  # Will contain all the exclusion matchers\n        self.assign_matchers = defaultdict(list)  # Will contain all the assign matchers\n\n        # Will contain the reduce mode (for each source and assign matcher)\n        self.reduce_mode = {}\n\n        # Will contain the name of the assign matcher from which\n        # entity will be replaced (for each source)\n        self.replace_key = {}\n\n        for source, p in self.patterns.items():\n\n            p = p.dict()\n\n            for exclude in p[\"exclude\"]:\n\n                exclude_matcher = RegexMatcher(\n                    attr=p[\"regex_attr\"] or self.attr,\n                    flags=p[\"regex_flags\"] or self.regex_flags,\n                    ignore_excluded=ignore_excluded,\n                    alignment_mode=\"expand\",\n                )\n\n                exclude_matcher.build_patterns(regex={\"exclude\": exclude[\"regex\"]})\n\n                self.exclude_matchers[source].append(\n                    dict(\n                        matcher=exclude_matcher,\n                        window=exclude[\"window\"],\n                    )\n                )\n\n            replace_key = None\n\n            for assign in p[\"assign\"]:\n\n                assign_matcher = RegexMatcher(\n                    attr=p[\"regex_attr\"] or self.attr,\n                    flags=p[\"regex_flags\"] or self.regex_flags,\n                    ignore_excluded=ignore_excluded,\n                    ignore_space_tokens=ignore_space_tokens,\n                    alignment_mode=alignment_mode,\n                    span_from_group=True,\n                )\n\n                assign_matcher.build_patterns(\n                    regex={assign[\"name\"]: assign[\"regex\"]},\n                )\n\n                self.assign_matchers[source].append(\n                    dict(\n                        name=assign[\"name\"],\n                        matcher=assign_matcher,\n                        window=assign[\"window\"],\n                        replace_entity=assign[\"replace_entity\"],\n                        reduce_mode=assign[\"reduce_mode\"],\n                    )\n                )\n\n                if assign[\"replace_entity\"]:\n                    # We know that there is only one assign name\n                    # with `replace_entity==True`\n                    # from PyDantic validation\n                    replace_key = assign[\"name\"]\n\n            self.replace_key[source] = replace_key\n\n            self.reduce_mode[source] = {\n                d[\"name\"]: d[\"reduce_mode\"] for d in self.assign_matchers[source]\n            }\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        if not Span.has_extension(\"assigned\"):\n            Span.set_extension(\"assigned\", default=dict())\n        if not Span.has_extension(\"source\"):\n            Span.set_extension(\"source\", default=None)\n\n    def filter_one(self, span: Span) -&gt; Span:\n\"\"\"\n        Filter extracted entity based on the \"exclusion filter\" mentionned\n        in the configuration\n\n        Parameters\n        ----------\n        span : Span\n            Span to filter\n\n        Returns\n        -------\n        Optional[Span]\n            None if the span was filtered, the span else\n        \"\"\"\n        source = span.label_\n        to_keep = True\n        for matcher in self.exclude_matchers[source]:\n\n            window = matcher[\"window\"]\n            snippet = get_window(\n                doclike=span,\n                window=window,\n            )\n\n            if (\n                next(\n                    matcher[\"matcher\"](snippet, as_spans=True),\n                    None,\n                )\n                is not None\n            ):\n                to_keep = False\n                logger.trace(f\"Entity {span} was filtered out\")\n                break\n\n        if to_keep:\n            return span\n\n    def assign_one(self, span: Span) -&gt; Span:\n\"\"\"\n        Get additional information in the context\n        of each entity. This function will populate two custom attributes:\n\n        - `ent._.source`\n        - `ent._.assigned`, a dictionary with all retrieved information\n\n        Parameters\n        ----------\n        span : Span\n            Span to enrich\n\n        Returns\n        -------\n        Span\n            Span with additional information\n        \"\"\"\n\n        if span is None:\n            yield from []\n            return\n\n        source = span.label_\n        assigned_dict = models.AssignDict(reduce_mode=self.reduce_mode[source])\n        replace_key = None\n\n        for matcher in self.assign_matchers[source]:\n\n            attr = self.patterns[source].regex_attr or matcher[\"matcher\"].default_attr\n            window = matcher[\"window\"]\n            replace_entity = matcher[\"replace_entity\"]  # Boolean\n\n            snippet = get_window(\n                doclike=span,\n                window=window,\n            )\n\n            # Getting the matches\n            assigned_list = list(matcher[\"matcher\"].match(snippet))\n\n            assigned_list = [\n                (span, span)\n                if not match.groups()\n                else (\n                    span,\n                    create_span(\n                        doclike=snippet,\n                        start_char=match.start(0),\n                        end_char=match.end(0),\n                        key=matcher[\"matcher\"].regex[0][0],\n                        attr=matcher[\"matcher\"].regex[0][2],\n                        alignment_mode=matcher[\"matcher\"].regex[0][5],\n                        ignore_excluded=matcher[\"matcher\"].regex[0][3],\n                        ignore_space_tokens=matcher[\"matcher\"].regex[0][4],\n                    ),\n                )\n                for (span, match) in assigned_list\n            ]\n\n            # assigned_list now contains tuples with\n            # - the first element being the span extracted from the group\n            # - the second element being the full match\n\n            if not assigned_list:  # No match was found\n                continue\n\n            for assigned in assigned_list:\n                if assigned is None:\n                    continue\n                if replace_entity:\n                    replace_key = assigned[1].label_\n\n                # Using he overrid `__setitem__` method from AssignDict here:\n                assigned_dict[assigned[1].label_] = {\n                    \"span\": assigned[1],  # Full span\n                    \"value_span\": assigned[0],  # Span of the group\n                    \"value_text\": get_text(\n                        assigned[0],\n                        attr=attr,\n                        ignore_excluded=self.ignore_excluded,\n                    ),  # Text of the group\n                }\n                logger.trace(f\"Assign key {matcher['name']} matched on entity {span}\")\n        if replace_key is None and self.replace_key[source] is not None:\n            # There should have been a replacement, but none was found\n            # So we discard the entity\n            yield from []\n            return\n\n        # Entity replacement\n        if replace_key is not None:\n            replacables = assigned_dict[replace_key][\"span\"]\n            kept_ents = (\n                replacables if isinstance(replacables, list) else [replacables]\n            ).copy()\n\n            if self.include_assigned:\n                # We look for the closest\n                closest = min(\n                    kept_ents,\n                    key=lambda e: abs(e.start - span.start),\n                )\n                kept_ents.remove(closest)\n\n                expandables = flatten(\n                    [a[\"span\"] for k, a in assigned_dict.items() if k != replace_key]\n                ) + [span, closest]\n\n                closest = Span(\n                    span.doc,\n                    min(expandables, key=attrgetter(\"start\")).start,\n                    max(expandables, key=attrgetter(\"end\")).end,\n                    span.label_,\n                )\n\n                kept_ents.append(closest)\n                kept_ents.sort(key=attrgetter(\"start\"))\n\n            for replaced in kept_ents:\n                # Propagating attributes from the anchor\n                replaced._.source = source\n                replaced.label_ = self.name\n\n        else:\n\n            # Entity expansion\n            expandables = flatten([a[\"span\"] for a in assigned_dict.values()])\n\n            if self.include_assigned and expandables:\n\n                span = Span(\n                    span.doc,\n                    min(expandables + [span], key=attrgetter(\"start\")).start,\n                    max(expandables + [span], key=attrgetter(\"end\")).end,\n                    span.label_,\n                )\n\n            span._.source = source\n            span.label_ = self.name\n            kept_ents = [span]\n\n        key = \"value_span\" if self.assign_as_span else \"value_text\"\n\n        for idx, e in enumerate(kept_ents):\n            e._.assigned = {\n                k: v[key][idx]\n                if ((k == replace_key) and self.reduce_mode[source][k] is None)\n                else v[key]\n                for k, v in assigned_dict.items()\n            }\n\n        yield from kept_ents\n\n    def process_one(self, span):\n        filtered = self.filter_one(span)\n        yield from self.assign_one(filtered)\n\n    def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Process the document, looking for named entities.\n\n        Parameters\n        ----------\n        doc : Doc\n            spaCy Doc object\n\n        Returns\n        -------\n        List[Span]\n            List of detected spans.\n        \"\"\"\n\n        matches = self.phrase_matcher(doc, as_spans=True)\n        regex_matches = self.regex_matcher(doc, as_spans=True)\n\n        spans = (*matches, *regex_matches)\n        for span in spans:\n            yield from self.process_one(span)\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Adds spans to document.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for extracted terms.\n        \"\"\"\n\n        ents = list(self.process(doc))\n\n        doc.spans[self.name] = ents\n\n        ents, discarded = filter_spans(list(doc.ents) + ents, return_discarded=True)\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.filter_one","title":"<code>filter_one(span)</code>","text":"<p>Filter extracted entity based on the \"exclusion filter\" mentionned in the configuration</p> PARAMETER DESCRIPTION <code>span</code> <p>Span to filter</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>Optional[Span]</code> <p>None if the span was filtered, the span else</p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py</code> <pre><code>def filter_one(self, span: Span) -&gt; Span:\n\"\"\"\n    Filter extracted entity based on the \"exclusion filter\" mentionned\n    in the configuration\n\n    Parameters\n    ----------\n    span : Span\n        Span to filter\n\n    Returns\n    -------\n    Optional[Span]\n        None if the span was filtered, the span else\n    \"\"\"\n    source = span.label_\n    to_keep = True\n    for matcher in self.exclude_matchers[source]:\n\n        window = matcher[\"window\"]\n        snippet = get_window(\n            doclike=span,\n            window=window,\n        )\n\n        if (\n            next(\n                matcher[\"matcher\"](snippet, as_spans=True),\n                None,\n            )\n            is not None\n        ):\n            to_keep = False\n            logger.trace(f\"Entity {span} was filtered out\")\n            break\n\n    if to_keep:\n        return span\n</code></pre>"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.assign_one","title":"<code>assign_one(span)</code>","text":"<p>Get additional information in the context of each entity. This function will populate two custom attributes:</p> <ul> <li><code>ent._.source</code></li> <li><code>ent._.assigned</code>, a dictionary with all retrieved information</li> </ul> PARAMETER DESCRIPTION <code>span</code> <p>Span to enrich</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>Span</code> <p>Span with additional information</p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py</code> <pre><code>def assign_one(self, span: Span) -&gt; Span:\n\"\"\"\n    Get additional information in the context\n    of each entity. This function will populate two custom attributes:\n\n    - `ent._.source`\n    - `ent._.assigned`, a dictionary with all retrieved information\n\n    Parameters\n    ----------\n    span : Span\n        Span to enrich\n\n    Returns\n    -------\n    Span\n        Span with additional information\n    \"\"\"\n\n    if span is None:\n        yield from []\n        return\n\n    source = span.label_\n    assigned_dict = models.AssignDict(reduce_mode=self.reduce_mode[source])\n    replace_key = None\n\n    for matcher in self.assign_matchers[source]:\n\n        attr = self.patterns[source].regex_attr or matcher[\"matcher\"].default_attr\n        window = matcher[\"window\"]\n        replace_entity = matcher[\"replace_entity\"]  # Boolean\n\n        snippet = get_window(\n            doclike=span,\n            window=window,\n        )\n\n        # Getting the matches\n        assigned_list = list(matcher[\"matcher\"].match(snippet))\n\n        assigned_list = [\n            (span, span)\n            if not match.groups()\n            else (\n                span,\n                create_span(\n                    doclike=snippet,\n                    start_char=match.start(0),\n                    end_char=match.end(0),\n                    key=matcher[\"matcher\"].regex[0][0],\n                    attr=matcher[\"matcher\"].regex[0][2],\n                    alignment_mode=matcher[\"matcher\"].regex[0][5],\n                    ignore_excluded=matcher[\"matcher\"].regex[0][3],\n                    ignore_space_tokens=matcher[\"matcher\"].regex[0][4],\n                ),\n            )\n            for (span, match) in assigned_list\n        ]\n\n        # assigned_list now contains tuples with\n        # - the first element being the span extracted from the group\n        # - the second element being the full match\n\n        if not assigned_list:  # No match was found\n            continue\n\n        for assigned in assigned_list:\n            if assigned is None:\n                continue\n            if replace_entity:\n                replace_key = assigned[1].label_\n\n            # Using he overrid `__setitem__` method from AssignDict here:\n            assigned_dict[assigned[1].label_] = {\n                \"span\": assigned[1],  # Full span\n                \"value_span\": assigned[0],  # Span of the group\n                \"value_text\": get_text(\n                    assigned[0],\n                    attr=attr,\n                    ignore_excluded=self.ignore_excluded,\n                ),  # Text of the group\n            }\n            logger.trace(f\"Assign key {matcher['name']} matched on entity {span}\")\n    if replace_key is None and self.replace_key[source] is not None:\n        # There should have been a replacement, but none was found\n        # So we discard the entity\n        yield from []\n        return\n\n    # Entity replacement\n    if replace_key is not None:\n        replacables = assigned_dict[replace_key][\"span\"]\n        kept_ents = (\n            replacables if isinstance(replacables, list) else [replacables]\n        ).copy()\n\n        if self.include_assigned:\n            # We look for the closest\n            closest = min(\n                kept_ents,\n                key=lambda e: abs(e.start - span.start),\n            )\n            kept_ents.remove(closest)\n\n            expandables = flatten(\n                [a[\"span\"] for k, a in assigned_dict.items() if k != replace_key]\n            ) + [span, closest]\n\n            closest = Span(\n                span.doc,\n                min(expandables, key=attrgetter(\"start\")).start,\n                max(expandables, key=attrgetter(\"end\")).end,\n                span.label_,\n            )\n\n            kept_ents.append(closest)\n            kept_ents.sort(key=attrgetter(\"start\"))\n\n        for replaced in kept_ents:\n            # Propagating attributes from the anchor\n            replaced._.source = source\n            replaced.label_ = self.name\n\n    else:\n\n        # Entity expansion\n        expandables = flatten([a[\"span\"] for a in assigned_dict.values()])\n\n        if self.include_assigned and expandables:\n\n            span = Span(\n                span.doc,\n                min(expandables + [span], key=attrgetter(\"start\")).start,\n                max(expandables + [span], key=attrgetter(\"end\")).end,\n                span.label_,\n            )\n\n        span._.source = source\n        span.label_ = self.name\n        kept_ents = [span]\n\n    key = \"value_span\" if self.assign_as_span else \"value_text\"\n\n    for idx, e in enumerate(kept_ents):\n        e._.assigned = {\n            k: v[key][idx]\n            if ((k == replace_key) and self.reduce_mode[source][k] is None)\n            else v[key]\n            for k, v in assigned_dict.items()\n        }\n\n    yield from kept_ents\n</code></pre>"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.process","title":"<code>process(doc)</code>","text":"<p>Process the document, looking for named entities.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected spans.</p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py</code> <pre><code>def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Process the document, looking for named entities.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy Doc object\n\n    Returns\n    -------\n    List[Span]\n        List of detected spans.\n    \"\"\"\n\n    matches = self.phrase_matcher(doc, as_spans=True)\n    regex_matches = self.regex_matcher(doc, as_spans=True)\n\n    spans = (*matches, *regex_matches)\n    for span in spans:\n        yield from self.process_one(span)\n</code></pre>"},{"location":"reference/pipelines/core/contextual_matcher/contextual_matcher/#edsnlp.pipelines.core.contextual_matcher.contextual_matcher.ContextualMatcher.__call__","title":"<code>__call__(doc)</code>","text":"<p>Adds spans to document.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/contextual_matcher.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Adds spans to document.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for extracted terms.\n    \"\"\"\n\n    ents = list(self.process(doc))\n\n    doc.spans[self.name] = ents\n\n    ents, discarded = filter_spans(list(doc.ents) + ents, return_discarded=True)\n\n    doc.ents = ents\n\n    if \"discarded\" not in doc.spans:\n        doc.spans[\"discarded\"] = []\n    doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/contextual_matcher/factory/","title":"<code>edsnlp.pipelines.core.contextual_matcher.factory</code>","text":""},{"location":"reference/pipelines/core/contextual_matcher/factory/#edsnlp.pipelines.core.contextual_matcher.factory.create_component","title":"<code>create_component(nlp, name, patterns, assign_as_span, alignment_mode, attr, ignore_excluded, ignore_space_tokens, regex_flags, include_assigned)</code>","text":"<p>Allows additional matching in the surrounding context of the main match group, for qualification/filtering.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> </p> <code>patterns</code> <p>The configuration dictionary</p> <p> TYPE: <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> </p> <code>assign_as_span</code> <p>Whether to store eventual extractions defined via the <code>assign</code> key as Spans or as string</p> <p> TYPE: <code>bool</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> </p> <code>alignment_mode</code> <p>Overwrite alignment mode.</p> <p> TYPE: <code>str</code> </p> <code>regex_flags</code> <p>RegExp flags to use when matching, filtering and assigning (See here)</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> </p> <code>include_assigned</code> <p>Whether to include (eventual) assign matches to the final entity</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/factory.py</code> <pre><code>@deprecated_factory(\n    \"contextual-matcher\", \"eds.contextual-matcher\", default_config=DEFAULT_CONFIG\n)\n@Language.factory(\"eds.contextual-matcher\", default_config=DEFAULT_CONFIG)\ndef create_component(\n    nlp: Language,\n    name: str,\n    patterns: Union[Dict[str, Any], List[Dict[str, Any]]],\n    assign_as_span: bool,\n    alignment_mode: str,\n    attr: str,\n    ignore_excluded: bool,\n    ignore_space_tokens: bool,\n    regex_flags: Union[re.RegexFlag, int],\n    include_assigned: bool,\n):\n\"\"\"\n    Allows additional matching in the surrounding context of the main match group,\n    for qualification/filtering.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy `Language` object.\n    name : str\n        The name of the pipe\n    patterns: Union[Dict[str, Any], List[Dict[str, Any]]]\n        The configuration dictionary\n    assign_as_span : bool\n        Whether to store eventual extractions defined via the `assign` key as Spans\n        or as string\n    attr : str\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n    ignore_excluded : bool\n        Whether to skip excluded tokens during matching.\n    alignment_mode : str\n        Overwrite alignment mode.\n    regex_flags : Union[re.RegexFlag, int]\n        RegExp flags to use when matching, filtering and assigning (See\n        [here](https://docs.python.org/3/library/re.html#flags))\n    include_assigned : bool\n        Whether to include (eventual) assign matches to the final entity\n\n    \"\"\"\n\n    return ContextualMatcher(\n        nlp,\n        name,\n        patterns,\n        assign_as_span,\n        alignment_mode,\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        regex_flags=regex_flags,\n        include_assigned=include_assigned,\n    )\n</code></pre>"},{"location":"reference/pipelines/core/contextual_matcher/models/","title":"<code>edsnlp.pipelines.core.contextual_matcher.models</code>","text":""},{"location":"reference/pipelines/core/contextual_matcher/models/#edsnlp.pipelines.core.contextual_matcher.models.AssignDict","title":"<code>AssignDict</code>","text":"<p>         Bases: <code>dict</code></p> <p>Custom dictionary that overrides the setitem method depending on the reduce_mode</p> Source code in <code>edsnlp/pipelines/core/contextual_matcher/models.py</code> <pre><code>class AssignDict(dict):\n\"\"\"\n    Custom dictionary that overrides the __setitem__ method\n    depending on the reduce_mode\n    \"\"\"\n\n    def __init__(self, reduce_mode: dict):\n        super().__init__()\n        self.reduce_mode = reduce_mode\n        self._setitem_ = self.__setitem_options__()\n\n    def __missing__(self, key):\n        return (\n            {\n                \"span\": [],\n                \"value_span\": [],\n                \"value_text\": [],\n            }\n            if self.reduce_mode[key] is None\n            else {}\n        )\n\n    def __setitem__(self, key, value):\n        self._setitem_[self.reduce_mode[key]](key, value)\n\n    def __setitem_options__(self):\n        def keep_list(key, value):\n            old_values = self.__getitem__(key)\n            value[\"span\"] = old_values[\"span\"] + [value[\"span\"]]\n            value[\"value_span\"] = old_values[\"value_span\"] + [value[\"value_span\"]]\n            value[\"value_text\"] = old_values[\"value_text\"] + [value[\"value_text\"]]\n\n            dict.__setitem__(self, key, value)\n\n        def keep_first(key, value):\n            old_values = self.__getitem__(key)\n            if (\n                old_values.get(\"span\") is None\n                or value[\"span\"].start &lt;= old_values[\"span\"].start\n            ):\n                dict.__setitem__(self, key, value)\n\n        def keep_last(key, value):\n            old_values = self.__getitem__(key)\n            if (\n                old_values.get(\"span\") is None\n                or value[\"span\"].start &gt;= old_values[\"span\"].start\n            ):\n                dict.__setitem__(self, key, value)\n\n        return {\n            None: keep_list,\n            \"keep_first\": keep_first,\n            \"keep_last\": keep_last,\n        }\n</code></pre>"},{"location":"reference/pipelines/core/endlines/","title":"<code>edsnlp.pipelines.core.endlines</code>","text":""},{"location":"reference/pipelines/core/endlines/endlines/","title":"<code>edsnlp.pipelines.core.endlines.endlines</code>","text":""},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines","title":"<code>EndLines</code>","text":"<p>         Bases: <code>GenericMatcher</code></p> <p>spaCy Pipeline to detect whether a newline character should be considered a space (ie introduced by the PDF).</p> <p>The pipeline will add the extension <code>end_line</code> to spans and tokens. The <code>end_line</code> attribute is a boolean or <code>None</code>, set to <code>True</code> if the pipeline predicts that the new line is an end line character. Otherwise, it is  set to <code>False</code> if the new line is classified as a space. If no classification has been done over that token, it will remain <code>None</code>.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <p>end_lines_model : Optional[Union[str, EndLinesModel]], by default None     path to trained model. If None, it will use a default model</p> Source code in <code>edsnlp/pipelines/core/endlines/endlines.py</code> <pre><code>class EndLines(GenericMatcher):\n\"\"\"\n    spaCy Pipeline to detect whether a newline character should\n    be considered a space (ie introduced by the PDF).\n\n    The pipeline will add the extension `end_line` to spans\n    and tokens. The `end_line` attribute is a boolean or `None`,\n    set to `True` if the pipeline predicts that the new line\n    is an end line character. Otherwise, it is  set to `False`\n    if the new line is classified as a space. If no classification\n    has been done over that token, it will remain `None`.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n\n    end_lines_model : Optional[Union[str, EndLinesModel]], by default None\n        path to trained model. If None, it will use a default model\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        end_lines_model: Optional[Union[str, EndLinesModel]],\n        **kwargs,\n    ):\n\n        super().__init__(\n            nlp,\n            terms=None,\n            attr=\"TEXT\",\n            regex=dict(\n                new_line=r\"\\n+\",\n            ),\n            ignore_excluded=False,\n            ignore_space_tokens=False,\n            **kwargs,\n        )\n\n        self._read_model(end_lines_model)\n\n    def _read_model(self, end_lines_model: Optional[Union[str, EndLinesModel]]):\n\"\"\"\n        Parameters\n        ----------\n        end_lines_model : Optional[Union[str, EndLinesModel]]\n\n        Raises\n        ------\n        TypeError\n        \"\"\"\n        if end_lines_model is None:\n            path = build_path(__file__, \"base_model.pkl\")\n\n            with open(path, \"rb\") as inp:\n                self.model = pickle.load(inp)\n        elif type(end_lines_model) == str:\n            with open(end_lines_model, \"rb\") as inp:\n                self.model = pickle.load(inp)\n        elif type(end_lines_model) == EndLinesModel:\n            self.model = end_lines_model\n        else:\n            raise TypeError(\n                \"type(`end_lines_model`) should be one of {None, str, EndLinesModel}\"\n            )\n\n    @classmethod\n    def _spacy_compute_a3a4(cls, token: Token) -&gt; str:\n\"\"\"Function to compute A3 and A4\n\n        Parameters\n        ----------\n        token : Token\n\n        Returns\n        -------\n        str\n        \"\"\"\n\n        if token.is_upper:\n            return \"UPPER\"\n\n        elif token.shape_.startswith(\"Xx\"):\n            return \"S_UPPER\"\n\n        elif token.shape_.startswith(\"x\"):\n            return \"LOWER\"\n\n        elif (token.is_digit) &amp; (\n            (token.doc[max(token.i - 1, 0)].is_punct)\n            | (token.doc[min(token.i + 1, len(token.doc) - 1)].is_punct)\n        ):\n            return \"ENUMERATION\"\n\n        elif token.is_digit:\n            return \"DIGIT\"\n\n        elif (token.is_punct) &amp; (token.text in [\".\", \";\", \"..\", \"...\"]):\n            return \"STRONG_PUNCT\"\n\n        elif (token.is_punct) &amp; (token.text not in [\".\", \";\", \"..\", \"...\"]):\n            return \"SOFT_PUNCT\"\n\n        else:\n            return \"OTHER\"\n\n    @classmethod\n    def _compute_length(cls, doc: Doc, start: int, end: int) -&gt; int:\n\"\"\"Compute length without spaces\n\n        Parameters\n        ----------\n        doc : Doc\n        start : int\n        end : int\n\n        Returns\n        -------\n        int\n        \"\"\"\n        length = 0\n        for t in doc[start:end]:\n            length += len(t.text)\n\n        return length\n\n    def _get_df(self, doc: Doc, new_lines: List[Span]) -&gt; pd.DataFrame:\n\"\"\"Get a pandas DataFrame to call the classifier\n\n        Parameters\n        ----------\n        doc : Doc\n        new_lines : List[Span]\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n\n        data = []\n        for i, span in enumerate(new_lines):\n            start = span.start\n            end = span.end\n\n            max_index = len(doc) - 1\n            a1_token = doc[max(start - 1, 0)]\n            a2_token = doc[min(start + 1, max_index)]\n            a1 = a1_token.orth\n            a2 = a2_token.orth\n            a3 = self._spacy_compute_a3a4(a1_token)\n            a4 = self._spacy_compute_a3a4(a2_token)\n            blank_line = \"\\n\\n\" in span.text\n\n            if i &gt; 0:\n                start_previous = new_lines[i - 1].start + 1\n            else:\n                start_previous = 0\n\n            length = self._compute_length(\n                doc, start=start_previous, end=start\n            )  # It's ok cause i count the total length from the previous up to this one\n\n            data_dict = dict(\n                span_start=start,\n                span_end=end,\n                A1=a1,\n                A2=a2,\n                A3=a3,\n                A4=a4,\n                BLANK_LINE=blank_line,\n                length=length,\n            )\n            data.append(data_dict)\n\n        df = pd.DataFrame(data)\n\n        mu = df[\"length\"].mean()\n        sigma = df[\"length\"].std()\n        if np.isnan(sigma):\n            sigma = 1\n\n        cv = sigma / mu\n        df[\"B1\"] = (df[\"length\"] - mu) / sigma\n        df[\"B2\"] = cv\n\n        return df\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Predict for each new line if it's an end of line or a space.\n\n        Parameters\n        ----------\n        doc: spaCy Doc object\n\n        Returns\n        -------\n        doc: spaCy Doc object, with each new line annotated\n        \"\"\"\n\n        matches = self.process(doc)\n        new_lines = get_spans(matches, \"new_line\")\n\n        if len(new_lines) &gt; 0:\n            df = self._get_df(doc=doc, new_lines=new_lines)\n            df = self.model.predict(df)\n\n            for span, prediction in zip(new_lines, df.PREDICTED_END_LINE):\n\n                for t in span:\n                    t.tag_ = \"ENDLINE\" if prediction else \"EXCLUDED\"\n                    if prediction:\n                        t._.excluded = True\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/endlines/endlines/#edsnlp.pipelines.core.endlines.endlines.EndLines.__call__","title":"<code>__call__(doc)</code>","text":"<p>Predict for each new line if it's an end of line or a space.</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy Doc object, with each new line annotated</code> </p> Source code in <code>edsnlp/pipelines/core/endlines/endlines.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Predict for each new line if it's an end of line or a space.\n\n    Parameters\n    ----------\n    doc: spaCy Doc object\n\n    Returns\n    -------\n    doc: spaCy Doc object, with each new line annotated\n    \"\"\"\n\n    matches = self.process(doc)\n    new_lines = get_spans(matches, \"new_line\")\n\n    if len(new_lines) &gt; 0:\n        df = self._get_df(doc=doc, new_lines=new_lines)\n        df = self.model.predict(df)\n\n        for span, prediction in zip(new_lines, df.PREDICTED_END_LINE):\n\n            for t in span:\n                t.tag_ = \"ENDLINE\" if prediction else \"EXCLUDED\"\n                if prediction:\n                    t._.excluded = True\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/endlines/endlinesmodel/","title":"<code>edsnlp.pipelines.core.endlines.endlinesmodel</code>","text":""},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel","title":"<code>EndLinesModel</code>","text":"<p>Model to classify if an end line is a real one or it should be a space.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> Source code in <code>edsnlp/pipelines/core/endlines/endlinesmodel.py</code> <pre><code>class EndLinesModel:\n\"\"\"Model to classify if an end line is a real one or it should be a space.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    \"\"\"\n\n    def __init__(self, nlp: Language):\n        self.nlp = nlp\n\n    def _preprocess_data(self, corpus: Iterable[Doc]) -&gt; pd.DataFrame:\n\"\"\"\n        Parameters\n        ----------\n        corpus : Iterable[Doc]\n            Corpus of documents\n\n        Returns\n        -------\n        pd.DataFrame\n            Preprocessed data\n        \"\"\"\n        # Extract the vocabulary\n        string_store = self.nlp.vocab.strings\n\n        # Iterate in the corpus and construct a dataframe\n        train_data_list = []\n        for i, doc in enumerate(corpus):\n            train_data_list.append(self._get_attributes(doc, i))\n\n        df = pd.concat(train_data_list)\n        df.reset_index(inplace=True, drop=False)\n        df.rename(columns={\"ORTH\": \"A1\", \"index\": \"original_token_index\"}, inplace=True)\n\n        # Retrieve string representation of token_id and shape\n        df[\"TEXT\"] = df.A1.apply(self._get_string, string_store=string_store)\n        df[\"SHAPE_\"] = df.SHAPE.apply(self._get_string, string_store=string_store)\n\n        # Convert new lines as an attribute instead of a row\n        df = self._convert_line_to_attribute(df, expr=\"\\n\", col=\"END_LINE\")\n        df = self._convert_line_to_attribute(df, expr=\"\\n\\n\", col=\"BLANK_LINE\")\n        df = df.loc[~(df.END_LINE | df.BLANK_LINE)]\n        df = df.drop(columns=\"END_LINE\")\n        df = df.drop(columns=\"BLANK_LINE\")\n        df.rename(\n            columns={\"TEMP_END_LINE\": \"END_LINE\", \"TEMP_BLANK_LINE\": \"BLANK_LINE\"},\n            inplace=True,\n        )\n\n        # Construct A2 by shifting\n        df = self._shift_col(df, \"A1\", \"A2\", direction=\"backward\")\n\n        # Compute A3 and A4\n        df = self._compute_a3(df)\n        df = self._shift_col(df, \"A3\", \"A4\", direction=\"backward\")\n\n        # SPACE is the class to predict. Set 1 if not an END_LINE\n        df[\"SPACE\"] = np.logical_not(df[\"END_LINE\"]).astype(\"int\")\n\n        df[[\"END_LINE\", \"BLANK_LINE\"]] = df[[\"END_LINE\", \"BLANK_LINE\"]].fillna(\n            True, inplace=False\n        )\n\n        # Assign a sentence id to each token\n        df = df.groupby(\"DOC_ID\").apply(self._retrieve_lines)\n        df[\"SENTENCE_ID\"] = df[\"SENTENCE_ID\"].astype(\"int\")\n\n        # Compute B1 and B2\n        df = self._compute_B(df)\n\n        # Drop Tokens without info (last token of doc)\n        df.dropna(subset=[\"A1\", \"A2\", \"A3\", \"A4\"], inplace=True)\n\n        # Export the vocabularies to be able to use the model with another corpus\n        voc_a3a4 = self._create_vocabulary(df.A3_.cat.categories)\n        voc_B2 = self._create_vocabulary(df.cv_bin.cat.categories)\n        voc_B1 = self._create_vocabulary(df.l_norm_bin.cat.categories)\n\n        vocabulary = {\"A3A4\": voc_a3a4, \"B1\": voc_B1, \"B2\": voc_B2}\n\n        self.vocabulary = vocabulary\n\n        return df\n\n    def fit_and_predict(self, corpus: Iterable[Doc]) -&gt; pd.DataFrame:\n\"\"\"Fit the model and predict for the training data\n\n        Parameters\n        ----------\n        corpus : Iterable[Doc]\n            An iterable of Documents\n\n        Returns\n        -------\n        pd.DataFrame\n            one line by end_line prediction\n        \"\"\"\n\n        # Preprocess data to have a pd DF\n        df = self._preprocess_data(corpus)\n\n        # Train and predict M1\n        self._fit_M1(df.A1, df.A2, df.A3, df.A4, df.SPACE)\n        outputs_M1 = self._predict_M1(\n            df.A1,\n            df.A2,\n            df.A3,\n            df.A4,\n        )\n        df[\"M1\"] = outputs_M1[\"predictions\"]\n        df[\"M1_proba\"] = outputs_M1[\"predictions_proba\"]\n\n        # Force Blank lines to 0\n        df.loc[df.BLANK_LINE, \"M1\"] = 0\n\n        # Train and predict M2\n        df_endlines = df.loc[df.END_LINE]\n        self._fit_M2(B1=df_endlines.B1, B2=df_endlines.B2, label=df_endlines.M1)\n        outputs_M2 = self._predict_M2(B1=df_endlines.B1, B2=df_endlines.B2)\n\n        df.loc[df.END_LINE, \"M2\"] = outputs_M2[\"predictions\"]\n        df.loc[df.END_LINE, \"M2_proba\"] = outputs_M2[\"predictions_proba\"]\n\n        df[\"M2\"] = df[\"M2\"].astype(\n            pd.Int64Dtype()\n        )  # cast to pd.Int64Dtype cause there are None values\n\n        # M1M2\n        df = df.loc[df.END_LINE]\n        df[\"M1M2_lr\"] = (df[\"M2_proba\"] / (1 - df[\"M2_proba\"])) * (\n            df[\"M1_proba\"] / (1 - df[\"M1_proba\"])\n        )\n        df[\"M1M2\"] = (df[\"M1M2_lr\"] &gt; 1).astype(\"int\")\n\n        # Force Blank lines to 0\n        df.loc[df.BLANK_LINE, [\"M2\", \"M1M2\"]] = 0\n\n        # Make binary col\n        df[\"PREDICTED_END_LINE\"] = np.logical_not(df[\"M1M2\"].astype(bool))\n\n        return df\n\n    def predict(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Use the model for inference\n\n        The df should have the following columns:\n        `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]`\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            The df should have the following columns:\n            `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]`\n\n        Returns\n        -------\n        pd.DataFrame\n            The result is added to the column `PREDICTED_END_LINE`\n        \"\"\"\n\n        df = self._convert_raw_data_to_codes(df)\n\n        outputs_M1 = self._predict_M1(df.A1, df.A2, df._A3, df._A4)\n        df[\"M1\"] = outputs_M1[\"predictions\"]\n        df[\"M1_proba\"] = outputs_M1[\"predictions_proba\"]\n\n        outputs_M2 = self._predict_M2(B1=df._B1, B2=df._B2)\n        df[\"M2\"] = outputs_M2[\"predictions\"]\n        df[\"M2_proba\"] = outputs_M2[\"predictions_proba\"]\n        df[\"M2\"] = df[\"M2\"].astype(\n            pd.Int64Dtype()\n        )  # cast to pd.Int64Dtype cause there are None values\n\n        # M1M2\n        df[\"M1M2_lr\"] = (df[\"M2_proba\"] / (1 - df[\"M2_proba\"])) * (\n            df[\"M1_proba\"] / (1 - df[\"M1_proba\"])\n        )\n        df[\"M1M2\"] = (df[\"M1M2_lr\"] &gt; 1).astype(\"int\")\n\n        # Force Blank lines to 0\n        df.loc[\n            df.BLANK_LINE,\n            [\n                \"M1M2\",\n            ],\n        ] = 0\n\n        # Make binary col\n        df[\"PREDICTED_END_LINE\"] = np.logical_not(df[\"M1M2\"].astype(bool))\n\n        return df\n\n    def save(self, path=\"base_model.pkl\"):\n\"\"\"Save a pickle of the model. It could be read by the pipeline later.\n\n        Parameters\n        ----------\n        path : str, optional\n            path to file .pkl, by default `base_model.pkl`\n        \"\"\"\n        with open(path, \"wb\") as outp:\n            del self.nlp\n            pickle.dump(self, outp, pickle.HIGHEST_PROTOCOL)\n\n    def _convert_A(self, df: pd.DataFrame, col: str) -&gt; pd.DataFrame:\n\"\"\"\n        Parameters\n        ----------\n        df : pd.DataFrame\n        col : str\n            column to translate\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        cat_type_A = CategoricalDtype(\n            categories=self.vocabulary[\"A3A4\"].keys(), ordered=True\n        )\n        new_col = \"_\" + col\n        df[new_col] = df[col].astype(cat_type_A)\n        df[new_col] = df[new_col].cat.codes\n        # Ensure that not known values are coded as OTHER\n        df.loc[\n            ~df[col].isin(self.vocabulary[\"A3A4\"].keys()), new_col\n        ] = self.vocabulary[\"A3A4\"][\"OTHER\"]\n        return df\n\n    def _convert_B(self, df: pd.DataFrame, col: str) -&gt; pd.DataFrame:\n\"\"\"\n        Parameters\n        ----------\n        df : pd.DataFrame\n            [description]\n        col : str\n            column to translate\n\n        Returns\n        -------\n        pd.DataFrame\n            [description]\n        \"\"\"\n        # Translate B1\n        index_B = pd.IntervalIndex(list(self.vocabulary[col].keys()))\n        new_col = \"_\" + col\n        df[new_col] = pd.cut(df[col], index_B)\n        df[new_col] = df[new_col].cat.codes\n        df.loc[df[col] &gt;= index_B.right.max(), new_col] = max(\n            self.vocabulary[col].values()\n        )\n        df.loc[df[col] &lt;= index_B.left.min(), new_col] = min(\n            self.vocabulary[col].values()\n        )\n\n        return df\n\n    def _convert_raw_data_to_codes(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        Function to translate data as extracted from spacy to the model codes.\n        `A1` and `A2` are not translated cause are supposed to be already\n        in good encoding.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            It should have columns `['A3','A4','B1','B2']`\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        df = self._convert_A(df, \"A3\")\n        df = self._convert_A(df, \"A4\")\n        df = self._convert_B(df, \"B1\")\n        df = self._convert_B(df, \"B2\")\n        return df\n\n    def _convert_line_to_attribute(\n        self, df: pd.DataFrame, expr: str, col: str\n    ) -&gt; pd.DataFrame:\n\"\"\"\n        Function to convert a line into an attribute (column) of the\n        previous row. Particularly we use it to identify \"\\\\n\" and \"\\\\n\\\\n\"\n        that are considered tokens, express this information as an attribute\n        of the previous token.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n        expr : str\n            pattern to search in the text. Ex.: \"\\\\n\"\n        col : str\n            name of the new column\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n        idx = df.TEXT.str.contains(expr)\n        df.loc[idx, col] = True\n        df[col] = df[col].fillna(False)\n        df = self._shift_col(df, col, \"TEMP_\" + col, direction=\"backward\")\n\n        return df\n\n    def _compute_a3(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"\n        A3 (A4 respectively): typographic form  of left word (or right) :\n\n        - All in capital letter\n        - It starts with a capital letter\n        - Starts by lowercase\n        - It's a number\n        - Strong punctuation\n        - Soft punctuation\n        - A number followed or preced by a punctuation (it's the case of enumerations)\n\n        Parameters\n        ----------\n        df: pd.DataFrame\n\n        Returns\n        -------\n        df: pd.DataFrame with the columns `A3` and `A3_`\n\n        \"\"\"\n        df = self._shift_col(\n            df, \"IS_PUNCT\", \"IS_PUNCT_+1\", direction=\"backward\", fill=False\n        )\n        df = self._shift_col(\n            df, \"IS_PUNCT\", \"IS_PUNCT_-1\", direction=\"forward\", fill=False\n        )\n\n        CONDITION1 = df.IS_UPPER\n        CONDITION2 = df.SHAPE_.str.startswith(\"Xx\", na=False)\n        CONDITION3 = df.SHAPE_.str.startswith(\"x\", na=False)\n        CONDITION4 = df.IS_DIGIT\n        STRONG_PUNCT = [\".\", \";\", \"..\", \"...\"]\n        CONDITION5 = (df.IS_PUNCT) &amp; (df.TEXT.isin(STRONG_PUNCT))\n        CONDITION6 = (df.IS_PUNCT) &amp; (~df.TEXT.isin(STRONG_PUNCT))\n        CONDITION7 = (df.IS_DIGIT) &amp; (df[\"IS_PUNCT_+1\"] | df[\"IS_PUNCT_-1\"])  # discuss\n\n        df[\"A3_\"] = None\n        df.loc[CONDITION1, \"A3_\"] = \"UPPER\"\n        df.loc[CONDITION2, \"A3_\"] = \"S_UPPER\"\n        df.loc[CONDITION3, \"A3_\"] = \"LOWER\"\n        df.loc[CONDITION4, \"A3_\"] = \"DIGIT\"\n        df.loc[CONDITION5, \"A3_\"] = \"STRONG_PUNCT\"\n        df.loc[CONDITION6, \"A3_\"] = \"SOFT_PUNCT\"\n        df.loc[CONDITION7, \"A3_\"] = \"ENUMERATION\"\n\n        df = df.drop(columns=[\"IS_PUNCT_+1\", \"IS_PUNCT_-1\"])\n        df[\"A3_\"] = df[\"A3_\"].astype(\"category\")\n\n        df[\"A3_\"] = df[\"A3_\"].cat.add_categories(\"OTHER\")\n        df[\"A3_\"].fillna(\"OTHER\", inplace=True)\n\n        df[\"A3\"] = df[\"A3_\"].cat.codes\n\n        return df\n\n    def _fit_M1(\n        self,\n        A1: pd.Series,\n        A2: pd.Series,\n        A3: pd.Series,\n        A4: pd.Series,\n        label: pd.Series,\n    ):\n\"\"\"Function to train M1 classifier (Naive Bayes)\n\n        Parameters\n        ----------\n        A1 : pd.Series\n            [description]\n        A2 : pd.Series\n            [description]\n        A3 : pd.Series\n            [description]\n        A4 : pd.Series\n            [description]\n        label : pd.Series\n            [description]\n\n        \"\"\"\n        # Encode classes to OneHotEncoder representation\n        encoder_A1_A2 = self._fit_encoder_2S(A1, A2)\n        self.encoder_A1_A2 = encoder_A1_A2\n\n        encoder_A3_A4 = self._fit_encoder_2S(A3, A4)\n        self.encoder_A3_A4 = encoder_A3_A4\n\n        # M1\n        m1 = MultinomialNB(alpha=1)\n\n        X = self._get_X_for_M1(A1, A2, A3, A4)\n        m1.fit(X, label)\n        self.m1 = m1\n\n    def _fit_M2(self, B1: pd.Series, B2: pd.Series, label: pd.Series):\n\"\"\"Function to train M2 classifier (Naive Bayes)\n\n        Parameters\n        ----------\n        B1 : pd.Series\n        B2 : pd.Series\n        label : pd.Series\n        \"\"\"\n\n        # Encode classes to OneHotEncoder representation\n        encoder_B1 = self._fit_encoder_1S(B1)\n        self.encoder_B1 = encoder_B1\n        encoder_B2 = self._fit_encoder_1S(B2)\n        self.encoder_B2 = encoder_B2\n\n        # Multinomial Naive Bayes\n        m2 = MultinomialNB(alpha=1)\n        X = self._get_X_for_M2(B1, B2)\n        m2.fit(X, label)\n        self.m2 = m2\n\n    def _get_X_for_M1(\n        self, A1: pd.Series, A2: pd.Series, A3: pd.Series, A4: pd.Series\n    ) -&gt; np.ndarray:\n\"\"\"Get X matrix for classifier\n\n        Parameters\n        ----------\n        A1 : pd.Series\n        A2 : pd.Series\n        A3 : pd.Series\n        A4 : pd.Series\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        A1_enc = self._encode_series(self.encoder_A1_A2, A1)\n        A2_enc = self._encode_series(self.encoder_A1_A2, A2)\n        A3_enc = self._encode_series(self.encoder_A3_A4, A3)\n        A4_enc = self._encode_series(self.encoder_A3_A4, A4)\n        X = hstack([A1_enc, A2_enc, A3_enc, A4_enc])\n        return X\n\n    def _get_X_for_M2(self, B1: pd.Series, B2: pd.Series) -&gt; np.ndarray:\n\"\"\"Get X matrix for classifier\n\n        Parameters\n        ----------\n        B1 : pd.Series\n        B2 : pd.Series\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        B1_enc = self._encode_series(self.encoder_B1, B1)\n        B2_enc = self._encode_series(self.encoder_B2, B2)\n        X = hstack([B1_enc, B2_enc])\n        return X\n\n    def _predict_M1(\n        self, A1: pd.Series, A2: pd.Series, A3: pd.Series, A4: pd.Series\n    ) -&gt; Dict[str, Any]:\n\"\"\"Use M1 for prediction\n\n        Parameters\n        ----------\n        A1 : pd.Series\n        A2 : pd.Series\n        A3 : pd.Series\n        A4 : pd.Series\n\n        Returns\n        -------\n        Dict[str, Any]\n        \"\"\"\n        X = self._get_X_for_M1(A1, A2, A3, A4)\n        predictions = self.m1.predict(X)\n        predictions_proba = self.m1.predict_proba(X)[:, 1]\n        outputs = {\"predictions\": predictions, \"predictions_proba\": predictions_proba}\n        return outputs\n\n    def _predict_M2(self, B1: pd.Series, B2: pd.Series) -&gt; Dict[str, Any]:\n\"\"\"Use M2 for prediction\n\n        Parameters\n        ----------\n        B1 : pd.Series\n        B2 : pd.Series\n\n        Returns\n        -------\n        Dict[str, Any]\n        \"\"\"\n        X = self._get_X_for_M2(B1, B2)\n        predictions = self.m2.predict(X)\n        predictions_proba = self.m2.predict_proba(X)[:, 1]\n        outputs = {\"predictions\": predictions, \"predictions_proba\": predictions_proba}\n        return outputs\n\n    def _fit_encoder_2S(self, S1: pd.Series, S2: pd.Series) -&gt; OneHotEncoder:\n\"\"\"Fit a one hot encoder with 2 Series. It concatenates the series and after it fits.\n\n        Parameters\n        ----------\n        S1 : pd.Series\n        S2 : pd.Series\n\n        Returns\n        -------\n        OneHotEncoder\n        \"\"\"\n        _S1 = _convert_series_to_array(S1)\n        _S2 = _convert_series_to_array(S2)\n        S = np.concatenate([_S1, _S2])\n        encoder = self._fit_one_hot_encoder(S)\n        return encoder\n\n    def _fit_encoder_1S(self, S1: pd.Series) -&gt; OneHotEncoder:\n\"\"\"Fit a one hot encoder with 1 Series.\n\n        Parameters\n        ----------\n        S1 : pd.Series\n\n        Returns\n        -------\n        OneHotEncoder\n        \"\"\"\n        _S1 = _convert_series_to_array(S1)\n        encoder = self._fit_one_hot_encoder(_S1)\n        return encoder\n\n    def _encode_series(self, encoder: OneHotEncoder, S: pd.Series) -&gt; np.ndarray:\n\"\"\"Use the one hot encoder to transform a series.\n\n        Parameters\n        ----------\n        encoder : OneHotEncoder\n        S : pd.Series\n            a series to encode (transform)\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        _S = _convert_series_to_array(S)\n        S_enc = encoder.transform(_S)\n        return S_enc\n\n    @classmethod\n    def _retrieve_lines(cls, dfg: DataFrameGroupBy) -&gt; DataFrameGroupBy:\n\"\"\"Function to give a sentence_id to each token.\n\n        Parameters\n        ----------\n        dfg : DataFrameGroupBy\n\n        Returns\n        -------\n        DataFrameGroupBy\n            Same DataFrameGroupBy with the column `SENTENCE_ID`\n        \"\"\"\n        sentences_ids = np.arange(dfg.END_LINE.sum())\n        dfg.loc[dfg.END_LINE, \"SENTENCE_ID\"] = sentences_ids\n        dfg[\"SENTENCE_ID\"] = dfg[\"SENTENCE_ID\"].fillna(method=\"bfill\")\n        return dfg\n\n    @classmethod\n    def _create_vocabulary(cls, x: iterable) -&gt; dict:\n\"\"\"Function to create a vocabulary for attributes in the training set.\n\n        Parameters\n        ----------\n        x : iterable\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        v = {}\n\n        for i, key in enumerate(x):\n            v[key] = i\n\n        return v\n\n    @classmethod\n    def _compute_B(cls, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Function to compute B1 and B2\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n\n        Returns\n        -------\n        pd.DataFrame\n        \"\"\"\n\n        data = df.groupby([\"DOC_ID\", \"SENTENCE_ID\"]).agg(l=(\"LENGTH\", \"sum\"))\n        df_t = df.loc[df.END_LINE, [\"DOC_ID\", \"SENTENCE_ID\"]].merge(\n            data, left_on=[\"DOC_ID\", \"SENTENCE_ID\"], right_index=True, how=\"left\"\n        )\n\n        stats_doc = df_t.groupby(\"DOC_ID\").agg(mu=(\"l\", \"mean\"), sigma=(\"l\", \"std\"))\n        stats_doc[\"sigma\"].replace(\n            0.0, 1.0, inplace=True\n        )  # Replace the 0 std by unit std, otherwise it breaks the code.\n        stats_doc[\"cv\"] = stats_doc[\"sigma\"] / stats_doc[\"mu\"]\n\n        df_t = df_t.drop(columns=[\"DOC_ID\", \"SENTENCE_ID\"])\n        df2 = df.merge(df_t, left_index=True, right_index=True, how=\"left\")\n\n        df2 = df2.merge(stats_doc, on=[\"DOC_ID\"], how=\"left\")\n        df2[\"l_norm\"] = (df2[\"l\"] - df2[\"mu\"]) / df2[\"sigma\"]\n\n        df2[\"cv_bin\"] = pd.cut(df2[\"cv\"], bins=10)\n        df2[\"B2\"] = df2[\"cv_bin\"].cat.codes\n\n        df2[\"l_norm_bin\"] = pd.cut(df2[\"l_norm\"], bins=10)\n        df2[\"B1\"] = df2[\"l_norm_bin\"].cat.codes\n\n        return df2\n\n    @classmethod\n    def _shift_col(\n        cls, df: pd.DataFrame, col: str, new_col: str, direction=\"backward\", fill=None\n    ) -&gt; pd.DataFrame:\n\"\"\"Shifts a column one position into backward / forward direction.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n        col : str\n            column to shift\n        new_col : str\n            column name to save the results\n        direction : str, optional\n            one of {\"backward\", \"forward\"}, by default \"backward\"\n        fill : [type], optional\n            , by default None\n\n        Returns\n        -------\n        pd.DataFrame\n            same df with `new_col` added.\n        \"\"\"\n        df[new_col] = fill\n\n        if direction == \"backward\":\n            df.loc[df.index[:-1], new_col] = df[col].values[1:]\n\n            different_doc_id = df[\"DOC_ID\"].values[:-1] != df[\"DOC_ID\"].values[1:]\n            different_doc_id = np.append(different_doc_id, True)\n\n        if direction == \"forward\":\n            df.loc[df.index[1:], new_col] = df[col].values[:-1]\n            different_doc_id = df[\"DOC_ID\"].values[1:] != df[\"DOC_ID\"].values[:-1]\n            different_doc_id = np.append(True, different_doc_id)\n\n        df.loc[different_doc_id, new_col] = fill\n        return df\n\n    @classmethod\n    def _get_attributes(cls, doc: Doc, i=0):\n\"\"\"Function to get the attributes of tokens of a spacy doc in a pd.DataFrame format.\n\n        Parameters\n        ----------\n        doc : Doc\n            spacy Doc\n        i : int, optional\n            document id, by default 0\n\n        Returns\n        -------\n        pd.DataFrame\n            Returns a dataframe with one line per token. It has the following columns :\n            `[\n            \"ORTH\",\n            \"LOWER\",\n            \"SHAPE\",\n            \"IS_DIGIT\",\n            \"IS_SPACE\",\n            \"IS_UPPER\",\n            \"IS_PUNCT\",\n            \"LENGTH\",\n            ]`\n        \"\"\"\n        attributes = [\n            \"ORTH\",\n            \"LOWER\",\n            \"SHAPE\",\n            \"IS_DIGIT\",\n            \"IS_SPACE\",\n            \"IS_UPPER\",\n            \"IS_PUNCT\",\n            \"LENGTH\",\n        ]\n        attributes_array = doc.to_array(attributes)\n        attributes_df = pd.DataFrame(attributes_array, columns=attributes)\n        attributes_df[\"DOC_ID\"] = i\n        boolean_attr = []\n        for a in attributes:\n            if a[:3] == \"IS_\":\n                boolean_attr.append(a)\n        attributes_df[boolean_attr] = attributes_df[boolean_attr].astype(\"boolean\")\n        return attributes_df\n\n    @classmethod\n    def _get_string(cls, _id: int, string_store: StringStore) -&gt; str:\n\"\"\"Returns the string corresponding to the token_id\n\n        Parameters\n        ----------\n        _id : int\n            token id\n        string_store : StringStore\n            spaCy Language String Store\n\n        Returns\n        -------\n        str\n            string representation of the token.\n        \"\"\"\n        return string_store[_id]\n\n    @classmethod\n    def _fit_one_hot_encoder(cls, X: np.ndarray) -&gt; OneHotEncoder:\n\"\"\"Fit a one hot encoder.\n\n        Parameters\n        ----------\n        X : np.ndarray\n            of shape (n,1)\n\n        Returns\n        -------\n        OneHotEncoder\n        \"\"\"\n        encoder = OneHotEncoder(handle_unknown=\"ignore\")\n        encoder.fit(X)\n        return encoder\n</code></pre>"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.fit_and_predict","title":"<code>fit_and_predict(corpus)</code>","text":"<p>Fit the model and predict for the training data</p> PARAMETER DESCRIPTION <code>corpus</code> <p>An iterable of Documents</p> <p> TYPE: <code>Iterable[Doc]</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>one line by end_line prediction</p> Source code in <code>edsnlp/pipelines/core/endlines/endlinesmodel.py</code> <pre><code>def fit_and_predict(self, corpus: Iterable[Doc]) -&gt; pd.DataFrame:\n\"\"\"Fit the model and predict for the training data\n\n    Parameters\n    ----------\n    corpus : Iterable[Doc]\n        An iterable of Documents\n\n    Returns\n    -------\n    pd.DataFrame\n        one line by end_line prediction\n    \"\"\"\n\n    # Preprocess data to have a pd DF\n    df = self._preprocess_data(corpus)\n\n    # Train and predict M1\n    self._fit_M1(df.A1, df.A2, df.A3, df.A4, df.SPACE)\n    outputs_M1 = self._predict_M1(\n        df.A1,\n        df.A2,\n        df.A3,\n        df.A4,\n    )\n    df[\"M1\"] = outputs_M1[\"predictions\"]\n    df[\"M1_proba\"] = outputs_M1[\"predictions_proba\"]\n\n    # Force Blank lines to 0\n    df.loc[df.BLANK_LINE, \"M1\"] = 0\n\n    # Train and predict M2\n    df_endlines = df.loc[df.END_LINE]\n    self._fit_M2(B1=df_endlines.B1, B2=df_endlines.B2, label=df_endlines.M1)\n    outputs_M2 = self._predict_M2(B1=df_endlines.B1, B2=df_endlines.B2)\n\n    df.loc[df.END_LINE, \"M2\"] = outputs_M2[\"predictions\"]\n    df.loc[df.END_LINE, \"M2_proba\"] = outputs_M2[\"predictions_proba\"]\n\n    df[\"M2\"] = df[\"M2\"].astype(\n        pd.Int64Dtype()\n    )  # cast to pd.Int64Dtype cause there are None values\n\n    # M1M2\n    df = df.loc[df.END_LINE]\n    df[\"M1M2_lr\"] = (df[\"M2_proba\"] / (1 - df[\"M2_proba\"])) * (\n        df[\"M1_proba\"] / (1 - df[\"M1_proba\"])\n    )\n    df[\"M1M2\"] = (df[\"M1M2_lr\"] &gt; 1).astype(\"int\")\n\n    # Force Blank lines to 0\n    df.loc[df.BLANK_LINE, [\"M2\", \"M1M2\"]] = 0\n\n    # Make binary col\n    df[\"PREDICTED_END_LINE\"] = np.logical_not(df[\"M1M2\"].astype(bool))\n\n    return df\n</code></pre>"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.predict","title":"<code>predict(df)</code>","text":"<p>Use the model for inference</p> <p>The df should have the following columns: <code>[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]</code></p> PARAMETER DESCRIPTION <code>df</code> <p>The df should have the following columns: <code>[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]</code></p> <p> TYPE: <code>pd.DataFrame</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>The result is added to the column <code>PREDICTED_END_LINE</code></p> Source code in <code>edsnlp/pipelines/core/endlines/endlinesmodel.py</code> <pre><code>def predict(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n\"\"\"Use the model for inference\n\n    The df should have the following columns:\n    `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]`\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The df should have the following columns:\n        `[\"A1\",\"A2\",\"A3\",\"A4\",\"B1\",\"B2\",\"BLANK_LINE\"]`\n\n    Returns\n    -------\n    pd.DataFrame\n        The result is added to the column `PREDICTED_END_LINE`\n    \"\"\"\n\n    df = self._convert_raw_data_to_codes(df)\n\n    outputs_M1 = self._predict_M1(df.A1, df.A2, df._A3, df._A4)\n    df[\"M1\"] = outputs_M1[\"predictions\"]\n    df[\"M1_proba\"] = outputs_M1[\"predictions_proba\"]\n\n    outputs_M2 = self._predict_M2(B1=df._B1, B2=df._B2)\n    df[\"M2\"] = outputs_M2[\"predictions\"]\n    df[\"M2_proba\"] = outputs_M2[\"predictions_proba\"]\n    df[\"M2\"] = df[\"M2\"].astype(\n        pd.Int64Dtype()\n    )  # cast to pd.Int64Dtype cause there are None values\n\n    # M1M2\n    df[\"M1M2_lr\"] = (df[\"M2_proba\"] / (1 - df[\"M2_proba\"])) * (\n        df[\"M1_proba\"] / (1 - df[\"M1_proba\"])\n    )\n    df[\"M1M2\"] = (df[\"M1M2_lr\"] &gt; 1).astype(\"int\")\n\n    # Force Blank lines to 0\n    df.loc[\n        df.BLANK_LINE,\n        [\n            \"M1M2\",\n        ],\n    ] = 0\n\n    # Make binary col\n    df[\"PREDICTED_END_LINE\"] = np.logical_not(df[\"M1M2\"].astype(bool))\n\n    return df\n</code></pre>"},{"location":"reference/pipelines/core/endlines/endlinesmodel/#edsnlp.pipelines.core.endlines.endlinesmodel.EndLinesModel.save","title":"<code>save(path='base_model.pkl')</code>","text":"<p>Save a pickle of the model. It could be read by the pipeline later.</p> PARAMETER DESCRIPTION <code>path</code> <p>path to file .pkl, by default <code>base_model.pkl</code></p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'base_model.pkl'</code> </p> Source code in <code>edsnlp/pipelines/core/endlines/endlinesmodel.py</code> <pre><code>def save(self, path=\"base_model.pkl\"):\n\"\"\"Save a pickle of the model. It could be read by the pipeline later.\n\n    Parameters\n    ----------\n    path : str, optional\n        path to file .pkl, by default `base_model.pkl`\n    \"\"\"\n    with open(path, \"wb\") as outp:\n        del self.nlp\n        pickle.dump(self, outp, pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"reference/pipelines/core/endlines/factory/","title":"<code>edsnlp.pipelines.core.endlines.factory</code>","text":""},{"location":"reference/pipelines/core/endlines/functional/","title":"<code>edsnlp.pipelines.core.endlines.functional</code>","text":""},{"location":"reference/pipelines/core/endlines/functional/#edsnlp.pipelines.core.endlines.functional.build_path","title":"<code>build_path(file, relative_path)</code>","text":"<p>Function to build an absolut path.</p> PARAMETER DESCRIPTION <code>file</code> <p> </p> <code>relative_path</code> <p>relative path from the main file to the desired output</p> <p> </p> RETURNS DESCRIPTION <code>path</code> <p> TYPE: <code>absolute path</code> </p> Source code in <code>edsnlp/pipelines/core/endlines/functional.py</code> <pre><code>def build_path(file, relative_path):\n\"\"\"\n    Function to build an absolut path.\n\n    Parameters\n    ----------\n    file: main file from where we are calling. It could be __file__\n    relative_path: str,\n        relative path from the main file to the desired output\n\n    Returns\n    -------\n    path: absolute path\n    \"\"\"\n    dir_path = get_dir_path(file)\n    path = os.path.abspath(os.path.join(dir_path, relative_path))\n    return path\n</code></pre>"},{"location":"reference/pipelines/core/matcher/","title":"<code>edsnlp.pipelines.core.matcher</code>","text":""},{"location":"reference/pipelines/core/matcher/factory/","title":"<code>edsnlp.pipelines.core.matcher.factory</code>","text":""},{"location":"reference/pipelines/core/matcher/factory/#edsnlp.pipelines.core.matcher.factory.create_component","title":"<code>create_component(nlp, name='eds.matcher', terms=None, attr=None, regex='TEXT', ignore_excluded=False, ignore_space_tokens=False, term_matcher=GenericTermMatcher.exact, term_matcher_config={})</code>","text":"<p>Provides a generic matcher component.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.matcher'</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>'TEXT'</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>GenericTermMatcher</code> DEFAULT: <code>GenericTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> Source code in <code>edsnlp/pipelines/core/matcher/factory.py</code> <pre><code>@deprecated_factory(\n    \"matcher\",\n    \"eds.matcher\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\n@Language.factory(\n    \"eds.matcher\", default_config=DEFAULT_CONFIG, assigns=[\"doc.ents\", \"doc.spans\"]\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.matcher\",\n    terms: Optional[Dict[str, Union[str, List[str]]]] = None,\n    attr: Union[str, Dict[str, str]] = None,\n    regex: Optional[Dict[str, Union[str, List[str]]]] = \"TEXT\",\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    term_matcher: GenericTermMatcher = GenericTermMatcher.exact,\n    term_matcher_config: Dict[str, Any] = {},\n):\n\"\"\"\n    Provides a generic matcher component.\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    name: str\n        The name of the component.\n    terms : Optional[Patterns]\n        A dictionary of terms.\n    regex : Optional[Patterns]\n        A dictionary of regular expressions.\n    attr : str\n        The default attribute to use for matching.\n        Can be overridden using the `terms` and `regex` configurations.\n    ignore_excluded : bool\n        Whether to skip excluded tokens (requires an upstream\n        pipeline to mark excluded tokens).\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n\n        You won't be able to match on newlines if this is enabled and\n        the \"spaces\"/\"newline\" option of `eds.normalizer` is enabled (by default).\n    term_matcher: GenericTermMatcher\n        The matcher to use for matching phrases ?\n        One of (exact, simstring)\n    term_matcher_config: Dict[str,Any]\n        Parameters of the matcher class\n    \"\"\"\n    assert not (terms is None and regex is None)\n\n    if terms is None:\n        terms = dict()\n    if regex is None:\n        regex = dict()\n\n    return GenericMatcher(\n        nlp,\n        terms=terms,\n        attr=attr,\n        regex=regex,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        term_matcher=term_matcher,\n        term_matcher_config=term_matcher_config,\n    )\n</code></pre>"},{"location":"reference/pipelines/core/matcher/matcher/","title":"<code>edsnlp.pipelines.core.matcher.matcher</code>","text":""},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher","title":"<code>GenericMatcher</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Provides a generic matcher component.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p>You won't be able to match on newlines if this is enabled and the \"spaces\"/\"newline\" option of <code>eds.normalizer</code> is enabled (by default).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>GenericTermMatcher</code> DEFAULT: <code>GenericTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/pipelines/core/matcher/matcher.py</code> <pre><code>class GenericMatcher(BaseComponent):\n\"\"\"\n    Provides a generic matcher component.\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    terms : Optional[Patterns]\n        A dictionary of terms.\n    regex : Optional[Patterns]\n        A dictionary of regular expressions.\n    attr : str\n        The default attribute to use for matching.\n        Can be overridden using the `terms` and `regex` configurations.\n    ignore_excluded : bool\n        Whether to skip excluded tokens (requires an upstream\n        pipeline to mark excluded tokens).\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n\n        You won't be able to match on newlines if this is enabled and\n        the \"spaces\"/\"newline\" option of `eds.normalizer` is enabled (by default).\n    term_matcher: GenericTermMatcher\n        The matcher to use for matching phrases ?\n        One of (exact, simstring)\n    term_matcher_config: Dict[str,Any]\n        Parameters of the matcher class\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        terms: Optional[Patterns],\n        regex: Optional[Patterns],\n        attr: str,\n        ignore_excluded: bool,\n        ignore_space_tokens: bool = False,\n        term_matcher: GenericTermMatcher = GenericTermMatcher.exact,\n        term_matcher_config: Dict[str, Any] = None,\n    ):\n\n        self.nlp = nlp\n\n        self.attr = attr\n\n        if term_matcher == GenericTermMatcher.exact:\n            self.phrase_matcher = EDSPhraseMatcher(\n                self.nlp.vocab,\n                attr=attr,\n                ignore_excluded=ignore_excluded,\n                ignore_space_tokens=ignore_space_tokens,\n                **(term_matcher_config or {}),\n            )\n        elif term_matcher == GenericTermMatcher.simstring:\n            self.phrase_matcher = SimstringMatcher(\n                self.nlp.vocab,\n                attr=attr,\n                ignore_excluded=ignore_excluded,\n                ignore_space_tokens=ignore_space_tokens,\n                **(term_matcher_config or {}),\n            )\n        else:\n            raise ValueError(\n                f\"Algorithm {repr(term_matcher)} does not belong to\"\n                f\" known matcher [exact, simstring].\"\n            )\n\n        self.regex_matcher = RegexMatcher(\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n        )\n\n        self.phrase_matcher.build_patterns(nlp=nlp, terms=terms)\n        self.regex_matcher.build_patterns(regex=regex)\n\n        self.set_extensions()\n\n    def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Find matching spans in doc.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object.\n\n        Returns\n        -------\n        spans:\n            List of Spans returned by the matchers.\n        \"\"\"\n\n        matches = self.phrase_matcher(doc, as_spans=True)\n        regex_matches = self.regex_matcher(doc, as_spans=True)\n\n        spans = list(matches) + list(regex_matches)\n\n        return spans\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Adds spans to document.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for extracted terms.\n        \"\"\"\n        matches = self.process(doc)\n\n        for span in matches:\n            if span.label_ not in doc.spans:\n                doc.spans[span.label_] = []\n            doc.spans[span.label_].append(span)\n\n        ents, discarded = filter_spans(list(doc.ents) + matches, return_discarded=True)\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.process","title":"<code>process(doc)</code>","text":"<p>Find matching spans in doc.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/core/matcher/matcher.py</code> <pre><code>def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Find matching spans in doc.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object.\n\n    Returns\n    -------\n    spans:\n        List of Spans returned by the matchers.\n    \"\"\"\n\n    matches = self.phrase_matcher(doc, as_spans=True)\n    regex_matches = self.regex_matcher(doc, as_spans=True)\n\n    spans = list(matches) + list(regex_matches)\n\n    return spans\n</code></pre>"},{"location":"reference/pipelines/core/matcher/matcher/#edsnlp.pipelines.core.matcher.matcher.GenericMatcher.__call__","title":"<code>__call__(doc)</code>","text":"<p>Adds spans to document.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/core/matcher/matcher.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Adds spans to document.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for extracted terms.\n    \"\"\"\n    matches = self.process(doc)\n\n    for span in matches:\n        if span.label_ not in doc.spans:\n            doc.spans[span.label_] = []\n        doc.spans[span.label_].append(span)\n\n    ents, discarded = filter_spans(list(doc.ents) + matches, return_discarded=True)\n\n    doc.ents = ents\n\n    if \"discarded\" not in doc.spans:\n        doc.spans[\"discarded\"] = []\n    doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/","title":"<code>edsnlp.pipelines.core.normalizer</code>","text":""},{"location":"reference/pipelines/core/normalizer/factory/","title":"<code>edsnlp.pipelines.core.normalizer.factory</code>","text":""},{"location":"reference/pipelines/core/normalizer/factory/#edsnlp.pipelines.core.normalizer.factory.create_component","title":"<code>create_component(nlp, name='eds.normalizer', accents=True, lowercase=True, quotes=True, spaces=True, pollution=True)</code>","text":"<p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul> PARAMETER DESCRIPTION <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>accents</code> <p><code>Accents</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>quotes</code> <p><code>Quotes</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>spaces</code> <p><code>Spaces</code> configuration object</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> configuration object.</p> <p> TYPE: <code>Union[bool, Dict[str, Any]]</code> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/factory.py</code> <pre><code>@deprecated_factory(\n    \"normalizer\",\n    \"eds.normalizer\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"token.norm\", \"token.tag\"],\n)\n@Language.factory(\n    \"eds.normalizer\", default_config=DEFAULT_CONFIG, assigns=[\"token.norm\", \"token.tag\"]\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.normalizer\",\n    accents: Union[bool, Dict[str, Any]] = True,\n    lowercase: Union[bool, Dict[str, Any]] = True,\n    quotes: Union[bool, Dict[str, Any]] = True,\n    spaces: Union[bool, Dict[str, Any]] = True,\n    pollution: Union[bool, Dict[str, Any]] = True,\n) -&gt; Normalizer:\n\"\"\"\n    Normalisation pipeline. Modifies the `NORM` attribute,\n    acting on five dimensions :\n\n    - `lowercase`: using the default `NORM`\n    - `accents`: deterministic and fixed-length normalisation of accents.\n    - `quotes`: deterministic and fixed-length normalisation of quotation marks.\n    - `spaces`: \"removal\" of spaces tokens (via the tag_ attribute).\n    - `pollution`: \"removal\" of pollutions (via the tag_ attribute).\n\n    Parameters\n    ----------\n    lowercase : bool\n        Whether to remove case.\n    accents : Union[bool, Dict[str, Any]]\n        `Accents` configuration object\n    quotes : Union[bool, Dict[str, Any]]\n        `Quotes` configuration object\n    spaces : Union[bool, Dict[str, Any]]\n        `Spaces` configuration object\n    pollution : Union[bool, Dict[str, Any]]\n        Optional `Pollution` configuration object.\n    \"\"\"\n\n    if accents:\n        config = dict(**accents_config)\n        if isinstance(accents, dict):\n            config.update(accents)\n        accents = registry.get(\"factories\", \"eds.accents\")(nlp, \"eds.accents\", **config)\n\n    if quotes:\n        config = dict(**quotes_config)\n        if isinstance(quotes, dict):\n            config.update(quotes)\n        quotes = registry.get(\"factories\", \"eds.quotes\")(nlp, \"eds.quotes\", **config)\n\n    if spaces:\n        config = dict(**spaces_config)\n        if isinstance(spaces, dict):\n            config.update(spaces)\n        spaces = registry.get(\"factories\", \"eds.spaces\")(nlp, \"eds.spaces\", **config)\n\n    if pollution:\n        config = dict(**pollution_config[\"pollution\"])\n        if isinstance(pollution, dict):\n            config.update(pollution)\n        pollution = registry.get(\"factories\", \"eds.pollution\")(\n            nlp, \"eds.pollution\", pollution=config\n        )\n\n    normalizer = Normalizer(\n        lowercase=lowercase,\n        accents=accents or None,\n        quotes=quotes or None,\n        pollution=pollution or None,\n        spaces=spaces or None,\n    )\n\n    return normalizer\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/normalizer/","title":"<code>edsnlp.pipelines.core.normalizer.normalizer</code>","text":""},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer","title":"<code>Normalizer</code>","text":"<p>         Bases: <code>object</code></p> <p>Normalisation pipeline. Modifies the <code>NORM</code> attribute, acting on five dimensions :</p> <ul> <li><code>lowercase</code>: using the default <code>NORM</code></li> <li><code>accents</code>: deterministic and fixed-length normalisation of accents.</li> <li><code>quotes</code>: deterministic and fixed-length normalisation of quotation marks.</li> <li><code>spaces</code>: \"removal\" of spaces tokens (via the tag_ attribute).</li> <li><code>pollution</code>: \"removal\" of pollutions (via the tag_ attribute).</li> </ul> PARAMETER DESCRIPTION <code>lowercase</code> <p>Whether to remove case.</p> <p> TYPE: <code>bool</code> </p> <code>accents</code> <p>Optional <code>Accents</code> object.</p> <p> TYPE: <code>Optional[Accents]</code> </p> <code>quotes</code> <p>Optional <code>Quotes</code> object.</p> <p> TYPE: <code>Optional[Quotes]</code> </p> <code>spaces</code> <p>Optional <code>Spaces</code> object.</p> <p> TYPE: <code>Optional[Spaces]</code> </p> <code>pollution</code> <p>Optional <code>Pollution</code> object.</p> <p> TYPE: <code>Optional[Pollution]</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/normalizer.py</code> <pre><code>class Normalizer(object):\n\"\"\"\n    Normalisation pipeline. Modifies the `NORM` attribute,\n    acting on five dimensions :\n\n    - `lowercase`: using the default `NORM`\n    - `accents`: deterministic and fixed-length normalisation of accents.\n    - `quotes`: deterministic and fixed-length normalisation of quotation marks.\n    - `spaces`: \"removal\" of spaces tokens (via the tag_ attribute).\n    - `pollution`: \"removal\" of pollutions (via the tag_ attribute).\n\n    Parameters\n    ----------\n    lowercase : bool\n        Whether to remove case.\n    accents : Optional[Accents]\n        Optional `Accents` object.\n    quotes : Optional[Quotes]\n        Optional `Quotes` object.\n    spaces : Optional[Spaces]\n        Optional `Spaces` object.\n    pollution : Optional[Pollution]\n        Optional `Pollution` object.\n    \"\"\"\n\n    def __init__(\n        self,\n        lowercase: bool,\n        accents: Optional[Accents],\n        quotes: Optional[Quotes],\n        spaces: Optional[Spaces],\n        pollution: Optional[Pollution],\n    ):\n        self.lowercase = lowercase\n        self.accents = accents\n        self.quotes = quotes\n        self.spaces = spaces\n        self.pollution = pollution\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Apply the normalisation pipeline, one component at a time.\n\n        Parameters\n        ----------\n        doc : Doc\n            spaCy `Doc` object\n\n        Returns\n        -------\n        Doc\n            Doc object with `NORM` attribute modified\n        \"\"\"\n        if not self.lowercase:\n            remove_lowercase(doc)\n        if self.accents is not None:\n            self.accents(doc)\n        if self.quotes is not None:\n            self.quotes(doc)\n        if self.spaces is not None:\n            self.spaces(doc)\n        if self.pollution is not None:\n            self.pollution(doc)\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/normalizer/#edsnlp.pipelines.core.normalizer.normalizer.Normalizer.__call__","title":"<code>__call__(doc)</code>","text":"<p>Apply the normalisation pipeline, one component at a time.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy <code>Doc</code> object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>Doc object with <code>NORM</code> attribute modified</p> Source code in <code>edsnlp/pipelines/core/normalizer/normalizer.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Apply the normalisation pipeline, one component at a time.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy `Doc` object\n\n    Returns\n    -------\n    Doc\n        Doc object with `NORM` attribute modified\n    \"\"\"\n    if not self.lowercase:\n        remove_lowercase(doc)\n    if self.accents is not None:\n        self.accents(doc)\n    if self.quotes is not None:\n        self.quotes(doc)\n    if self.spaces is not None:\n        self.spaces(doc)\n    if self.pollution is not None:\n        self.pollution(doc)\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/accents/","title":"<code>edsnlp.pipelines.core.normalizer.accents</code>","text":""},{"location":"reference/pipelines/core/normalizer/accents/accents/","title":"<code>edsnlp.pipelines.core.normalizer.accents.accents</code>","text":""},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents","title":"<code>Accents</code>","text":"<p>         Bases: <code>object</code></p> <p>Normalises accents, using a same-length strategy.</p> PARAMETER DESCRIPTION <code>accents</code> <p>List of accentuated characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/accents/accents.py</code> <pre><code>class Accents(object):\n\"\"\"\n    Normalises accents, using a same-length strategy.\n\n    Parameters\n    ----------\n    accents : List[Tuple[str, str]]\n        List of accentuated characters and their transcription.\n    \"\"\"\n\n    def __init__(self, accents: Optional[List[Tuple[str, str]]]) -&gt; None:\n        if accents is None:\n            accents = patterns.accents\n\n        self.translation_table = str.maketrans(\n            \"\".join(accent_group for accent_group, _ in accents),\n            \"\".join(rep * len(accent_group) for accent_group, rep in accents),\n        )\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Remove accents from spacy `NORM` attribute.\n\n        Parameters\n        ----------\n        doc : Doc\n            The spaCy `Doc` object.\n\n        Returns\n        -------\n        Doc\n            The document, with accents removed in `Token.norm_`.\n        \"\"\"\n\n        for token in doc:\n            token.norm_ = token.norm_.translate(self.translation_table)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/accents/accents/#edsnlp.pipelines.core.normalizer.accents.accents.Accents.__call__","title":"<code>__call__(doc)</code>","text":"<p>Remove accents from spacy <code>NORM</code> attribute.</p> PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>The document, with accents removed in <code>Token.norm_</code>.</p> Source code in <code>edsnlp/pipelines/core/normalizer/accents/accents.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Remove accents from spacy `NORM` attribute.\n\n    Parameters\n    ----------\n    doc : Doc\n        The spaCy `Doc` object.\n\n    Returns\n    -------\n    Doc\n        The document, with accents removed in `Token.norm_`.\n    \"\"\"\n\n    for token in doc:\n        token.norm_ = token.norm_.translate(self.translation_table)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/accents/factory/","title":"<code>edsnlp.pipelines.core.normalizer.accents.factory</code>","text":""},{"location":"reference/pipelines/core/normalizer/accents/patterns/","title":"<code>edsnlp.pipelines.core.normalizer.accents.patterns</code>","text":""},{"location":"reference/pipelines/core/normalizer/lowercase/","title":"<code>edsnlp.pipelines.core.normalizer.lowercase</code>","text":""},{"location":"reference/pipelines/core/normalizer/lowercase/factory/","title":"<code>edsnlp.pipelines.core.normalizer.lowercase.factory</code>","text":""},{"location":"reference/pipelines/core/normalizer/lowercase/factory/#edsnlp.pipelines.core.normalizer.lowercase.factory.remove_lowercase","title":"<code>remove_lowercase(doc)</code>","text":"<p>Add case on the <code>NORM</code> custom attribute. Should always be applied first.</p> PARAMETER DESCRIPTION <code>doc</code> <p>The spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>The document, with case put back in <code>NORM</code>.</p> Source code in <code>edsnlp/pipelines/core/normalizer/lowercase/factory.py</code> <pre><code>@Language.component(\"remove-lowercase\", assigns=[\"token.norm\"])\n@Language.component(\"eds.remove-lowercase\", assigns=[\"token.norm\"])\ndef remove_lowercase(doc: Doc):\n\"\"\"\n    Add case on the `NORM` custom attribute. Should always be applied first.\n\n    Parameters\n    ----------\n    doc : Doc\n        The spaCy `Doc` object.\n\n    Returns\n    -------\n    Doc\n        The document, with case put back in `NORM`.\n    \"\"\"\n\n    for token in doc:\n        token.norm_ = token.text\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/pollution/","title":"<code>edsnlp.pipelines.core.normalizer.pollution</code>","text":""},{"location":"reference/pipelines/core/normalizer/pollution/factory/","title":"<code>edsnlp.pipelines.core.normalizer.pollution.factory</code>","text":""},{"location":"reference/pipelines/core/normalizer/pollution/patterns/","title":"<code>edsnlp.pipelines.core.normalizer.pollution.patterns</code>","text":""},{"location":"reference/pipelines/core/normalizer/pollution/pollution/","title":"<code>edsnlp.pipelines.core.normalizer.pollution.pollution</code>","text":""},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution","title":"<code>Pollution</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Tags pollution tokens.</p> <p>Populates a number of spaCy extensions :</p> <ul> <li><code>Token._.pollution</code> : indicates whether the token is a pollution</li> <li><code>Doc._.clean</code> : lists non-pollution tokens</li> <li><code>Doc._.clean_</code> : original text with pollutions removed.</li> <li><code>Doc._.char_clean_span</code> : method to create a Span using character   indices extracted using the cleaned text.</li> </ul> PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>Language</code> </p> <code>pollution</code> <p>Dictionary containing regular expressions of pollution.</p> <p> TYPE: <code>Dict[str, Union[str, List[str]]]</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/pollution/pollution.py</code> <pre><code>class Pollution(BaseComponent):\n\"\"\"\n    Tags pollution tokens.\n\n    Populates a number of spaCy extensions :\n\n    - `Token._.pollution` : indicates whether the token is a pollution\n    - `Doc._.clean` : lists non-pollution tokens\n    - `Doc._.clean_` : original text with pollutions removed.\n    - `Doc._.char_clean_span` : method to create a Span using character\n      indices extracted using the cleaned text.\n\n    Parameters\n    ----------\n    nlp : Language\n        Language pipeline object\n    pollution : Dict[str, Union[str, List[str]]]\n        Dictionary containing regular expressions of pollution.\n    \"\"\"\n\n    # noinspection PyProtectedMember\n    def __init__(\n        self,\n        nlp: Language,\n        pollution: Optional[Dict[str, Union[bool, str, List[str]]]],\n    ):\n\n        self.nlp = nlp\n        self.nlp.vocab.strings.add(\"EXCLUDED\")\n\n        if pollution is None:\n            pollution = {k: True for k in patterns.pollution.keys()}\n        self.pollution = dict()\n\n        for k, v in pollution.items():\n            if v is True:\n                self.pollution[k] = [patterns.pollution[k]]\n            elif isinstance(v, str):\n                self.pollution[k] = [v]\n            elif isinstance(v, list):\n                self.pollution[k] = v\n\n        self.regex_matcher = RegexMatcher(flags=re.MULTILINE)\n        self.build_patterns()\n\n    def build_patterns(self) -&gt; None:\n\"\"\"\n        Builds the patterns for phrase matching.\n        \"\"\"\n\n        # efficiently build spaCy matcher patterns\n        for k, v in self.pollution.items():\n            self.regex_matcher.add(k, v)\n\n    def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Find pollutions in doc and clean candidate negations to remove pseudo negations\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        pollution:\n            list of pollution spans\n        \"\"\"\n\n        pollutions = self.regex_matcher(doc, as_spans=True)\n        pollutions = filter_spans(pollutions)\n\n        return pollutions\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Tags pollutions.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for pollutions.\n        \"\"\"\n        excluded_hash = doc.vocab.strings[\"EXCLUDED\"]\n        pollutions = self.process(doc)\n\n        for pollution in pollutions:\n\n            for token in pollution:\n                token._.excluded = True\n                token.tag = excluded_hash\n\n        doc.spans[\"pollutions\"] = pollutions\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.build_patterns","title":"<code>build_patterns()</code>","text":"<p>Builds the patterns for phrase matching.</p> Source code in <code>edsnlp/pipelines/core/normalizer/pollution/pollution.py</code> <pre><code>def build_patterns(self) -&gt; None:\n\"\"\"\n    Builds the patterns for phrase matching.\n    \"\"\"\n\n    # efficiently build spaCy matcher patterns\n    for k, v in self.pollution.items():\n        self.regex_matcher.add(k, v)\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.process","title":"<code>process(doc)</code>","text":"<p>Find pollutions in doc and clean candidate negations to remove pseudo negations</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>pollution</code> <p>list of pollution spans</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/pollution/pollution.py</code> <pre><code>def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Find pollutions in doc and clean candidate negations to remove pseudo negations\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    pollution:\n        list of pollution spans\n    \"\"\"\n\n    pollutions = self.regex_matcher(doc, as_spans=True)\n    pollutions = filter_spans(pollutions)\n\n    return pollutions\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/pollution/pollution/#edsnlp.pipelines.core.normalizer.pollution.pollution.Pollution.__call__","title":"<code>__call__(doc)</code>","text":"<p>Tags pollutions.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for pollutions.</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/pollution/pollution.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Tags pollutions.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for pollutions.\n    \"\"\"\n    excluded_hash = doc.vocab.strings[\"EXCLUDED\"]\n    pollutions = self.process(doc)\n\n    for pollution in pollutions:\n\n        for token in pollution:\n            token._.excluded = True\n            token.tag = excluded_hash\n\n    doc.spans[\"pollutions\"] = pollutions\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/quotes/","title":"<code>edsnlp.pipelines.core.normalizer.quotes</code>","text":""},{"location":"reference/pipelines/core/normalizer/quotes/factory/","title":"<code>edsnlp.pipelines.core.normalizer.quotes.factory</code>","text":""},{"location":"reference/pipelines/core/normalizer/quotes/patterns/","title":"<code>edsnlp.pipelines.core.normalizer.quotes.patterns</code>","text":""},{"location":"reference/pipelines/core/normalizer/quotes/quotes/","title":"<code>edsnlp.pipelines.core.normalizer.quotes.quotes</code>","text":""},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes","title":"<code>Quotes</code>","text":"<p>         Bases: <code>object</code></p> <p>We normalise quotes, following this <code>source &lt;https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html&gt;</code>_.</p> PARAMETER DESCRIPTION <code>quotes</code> <p>List of quotation characters and their transcription.</p> <p> TYPE: <code>List[Tuple[str, str]]</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/quotes/quotes.py</code> <pre><code>class Quotes(object):\n\"\"\"\n    We normalise quotes, following this\n    `source &lt;https://www.cl.cam.ac.uk/~mgk25/ucs/quotes.html&gt;`_.\n\n    Parameters\n    ----------\n    quotes : List[Tuple[str, str]]\n        List of quotation characters and their transcription.\n    \"\"\"\n\n    def __init__(self, quotes: Optional[List[Tuple[str, str]]]) -&gt; None:\n        if quotes is None:\n            quotes = quotes_and_apostrophes\n\n        self.translation_table = str.maketrans(\n            \"\".join(quote_group for quote_group, _ in quotes),\n            \"\".join(rep * len(quote_group) for quote_group, rep in quotes),\n        )\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Normalises quotes.\n\n        Parameters\n        ----------\n        doc : Doc\n            Document to process.\n\n        Returns\n        -------\n        Doc\n            Same document, with quotes normalised.\n        \"\"\"\n\n        for token in doc:\n            token.norm_ = token.norm_.translate(self.translation_table)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/quotes/quotes/#edsnlp.pipelines.core.normalizer.quotes.quotes.Quotes.__call__","title":"<code>__call__(doc)</code>","text":"<p>Normalises quotes.</p> PARAMETER DESCRIPTION <code>doc</code> <p>Document to process.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> <p>Same document, with quotes normalised.</p> Source code in <code>edsnlp/pipelines/core/normalizer/quotes/quotes.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Normalises quotes.\n\n    Parameters\n    ----------\n    doc : Doc\n        Document to process.\n\n    Returns\n    -------\n    Doc\n        Same document, with quotes normalised.\n    \"\"\"\n\n    for token in doc:\n        token.norm_ = token.norm_.translate(self.translation_table)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/spaces/","title":"<code>edsnlp.pipelines.core.normalizer.spaces</code>","text":""},{"location":"reference/pipelines/core/normalizer/spaces/factory/","title":"<code>edsnlp.pipelines.core.normalizer.spaces.factory</code>","text":""},{"location":"reference/pipelines/core/normalizer/spaces/factory/#edsnlp.pipelines.core.normalizer.spaces.factory.create_component","title":"<code>create_component(nlp, name, newline=True)</code>","text":"<p>Create a new component to update the <code>tag_</code> attribute of tokens.</p> <p>We assign \"SPACE\" to <code>token.tag</code> to be used by optimized components such as the EDSPhraseMatcher</p> PARAMETER DESCRIPTION <code>newline</code> <p>Whether to update the newline tokens too</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/spaces/factory.py</code> <pre><code>@Language.factory(\n    \"eds.spaces\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"token.tag\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str,\n    newline: bool = True,\n):\n\"\"\"\n    Create a new component to update the `tag_` attribute of tokens.\n\n    We assign \"SPACE\" to `token.tag` to be used by optimized components\n    such as the EDSPhraseMatcher\n\n    Parameters\n    ----------\n    newline : bool\n        Whether to update the newline tokens too\n    \"\"\"\n    return Spaces(newline=newline)\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/spaces/spaces/","title":"<code>edsnlp.pipelines.core.normalizer.spaces.spaces</code>","text":""},{"location":"reference/pipelines/core/normalizer/spaces/spaces/#edsnlp.pipelines.core.normalizer.spaces.spaces.Spaces","title":"<code>Spaces</code>","text":"<p>         Bases: <code>object</code></p> <p>We assign \"SPACE\" to <code>token.tag</code> to be used by optimized components such as the EDSPhraseMatcher</p> PARAMETER DESCRIPTION <code>newline</code> <p>Whether to update the newline tokens too</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/spaces/spaces.py</code> <pre><code>class Spaces(object):\n\"\"\"\n    We assign \"SPACE\" to `token.tag` to be used by optimized components\n    such as the EDSPhraseMatcher\n\n    Parameters\n    ----------\n    newline : bool\n        Whether to update the newline tokens too\n    \"\"\"\n\n    def __init__(self, newline: bool) -&gt; None:\n        self.newline = newline\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Apply the component to the doc.\n\n        Parameters\n        ----------\n        doc: Doc\n\n        Returns\n        -------\n        doc: Doc\n        \"\"\"\n        space_hash = doc.vocab.strings[\"SPACE\"]\n        for token in doc:\n            if len(token.text.strip()) == 0:\n                token.tag = space_hash\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/normalizer/spaces/spaces/#edsnlp.pipelines.core.normalizer.spaces.spaces.Spaces.__call__","title":"<code>__call__(doc)</code>","text":"<p>Apply the component to the doc.</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/core/normalizer/spaces/spaces.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Apply the component to the doc.\n\n    Parameters\n    ----------\n    doc: Doc\n\n    Returns\n    -------\n    doc: Doc\n    \"\"\"\n    space_hash = doc.vocab.strings[\"SPACE\"]\n    for token in doc:\n        if len(token.text.strip()) == 0:\n            token.tag = space_hash\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/core/sentences/","title":"<code>edsnlp.pipelines.core.sentences</code>","text":""},{"location":"reference/pipelines/core/sentences/factory/","title":"<code>edsnlp.pipelines.core.sentences.factory</code>","text":""},{"location":"reference/pipelines/core/sentences/terms/","title":"<code>edsnlp.pipelines.core.sentences.terms</code>","text":""},{"location":"reference/pipelines/core/terminology/","title":"<code>edsnlp.pipelines.core.terminology</code>","text":""},{"location":"reference/pipelines/core/terminology/factory/","title":"<code>edsnlp.pipelines.core.terminology.factory</code>","text":""},{"location":"reference/pipelines/core/terminology/factory/#edsnlp.pipelines.core.terminology.factory.create_component","title":"<code>create_component(nlp, label, terms, name='eds.terminology', attr='TEXT', regex=None, ignore_excluded=False, ignore_space_tokens=False, term_matcher='exact', term_matcher_config={})</code>","text":"<p>Provides a terminology matching component.</p> <p>The terminology matching component differs from the simple matcher component in that the <code>regex</code> and <code>terms</code> keys are used as spaCy's <code>kb_id</code>. All matched entities have the same label, defined in the top-level constructor (argument <code>label</code>).</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.terminology'</code> </p> <code>label</code> <p>Top-level label</p> <p> TYPE: <code>str</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>'exact'</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> Source code in <code>edsnlp/pipelines/core/terminology/factory.py</code> <pre><code>@Language.factory(\n    \"eds.terminology\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    label: str,\n    terms: Optional[Dict[str, Union[str, List[str]]]],\n    name: str = \"eds.terminology\",\n    attr: Union[str, Dict[str, str]] = \"TEXT\",\n    regex: Optional[Dict[str, Union[str, List[str]]]] = None,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    term_matcher: TerminologyTermMatcher = \"exact\",\n    term_matcher_config: Dict[str, Any] = {},\n):\n\"\"\"\n    Provides a terminology matching component.\n\n    The terminology matching component differs from the simple matcher component in that\n    the `regex` and `terms` keys are used as spaCy's `kb_id`. All matched entities\n    have the same label, defined in the top-level constructor (argument `label`).\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    name: str\n        The name of the component.\n    label : str\n        Top-level label\n    terms : Optional[Patterns]\n        A dictionary of terms.\n    regex : Optional[Patterns]\n        A dictionary of regular expressions.\n    attr : str\n        The default attribute to use for matching.\n        Can be overridden using the `terms` and `regex` configurations.\n    ignore_excluded : bool\n        Whether to skip excluded tokens (requires an upstream\n        pipeline to mark excluded tokens).\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n    term_matcher: TerminologyTermMatcher\n        The matcher to use for matching phrases ?\n        One of (exact, simstring)\n    term_matcher_config: Dict[str,Any]\n        Parameters of the matcher class\n    \"\"\"\n    assert not (terms is None and regex is None)\n\n    return TerminologyMatcher(\n        nlp,\n        label=label,\n        terms=terms or dict(),\n        attr=attr,\n        regex=regex or dict(),\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        term_matcher=term_matcher,\n        term_matcher_config=term_matcher_config,\n    )\n</code></pre>"},{"location":"reference/pipelines/core/terminology/terminology/","title":"<code>edsnlp.pipelines.core.terminology.terminology</code>","text":""},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher","title":"<code>TerminologyMatcher</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Provides a terminology matching component.</p> <p>The terminology matching component differs from the simple matcher component in that the <code>regex</code> and <code>terms</code> keys are used as spaCy's <code>kb_id</code>. All matched entities have the same label, defined in the top-level constructor (argument <code>label</code>).</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>label</code> <p>Top-level label</p> <p> TYPE: <code>str</code> </p> <code>terms</code> <p>A dictionary of terms.</p> <p> TYPE: <code>Optional[Patterns]</code> </p> <code>regex</code> <p>A dictionary of regular expressions.</p> <p> TYPE: <code>Optional[Patterns]</code> </p> <code>attr</code> <p>The default attribute to use for matching. Can be overridden using the <code>terms</code> and <code>regex</code> configurations.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens (requires an upstream pipeline to mark excluded tokens).</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The matcher to use for matching phrases ? One of (exact, simstring)</p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>Parameters of the matcher class</p> <p> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/pipelines/core/terminology/terminology.py</code> <pre><code>class TerminologyMatcher(BaseComponent):\n\"\"\"\n    Provides a terminology matching component.\n\n    The terminology matching component differs from the simple matcher component in that\n    the `regex` and `terms` keys are used as spaCy's `kb_id`. All matched entities\n    have the same label, defined in the top-level constructor (argument `label`).\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    label : str\n        Top-level label\n    terms : Optional[Patterns]\n        A dictionary of terms.\n    regex : Optional[Patterns]\n        A dictionary of regular expressions.\n    attr : str\n        The default attribute to use for matching.\n        Can be overridden using the `terms` and `regex` configurations.\n    ignore_excluded : bool\n        Whether to skip excluded tokens (requires an upstream\n        pipeline to mark excluded tokens).\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n    term_matcher: TerminologyTermMatcher\n        The matcher to use for matching phrases ?\n        One of (exact, simstring)\n    term_matcher_config: Dict[str,Any]\n        Parameters of the matcher class\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        label: str,\n        terms: Optional[Patterns],\n        regex: Optional[Patterns],\n        attr: str,\n        ignore_excluded: bool,\n        ignore_space_tokens: bool = False,\n        term_matcher: TerminologyTermMatcher = TerminologyTermMatcher.exact,\n        term_matcher_config=None,\n    ):\n\n        self.nlp = nlp\n\n        self.label = label\n\n        self.attr = attr\n\n        if term_matcher == TerminologyTermMatcher.exact:\n            self.phrase_matcher = EDSPhraseMatcher(\n                self.nlp.vocab,\n                attr=attr,\n                ignore_excluded=ignore_excluded,\n                ignore_space_tokens=ignore_space_tokens,\n                **(term_matcher_config or {}),\n            )\n        elif term_matcher == TerminologyTermMatcher.simstring:\n            self.phrase_matcher = SimstringMatcher(\n                vocab=self.nlp.vocab,\n                attr=attr,\n                ignore_excluded=ignore_excluded,\n                ignore_space_tokens=ignore_space_tokens,\n                **(term_matcher_config or {}),\n            )\n        else:\n            raise ValueError(\n                f\"Algorithm {repr(term_matcher)} does not belong to\"\n                f\" known matchers [exact, simstring].\"\n            )\n\n        self.regex_matcher = RegexMatcher(\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n        )\n\n        self.phrase_matcher.build_patterns(nlp=nlp, terms=terms, progress=True)\n        self.regex_matcher.build_patterns(regex=regex)\n\n        self.set_extensions()\n\n    def set_extensions(self) -&gt; None:\n        super().set_extensions()\n        if not Span.has_extension(self.label):\n            Span.set_extension(self.label, default=None)\n\n    def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Find matching spans in doc.\n\n        Post-process matches to account for terminology.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object.\n\n        Returns\n        -------\n        spans:\n            List of Spans returned by the matchers.\n        \"\"\"\n\n        matches = self.phrase_matcher(doc, as_spans=True)\n        regex_matches = self.regex_matcher(doc, as_spans=True)\n\n        spans = []\n\n        for match in chain(matches, regex_matches):\n            span = Span(\n                doc=match.doc,\n                start=match.start,\n                end=match.end,\n                label=self.label,\n                kb_id=match.label,\n            )\n            span._.set(self.label, match.label_)\n            spans.append(span)\n\n        return spans\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Adds spans to document.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for extracted terms.\n        \"\"\"\n        matches = self.process(doc)\n\n        if self.label not in doc.spans:\n            doc.spans[self.label] = matches\n\n        ents, discarded = filter_spans(list(doc.ents) + matches, return_discarded=True)\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.process","title":"<code>process(doc)</code>","text":"<p>Find matching spans in doc.</p> <p>Post-process matches to account for terminology.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>List of Spans returned by the matchers.</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/core/terminology/terminology.py</code> <pre><code>def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Find matching spans in doc.\n\n    Post-process matches to account for terminology.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object.\n\n    Returns\n    -------\n    spans:\n        List of Spans returned by the matchers.\n    \"\"\"\n\n    matches = self.phrase_matcher(doc, as_spans=True)\n    regex_matches = self.regex_matcher(doc, as_spans=True)\n\n    spans = []\n\n    for match in chain(matches, regex_matches):\n        span = Span(\n            doc=match.doc,\n            start=match.start,\n            end=match.end,\n            label=self.label,\n            kb_id=match.label,\n        )\n        span._.set(self.label, match.label_)\n        spans.append(span)\n\n    return spans\n</code></pre>"},{"location":"reference/pipelines/core/terminology/terminology/#edsnlp.pipelines.core.terminology.terminology.TerminologyMatcher.__call__","title":"<code>__call__(doc)</code>","text":"<p>Adds spans to document.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/core/terminology/terminology.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Adds spans to document.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for extracted terms.\n    \"\"\"\n    matches = self.process(doc)\n\n    if self.label not in doc.spans:\n        doc.spans[self.label] = matches\n\n    ents, discarded = filter_spans(list(doc.ents) + matches, return_discarded=True)\n\n    doc.ents = ents\n\n    if \"discarded\" not in doc.spans:\n        doc.spans[\"discarded\"] = []\n    doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/misc/","title":"<code>edsnlp.pipelines.misc</code>","text":""},{"location":"reference/pipelines/misc/consultation_dates/","title":"<code>edsnlp.pipelines.misc.consultation_dates</code>","text":""},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/","title":"<code>edsnlp.pipelines.misc.consultation_dates.consultation_dates</code>","text":""},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates","title":"<code>ConsultationDates</code>","text":"<p>         Bases: <code>GenericMatcher</code></p> <p>Class to extract consultation dates from \"CR-CONS\" documents.</p> <p>The pipeline populates the <code>doc.spans['consultation_dates']</code> list.</p> <p>For each extraction <code>s</code> in this list, the corresponding date is available as <code>s._.consultation_date</code>.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>Language</code> </p> <code>consultation_mention</code> <p>List of RegEx for consultation mentions.</p> <ul> <li>If <code>type==list</code>: Overrides the default list</li> <li>If <code>type==bool</code>: Uses the default list of True, disable if False</li> </ul> <p> TYPE: <code>Union[List[str], bool]</code> </p> <p>town_mention : Union[List[str], bool]     List of RegEx for all AP-HP hospitals' towns mentions.</p> <pre><code>- If `type==list`: Overrides the default list\n- If `type==bool`: Uses the default list of True, disable if False\n</code></pre> <p>document_date_mention : Union[List[str], bool]     List of RegEx for document date.</p> <pre><code>- If `type==list`: Overrides the default list\n- If `type==bool`: Uses the default list of True, disable if False\n</code></pre> Source code in <code>edsnlp/pipelines/misc/consultation_dates/consultation_dates.py</code> <pre><code>class ConsultationDates(GenericMatcher):\n\"\"\"\n    Class to extract consultation dates from \"CR-CONS\" documents.\n\n    The pipeline populates the `#!python doc.spans['consultation_dates']` list.\n\n    For each extraction `s` in this list, the corresponding date is available\n    as `s._.consultation_date`.\n\n    Parameters\n    ----------\n    nlp : Language\n        Language pipeline object\n    consultation_mention : Union[List[str], bool]\n        List of RegEx for consultation mentions.\n\n        - If `type==list`: Overrides the default list\n        - If `type==bool`: Uses the default list of True, disable if False\n\n    town_mention : Union[List[str], bool]\n        List of RegEx for all AP-HP hospitals' towns mentions.\n\n        - If `type==list`: Overrides the default list\n        - If `type==bool`: Uses the default list of True, disable if False\n    document_date_mention : Union[List[str], bool]\n        List of RegEx for document date.\n\n        - If `type==list`: Overrides the default list\n        - If `type==bool`: Uses the default list of True, disable if False\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        consultation_mention: Union[List[str], bool],\n        town_mention: Union[List[str], bool],\n        document_date_mention: Union[List[str], bool],\n        attr: str,\n        **kwargs,\n    ):\n\n        logger.warning(\"This pipeline is still in beta\")\n        logger.warning(\n            \"This pipeline should ONLY be used on notes \"\n            \"where `note_class_source_value == 'CR-CONS'`\"\n        )\n        logger.warning(\n\"\"\"This pipeline requires to use the normalizer pipeline with:\n        lowercase=True,\n        accents=True,\n        quotes=True\"\"\"\n        )\n\n        if not (nlp.has_pipe(\"dates\") and nlp.get_pipe(\"dates\").on_ents_only is False):\n\n            config = dict(**DEFAULT_CONFIG)\n            config[\"on_ents_only\"] = \"consultation_mentions\"\n\n            self.date_matcher = Dates(nlp, **config)\n\n        else:\n            self.date_matcher = None\n\n        if not consultation_mention:\n            consultation_mention = []\n        elif consultation_mention is True:\n            consultation_mention = consult_regex.consultation_mention\n\n        if not document_date_mention:\n            document_date_mention = []\n        elif document_date_mention is True:\n            document_date_mention = consult_regex.document_date_mention\n\n        if not town_mention:\n            town_mention = []\n        elif town_mention is True:\n            town_mention = consult_regex.town_mention\n\n        regex = dict(\n            consultation_mention=consultation_mention,\n            town_mention=town_mention,\n            document_date_mention=document_date_mention,\n        )\n\n        super().__init__(\n            nlp,\n            regex=regex,\n            terms=dict(),\n            attr=attr,\n            ignore_excluded=False,\n            **kwargs,\n        )\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        if not Span.has_extension(\"consultation_date\"):\n            Span.set_extension(\"consultation_date\", default=None)\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Finds entities\n\n        Parameters\n        ----------\n        doc: spaCy Doc object\n\n        Returns\n        -------\n        doc: Doc\n            spaCy Doc object with additional\n            `doc.spans['consultation_dates]` `SpanGroup`\n        \"\"\"\n\n        ents = self.process(doc)\n\n        doc.spans[\"consultation_mentions\"] = ents\n        doc.spans[\"consultation_dates\"] = []\n\n        if self.date_matcher is not None:\n            doc = self.date_matcher(doc)\n\n        for mention in ents:\n            # Looking for a date\n            # - In the same sentence\n            # - Not less than 10 tokens AFTER the consultation mention\n            matching_dates = [\n                date\n                for date in doc.spans[\"dates\"]\n                if (\n                    (mention.sent == date.sent)\n                    and (date.start &gt; mention.start)\n                    and (date.start - mention.end &lt;= 10)\n                )\n            ]\n\n            if matching_dates:\n                # We keep the first mention of a date\n                kept_date = min(matching_dates, key=lambda d: d.start)\n                span = doc[mention.start : kept_date.end]\n                span.label_ = mention.label_\n                span._.consultation_date = kept_date._.date\n\n                doc.spans[\"consultation_dates\"].append(span)\n\n        del doc.spans[\"consultation_mentions\"]\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/misc/consultation_dates/consultation_dates/#edsnlp.pipelines.misc.consultation_dates.consultation_dates.ConsultationDates.__call__","title":"<code>__call__(doc)</code>","text":"<p>Finds entities</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object with additional <code>doc.spans['consultation_dates]</code> <code>SpanGroup</code></p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/misc/consultation_dates/consultation_dates.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Finds entities\n\n    Parameters\n    ----------\n    doc: spaCy Doc object\n\n    Returns\n    -------\n    doc: Doc\n        spaCy Doc object with additional\n        `doc.spans['consultation_dates]` `SpanGroup`\n    \"\"\"\n\n    ents = self.process(doc)\n\n    doc.spans[\"consultation_mentions\"] = ents\n    doc.spans[\"consultation_dates\"] = []\n\n    if self.date_matcher is not None:\n        doc = self.date_matcher(doc)\n\n    for mention in ents:\n        # Looking for a date\n        # - In the same sentence\n        # - Not less than 10 tokens AFTER the consultation mention\n        matching_dates = [\n            date\n            for date in doc.spans[\"dates\"]\n            if (\n                (mention.sent == date.sent)\n                and (date.start &gt; mention.start)\n                and (date.start - mention.end &lt;= 10)\n            )\n        ]\n\n        if matching_dates:\n            # We keep the first mention of a date\n            kept_date = min(matching_dates, key=lambda d: d.start)\n            span = doc[mention.start : kept_date.end]\n            span.label_ = mention.label_\n            span._.consultation_date = kept_date._.date\n\n            doc.spans[\"consultation_dates\"].append(span)\n\n    del doc.spans[\"consultation_mentions\"]\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/misc/consultation_dates/factory/","title":"<code>edsnlp.pipelines.misc.consultation_dates.factory</code>","text":""},{"location":"reference/pipelines/misc/consultation_dates/patterns/","title":"<code>edsnlp.pipelines.misc.consultation_dates.patterns</code>","text":""},{"location":"reference/pipelines/misc/dates/","title":"<code>edsnlp.pipelines.misc.dates</code>","text":""},{"location":"reference/pipelines/misc/dates/dates/","title":"<code>edsnlp.pipelines.misc.dates.dates</code>","text":"<p><code>eds.dates</code> pipeline.</p>"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates","title":"<code>Dates</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Tags and normalizes dates, using the open-source <code>dateparser</code> library.</p> <p>The pipeline uses spaCy's <code>filter_spans</code> function. It filters out false positives, and introduce a hierarchy between patterns. For instance, in case of ambiguity, the pipeline will decide that a date is a date without a year rather than a date without a day.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>spacy.language.Language</code> </p> <code>absolute</code> <p>List of regular expressions for absolute dates.</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <code>relative</code> <p>List of regular expressions for relative dates (eg <code>hier</code>, <code>la semaine prochaine</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <code>duration</code> <p>List of regular expressions for durations (eg <code>pendant trois mois</code>).</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <code>false_positive</code> <p>List of regular expressions for false positive (eg phone numbers, etc).</p> <p> TYPE: <code>Union[List[str], str]</code> </p> <code>on_ents_only</code> <p>Wether to look on dates in the whole document or in specific sentences:</p> <ul> <li>If <code>True</code>: Only look in the sentences of each entity in doc.ents</li> <li>If False: Look in the whole document</li> <li>If given a string <code>key</code> or list of string: Only look in the sentences of each entity in <code>doc.spans[key]</code></li> </ul> <p> TYPE: <code>Union[bool, str, List[str]]</code> </p> <code>detect_periods</code> <p>Whether to detect periods (experimental)</p> <p> TYPE: <code>bool</code> </p> <code>as_ents</code> <p>Whether to treat dates as entities</p> <p> TYPE: <code>bool</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> </p> Source code in <code>edsnlp/pipelines/misc/dates/dates.py</code> <pre><code>class Dates(BaseComponent):\n\"\"\"\n    Tags and normalizes dates, using the open-source `dateparser` library.\n\n    The pipeline uses spaCy's `filter_spans` function.\n    It filters out false positives, and introduce a hierarchy between patterns.\n    For instance, in case of ambiguity, the pipeline will decide that a date is a\n    date without a year rather than a date without a day.\n\n    Parameters\n    ----------\n    nlp : spacy.language.Language\n        Language pipeline object\n    absolute : Union[List[str], str]\n        List of regular expressions for absolute dates.\n    relative : Union[List[str], str]\n        List of regular expressions for relative dates\n        (eg `hier`, `la semaine prochaine`).\n    duration : Union[List[str], str]\n        List of regular expressions for durations\n        (eg `pendant trois mois`).\n    false_positive : Union[List[str], str]\n        List of regular expressions for false positive (eg phone numbers, etc).\n    on_ents_only : Union[bool, str, List[str]]\n        Wether to look on dates in the whole document or in specific sentences:\n\n        - If `True`: Only look in the sentences of each entity in doc.ents\n        - If False: Look in the whole document\n        - If given a string `key` or list of string: Only look in the sentences of\n          each entity in `#!python doc.spans[key]`\n    detect_periods : bool\n        Whether to detect periods (experimental)\n    as_ents : bool\n        Whether to treat dates as entities\n    attr : str\n        spaCy attribute to use\n    \"\"\"\n\n    # noinspection PyProtectedMember\n    def __init__(\n        self,\n        nlp: Language,\n        absolute: Optional[List[str]],\n        relative: Optional[List[str]],\n        duration: Optional[List[str]],\n        false_positive: Optional[List[str]],\n        on_ents_only: Union[bool, List[str]],\n        detect_periods: bool,\n        detect_time: bool,\n        as_ents: bool,\n        attr: str,\n    ):\n\n        self.nlp = nlp\n\n        if absolute is None:\n            if detect_time:\n                absolute = patterns.absolute_pattern_with_time\n            else:\n                absolute = patterns.absolute_pattern\n        if relative is None:\n            relative = patterns.relative_pattern\n        if duration is None:\n            duration = patterns.duration_pattern\n        if false_positive is None:\n            false_positive = patterns.false_positive_pattern\n\n        if isinstance(absolute, str):\n            absolute = [absolute]\n        if isinstance(relative, str):\n            relative = [relative]\n        if isinstance(duration, str):\n            relative = [duration]\n        if isinstance(false_positive, str):\n            false_positive = [false_positive]\n\n        self.on_ents_only = on_ents_only\n        self.regex_matcher = RegexMatcher(attr=attr, alignment_mode=\"strict\")\n\n        self.regex_matcher.add(\"false_positive\", false_positive)\n        self.regex_matcher.add(\"absolute\", absolute)\n        self.regex_matcher.add(\"relative\", relative)\n        self.regex_matcher.add(\"duration\", duration)\n\n        self.detect_periods = detect_periods\n\n        self.as_ents = as_ents\n\n        if detect_periods:\n            logger.warning(\"The period extractor is experimental.\")\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\"\"\"\n        Set extensions for the dates pipeline.\n        \"\"\"\n\n        if not Span.has_extension(\"datetime\"):\n            Span.set_extension(\"datetime\", default=None)\n\n        if not Span.has_extension(\"date\"):\n            Span.set_extension(\"date\", default=None)\n\n        if not Span.has_extension(\"period\"):\n            Span.set_extension(\"period\", default=None)\n\n    def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Find dates in doc.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        dates:\n            list of date spans\n        \"\"\"\n\n        if self.on_ents_only:\n\n            if type(self.on_ents_only) == bool:\n                ents = doc.ents\n            else:\n                if type(self.on_ents_only) == str:\n                    self.on_ents_only = [self.on_ents_only]\n                ents = []\n                for key in self.on_ents_only:\n                    ents.extend(list(doc.spans[key]))\n\n            dates = []\n            for sent in set([ent.sent for ent in ents]):\n                dates = chain(\n                    dates,\n                    self.regex_matcher(\n                        sent,\n                        as_spans=True,\n                        return_groupdict=True,\n                    ),\n                )\n\n        else:\n            dates = self.regex_matcher(\n                doc,\n                as_spans=True,\n                return_groupdict=True,\n            )\n\n        dates = filter_spans(dates)\n        dates = [date for date in dates if date[0].label_ != \"false_positive\"]\n\n        return dates\n\n    def parse(self, dates: List[Tuple[Span, Dict[str, str]]]) -&gt; List[Span]:\n\"\"\"\n        Parse dates using the groupdict returned by the matcher.\n\n        Parameters\n        ----------\n        dates : List[Tuple[Span, Dict[str, str]]]\n            List of tuples containing the spans and groupdict\n            returned by the matcher.\n\n        Returns\n        -------\n        List[Span]\n            List of processed spans, with the date parsed.\n        \"\"\"\n\n        for span, groupdict in dates:\n            if span.label_ == \"relative\":\n                parsed = RelativeDate.parse_obj(groupdict)\n            elif span.label_ == \"absolute\":\n                parsed = AbsoluteDate.parse_obj(groupdict)\n            else:\n                parsed = Duration.parse_obj(groupdict)\n\n            span._.date = parsed\n\n        return [span for span, _ in dates]\n\n    def process_periods(self, dates: List[Span]) -&gt; List[Span]:\n\"\"\"\n        Experimental period detection.\n\n        Parameters\n        ----------\n        dates : List[Span]\n            List of detected dates.\n\n        Returns\n        -------\n        List[Span]\n            List of detected periods.\n        \"\"\"\n\n        if len(dates) &lt; 2:\n            return []\n\n        periods = []\n        seen = set()\n\n        dates = list(sorted(dates, key=lambda d: d.start))\n\n        for d1, d2 in zip(dates[:-1], dates[1:]):\n\n            if d1._.date.mode == Mode.DURATION or d2._.date.mode == Mode.DURATION:\n                pass\n            elif d1 in seen or d1._.date.mode is None or d2._.date.mode is None:\n                continue\n\n            if (\n                d1.end - d2.start &lt; PERIOD_PROXIMITY_THRESHOLD\n                and d1._.date.mode != d2._.date.mode\n            ):\n\n                period = Span(d1.doc, d1.start, d2.end, label=\"period\")\n\n                # If one date is a duration,\n                # the other may not have a registered mode.\n                m1 = d1._.date.mode or Mode.FROM\n                m2 = d2._.date.mode or Mode.FROM\n\n                period._.period = Period.parse_obj(\n                    {\n                        m1.value: d1,\n                        m2.value: d2,\n                    }\n                )\n\n                seen.add(d1)\n                seen.add(d2)\n\n                periods.append(period)\n\n        return periods\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Tags dates.\n\n        Parameters\n        ----------\n        doc : Doc\n            spaCy Doc object\n\n        Returns\n        -------\n        doc : Doc\n            spaCy Doc object, annotated for dates\n        \"\"\"\n        dates = self.process(doc)\n        dates = self.parse(dates)\n\n        doc.spans[\"dates\"] = dates\n\n        if self.detect_periods:\n            doc.spans[\"periods\"] = self.process_periods(dates)\n\n        if self.as_ents:\n            ents, discarded = filter_spans(\n                list(doc.ents) + dates, return_discarded=True\n            )\n\n            doc.ents = ents\n\n            if \"discarded\" not in doc.spans:\n                doc.spans[\"discarded\"] = []\n            doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.set_extensions","title":"<code>set_extensions()</code>  <code>classmethod</code>","text":"<p>Set extensions for the dates pipeline.</p> Source code in <code>edsnlp/pipelines/misc/dates/dates.py</code> <pre><code>@classmethod\ndef set_extensions(cls) -&gt; None:\n\"\"\"\n    Set extensions for the dates pipeline.\n    \"\"\"\n\n    if not Span.has_extension(\"datetime\"):\n        Span.set_extension(\"datetime\", default=None)\n\n    if not Span.has_extension(\"date\"):\n        Span.set_extension(\"date\", default=None)\n\n    if not Span.has_extension(\"period\"):\n        Span.set_extension(\"period\", default=None)\n</code></pre>"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.process","title":"<code>process(doc)</code>","text":"<p>Find dates in doc.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>dates</code> <p>list of date spans</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/misc/dates/dates.py</code> <pre><code>def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Find dates in doc.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    dates:\n        list of date spans\n    \"\"\"\n\n    if self.on_ents_only:\n\n        if type(self.on_ents_only) == bool:\n            ents = doc.ents\n        else:\n            if type(self.on_ents_only) == str:\n                self.on_ents_only = [self.on_ents_only]\n            ents = []\n            for key in self.on_ents_only:\n                ents.extend(list(doc.spans[key]))\n\n        dates = []\n        for sent in set([ent.sent for ent in ents]):\n            dates = chain(\n                dates,\n                self.regex_matcher(\n                    sent,\n                    as_spans=True,\n                    return_groupdict=True,\n                ),\n            )\n\n    else:\n        dates = self.regex_matcher(\n            doc,\n            as_spans=True,\n            return_groupdict=True,\n        )\n\n    dates = filter_spans(dates)\n    dates = [date for date in dates if date[0].label_ != \"false_positive\"]\n\n    return dates\n</code></pre>"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.parse","title":"<code>parse(dates)</code>","text":"<p>Parse dates using the groupdict returned by the matcher.</p> PARAMETER DESCRIPTION <code>dates</code> <p>List of tuples containing the spans and groupdict returned by the matcher.</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of processed spans, with the date parsed.</p> Source code in <code>edsnlp/pipelines/misc/dates/dates.py</code> <pre><code>def parse(self, dates: List[Tuple[Span, Dict[str, str]]]) -&gt; List[Span]:\n\"\"\"\n    Parse dates using the groupdict returned by the matcher.\n\n    Parameters\n    ----------\n    dates : List[Tuple[Span, Dict[str, str]]]\n        List of tuples containing the spans and groupdict\n        returned by the matcher.\n\n    Returns\n    -------\n    List[Span]\n        List of processed spans, with the date parsed.\n    \"\"\"\n\n    for span, groupdict in dates:\n        if span.label_ == \"relative\":\n            parsed = RelativeDate.parse_obj(groupdict)\n        elif span.label_ == \"absolute\":\n            parsed = AbsoluteDate.parse_obj(groupdict)\n        else:\n            parsed = Duration.parse_obj(groupdict)\n\n        span._.date = parsed\n\n    return [span for span, _ in dates]\n</code></pre>"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.process_periods","title":"<code>process_periods(dates)</code>","text":"<p>Experimental period detection.</p> PARAMETER DESCRIPTION <code>dates</code> <p>List of detected dates.</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected periods.</p> Source code in <code>edsnlp/pipelines/misc/dates/dates.py</code> <pre><code>def process_periods(self, dates: List[Span]) -&gt; List[Span]:\n\"\"\"\n    Experimental period detection.\n\n    Parameters\n    ----------\n    dates : List[Span]\n        List of detected dates.\n\n    Returns\n    -------\n    List[Span]\n        List of detected periods.\n    \"\"\"\n\n    if len(dates) &lt; 2:\n        return []\n\n    periods = []\n    seen = set()\n\n    dates = list(sorted(dates, key=lambda d: d.start))\n\n    for d1, d2 in zip(dates[:-1], dates[1:]):\n\n        if d1._.date.mode == Mode.DURATION or d2._.date.mode == Mode.DURATION:\n            pass\n        elif d1 in seen or d1._.date.mode is None or d2._.date.mode is None:\n            continue\n\n        if (\n            d1.end - d2.start &lt; PERIOD_PROXIMITY_THRESHOLD\n            and d1._.date.mode != d2._.date.mode\n        ):\n\n            period = Span(d1.doc, d1.start, d2.end, label=\"period\")\n\n            # If one date is a duration,\n            # the other may not have a registered mode.\n            m1 = d1._.date.mode or Mode.FROM\n            m2 = d2._.date.mode or Mode.FROM\n\n            period._.period = Period.parse_obj(\n                {\n                    m1.value: d1,\n                    m2.value: d2,\n                }\n            )\n\n            seen.add(d1)\n            seen.add(d2)\n\n            periods.append(period)\n\n    return periods\n</code></pre>"},{"location":"reference/pipelines/misc/dates/dates/#edsnlp.pipelines.misc.dates.dates.Dates.__call__","title":"<code>__call__(doc)</code>","text":"<p>Tags dates.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for dates</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/misc/dates/dates.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Tags dates.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy Doc object\n\n    Returns\n    -------\n    doc : Doc\n        spaCy Doc object, annotated for dates\n    \"\"\"\n    dates = self.process(doc)\n    dates = self.parse(dates)\n\n    doc.spans[\"dates\"] = dates\n\n    if self.detect_periods:\n        doc.spans[\"periods\"] = self.process_periods(dates)\n\n    if self.as_ents:\n        ents, discarded = filter_spans(\n            list(doc.ents) + dates, return_discarded=True\n        )\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/misc/dates/factory/","title":"<code>edsnlp.pipelines.misc.dates.factory</code>","text":""},{"location":"reference/pipelines/misc/dates/models/","title":"<code>edsnlp.pipelines.misc.dates.models</code>","text":""},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.BaseDate","title":"<code>BaseDate</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>edsnlp/pipelines/misc/dates/models.py</code> <pre><code>class BaseDate(BaseModel):\n\n    mode: Optional[Mode] = None\n\n    @validator(\"*\", pre=True)\n    def remove_space(cls, v):\n\"\"\"Remove spaces. Useful for coping with ill-formatted PDF extractions.\"\"\"\n        if isinstance(v, str):\n            return v.replace(\" \", \"\")\n        return v\n\n    @root_validator(pre=True)\n    def validate_strings(cls, d: Dict[str, str]) -&gt; Dict[str, str]:\n        result = d.copy()\n\n        for k, v in d.items():\n            if v is not None and \"_\" in k:\n                key, value = k.split(\"_\")\n                result.update({key: value})\n\n        return result\n</code></pre>"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.BaseDate.remove_space","title":"<code>remove_space(v)</code>","text":"<p>Remove spaces. Useful for coping with ill-formatted PDF extractions.</p> Source code in <code>edsnlp/pipelines/misc/dates/models.py</code> <pre><code>@validator(\"*\", pre=True)\ndef remove_space(cls, v):\n\"\"\"Remove spaces. Useful for coping with ill-formatted PDF extractions.\"\"\"\n    if isinstance(v, str):\n        return v.replace(\" \", \"\")\n    return v\n</code></pre>"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative","title":"<code>Relative</code>","text":"<p>         Bases: <code>BaseDate</code></p> Source code in <code>edsnlp/pipelines/misc/dates/models.py</code> <pre><code>class Relative(BaseDate):\n\n    year: Optional[int] = None\n    month: Optional[int] = None\n    week: Optional[int] = None\n    day: Optional[int] = None\n    hour: Optional[int] = None\n    minute: Optional[int] = None\n    second: Optional[int] = None\n\n    @root_validator(pre=True)\n    def parse_unit(cls, d: Dict[str, str]) -&gt; Dict[str, str]:\n\"\"\"\n        Units need to be handled separately.\n\n        This validator modifies the key corresponding to the unit\n        with the detected value\n\n        Parameters\n        ----------\n        d : Dict[str, str]\n            Original data\n\n        Returns\n        -------\n        Dict[str, str]\n            Transformed data\n        \"\"\"\n        unit = d.get(\"unit\")\n\n        if unit:\n            d[unit] = d.get(\"number\")\n\n        return d\n\n    def to_datetime(self, **kwargs) -&gt; pendulum.Duration:\n        d = self.dict(exclude_none=True)\n\n        direction = d.pop(\"direction\", None)\n        dir = -1 if direction == Direction.PAST else 1\n\n        d.pop(\"mode\", None)\n\n        d = {f\"{k}s\": v for k, v in d.items()}\n\n        td = dir * pendulum.duration(**d)\n        return td\n</code></pre>"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.Relative.parse_unit","title":"<code>parse_unit(d)</code>","text":"<p>Units need to be handled separately.</p> <p>This validator modifies the key corresponding to the unit with the detected value</p> PARAMETER DESCRIPTION <code>d</code> <p>Original data</p> <p> TYPE: <code>Dict[str, str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>Transformed data</p> Source code in <code>edsnlp/pipelines/misc/dates/models.py</code> <pre><code>@root_validator(pre=True)\ndef parse_unit(cls, d: Dict[str, str]) -&gt; Dict[str, str]:\n\"\"\"\n    Units need to be handled separately.\n\n    This validator modifies the key corresponding to the unit\n    with the detected value\n\n    Parameters\n    ----------\n    d : Dict[str, str]\n        Original data\n\n    Returns\n    -------\n    Dict[str, str]\n        Transformed data\n    \"\"\"\n    unit = d.get(\"unit\")\n\n    if unit:\n        d[unit] = d.get(\"number\")\n\n    return d\n</code></pre>"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate","title":"<code>RelativeDate</code>","text":"<p>         Bases: <code>Relative</code></p> Source code in <code>edsnlp/pipelines/misc/dates/models.py</code> <pre><code>class RelativeDate(Relative):\n    direction: Direction = Direction.CURRENT\n\n    def to_datetime(\n        self,\n        note_datetime: Optional[datetime] = None,\n        **kwargs,\n    ) -&gt; pendulum.Duration:\n        td = super(RelativeDate, self).to_datetime()\n\n        if note_datetime is not None and not isinstance(note_datetime, NaTType):\n            return note_datetime + td\n\n        return td\n\n    def norm(self) -&gt; str:\n\n        if self.direction == Direction.CURRENT:\n            d = self.dict(exclude_none=True)\n            d.pop(\"direction\", None)\n            d.pop(\"mode\", None)\n\n            key = next(iter(d.keys()), \"day\")\n\n            norm = f\"~0 {key}\"\n        else:\n            td = self.to_datetime()\n            norm = str(td)\n            if td.in_seconds() &gt; 0:\n                norm = f\"+{norm}\"\n\n        return norm\n\n    @root_validator(pre=True)\n    def handle_specifics(cls, d: Dict[str, str]) -&gt; Dict[str, str]:\n\"\"\"\n        Specific patterns such as `aujourd'hui`, `hier`, etc,\n        need to be handled separately.\n\n        Parameters\n        ----------\n        d : Dict[str, str]\n            Original data.\n\n        Returns\n        -------\n        Dict[str, str]\n            Modified data.\n        \"\"\"\n\n        specific = d.get(\"specific\")\n        specific = specific_dict.get(specific)\n\n        if specific:\n            d.update(specific)\n\n        return d\n</code></pre>"},{"location":"reference/pipelines/misc/dates/models/#edsnlp.pipelines.misc.dates.models.RelativeDate.handle_specifics","title":"<code>handle_specifics(d)</code>","text":"<p>Specific patterns such as <code>aujourd'hui</code>, <code>hier</code>, etc, need to be handled separately.</p> PARAMETER DESCRIPTION <code>d</code> <p>Original data.</p> <p> TYPE: <code>Dict[str, str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>Modified data.</p> Source code in <code>edsnlp/pipelines/misc/dates/models.py</code> <pre><code>@root_validator(pre=True)\ndef handle_specifics(cls, d: Dict[str, str]) -&gt; Dict[str, str]:\n\"\"\"\n    Specific patterns such as `aujourd'hui`, `hier`, etc,\n    need to be handled separately.\n\n    Parameters\n    ----------\n    d : Dict[str, str]\n        Original data.\n\n    Returns\n    -------\n    Dict[str, str]\n        Modified data.\n    \"\"\"\n\n    specific = d.get(\"specific\")\n    specific = specific_dict.get(specific)\n\n    if specific:\n        d.update(specific)\n\n    return d\n</code></pre>"},{"location":"reference/pipelines/misc/dates/patterns/","title":"<code>edsnlp.pipelines.misc.dates.patterns</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/absolute/","title":"<code>edsnlp.pipelines.misc.dates.patterns.absolute</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/current/","title":"<code>edsnlp.pipelines.misc.dates.patterns.current</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/duration/","title":"<code>edsnlp.pipelines.misc.dates.patterns.duration</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/false_positive/","title":"<code>edsnlp.pipelines.misc.dates.patterns.false_positive</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/relative/","title":"<code>edsnlp.pipelines.misc.dates.patterns.relative</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/days/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.days</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/delimiters/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.delimiters</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/directions/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.directions</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/modes/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.modes</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/months/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.months</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/numbers/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.numbers</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/time/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.time</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/units/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.units</code>","text":""},{"location":"reference/pipelines/misc/dates/patterns/atomic/years/","title":"<code>edsnlp.pipelines.misc.dates.patterns.atomic.years</code>","text":""},{"location":"reference/pipelines/misc/measurements/","title":"<code>edsnlp.pipelines.misc.measurements</code>","text":""},{"location":"reference/pipelines/misc/measurements/factory/","title":"<code>edsnlp.pipelines.misc.measurements.factory</code>","text":""},{"location":"reference/pipelines/misc/measurements/measurements/","title":"<code>edsnlp.pipelines.misc.measurements.measurements</code>","text":""},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MergeStrategy","title":"<code>MergeStrategy</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>The strategy to use when merging measurements.</p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>class MergeStrategy(str, Enum):\n\"\"\"\n    The strategy to use when merging measurements.\n    \"\"\"\n\n    # Align the new measurement to existing spans\n    align = \"align\"\n\n    # Only extract measurements if they fall within an existing span\n    intersect = \"intersect\"\n\n    # Extract measurements regardless of whether they fall within an existing span\n    union = \"union\"\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement","title":"<code>Measurement</code>","text":"<p>         Bases: <code>abc.ABC</code></p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>class Measurement(abc.ABC):\n    @abc.abstractmethod\n    def __len__(self) -&gt; Iterable[\"SimpleMeasurement\"]:\n\"\"\"\n        Number of items in the measure (only one for SimpleMeasurement)\n\n        Returns\n        -------\n        iterable : Iterable[\"SimpleMeasurement\"]\n\n        Returns\n        -------\n\n        \"\"\"\n\n    @abc.abstractmethod\n    def __iter__(self) -&gt; Iterable[\"SimpleMeasurement\"]:\n\"\"\"\n        Iter over items of the measure (only one for SimpleMeasurement)\n\n        Returns\n        -------\n        iterable : Iterable[\"SimpleMeasurement\"]\n        \"\"\"\n\n    @abc.abstractmethod\n    def __getitem__(self, item) -&gt; \"SimpleMeasurement\":\n\"\"\"\n        Access items of the measure (only one for SimpleMeasurement)\n\n        Parameters\n        ----------\n        item : int\n\n        Returns\n        -------\n        measure : SimpleMeasurement\n        \"\"\"\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Number of items in the measure (only one for SimpleMeasurement)</p> RETURNS DESCRIPTION <code>iterable</code> <p> TYPE: <code>Iterable[SimpleMeasurement]</code> </p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>@abc.abstractmethod\ndef __len__(self) -&gt; Iterable[\"SimpleMeasurement\"]:\n\"\"\"\n    Number of items in the measure (only one for SimpleMeasurement)\n\n    Returns\n    -------\n    iterable : Iterable[\"SimpleMeasurement\"]\n\n    Returns\n    -------\n\n    \"\"\"\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iter over items of the measure (only one for SimpleMeasurement)</p> RETURNS DESCRIPTION <code>iterable</code> <p> TYPE: <code>Iterable[SimpleMeasurement]</code> </p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>@abc.abstractmethod\ndef __iter__(self) -&gt; Iterable[\"SimpleMeasurement\"]:\n\"\"\"\n    Iter over items of the measure (only one for SimpleMeasurement)\n\n    Returns\n    -------\n    iterable : Iterable[\"SimpleMeasurement\"]\n    \"\"\"\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.Measurement.__getitem__","title":"<code>__getitem__(item)</code>  <code>abstractmethod</code>","text":"<p>Access items of the measure (only one for SimpleMeasurement)</p> PARAMETER DESCRIPTION <code>item</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>measure</code> <p> TYPE: <code>SimpleMeasurement</code> </p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>@abc.abstractmethod\ndef __getitem__(self, item) -&gt; \"SimpleMeasurement\":\n\"\"\"\n    Access items of the measure (only one for SimpleMeasurement)\n\n    Parameters\n    ----------\n    item : int\n\n    Returns\n    -------\n    measure : SimpleMeasurement\n    \"\"\"\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement","title":"<code>SimpleMeasurement</code>","text":"<p>         Bases: <code>Measurement</code></p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>class SimpleMeasurement(Measurement):\n    def __init__(self, value, unit, registry):\n\"\"\"\n        The SimpleMeasurement class contains the value and unit\n        for a single non-composite measure\n\n        Parameters\n        ----------\n        value : float\n        unit : str\n        \"\"\"\n        super().__init__()\n        self.value = value\n        self.unit = unit\n        self.registry = registry\n\n    def __iter__(self):\n        return iter((self,))\n\n    def __getitem__(self, item: int):\n        assert isinstance(item, int)\n        return [self][item]\n\n    def __str__(self):\n        return f\"{self.value} {self.unit}\"\n\n    def __len__(self):\n        return 1\n\n    def __repr__(self):\n        return f\"Measurement({self.value}, {repr(self.unit)})\"\n\n    def __eq__(self, other: Any):\n        if isinstance(other, SimpleMeasurement):\n            return self.convert_to(other.unit) == other.value\n        return False\n\n    def __add__(self, other: \"SimpleMeasurement\"):\n        if other.unit == self.unit:\n            return self.__class__(self.value + other.value, self.unit, self.registry)\n        return self.__class__(\n            self.value + other.convert_to(self.unit), self.unit, self.registry\n        )\n\n    def __lt__(self, other: Union[\"SimpleMeasurement\", \"RangeMeasurement\"]):\n        return self.convert_to(other.unit) &lt; min((part.value for part in other))\n\n    def __le__(self, other: Union[\"SimpleMeasurement\", \"RangeMeasurement\"]):\n        return self.convert_to(other.unit) &lt;= min((part.value for part in other))\n\n    def convert_to(self, other_unit):\n        self_degrees, self_scale = self.registry.parse_unit(self.unit)\n        other_degrees, other_scale = self.registry.parse_unit(other_unit)\n\n        if self_degrees != other_degrees:\n            raise AttributeError(\n                f\"Units {self.unit} and {other_unit} are not homogenous\"\n            )\n        ratio = self_scale / other_scale\n        if ratio != 1:\n            return self.value * ratio\n        return self.value\n\n    def __getattr__(self, other_unit):\n        return self.convert_to(other_unit)\n\n    @classmethod\n    def verify(cls, ent):\n        return True\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.SimpleMeasurement.__init__","title":"<code>__init__(value, unit, registry)</code>","text":"<p>The SimpleMeasurement class contains the value and unit for a single non-composite measure</p> PARAMETER DESCRIPTION <code>value</code> <p> TYPE: <code>float</code> </p> <code>unit</code> <p> TYPE: <code>str</code> </p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def __init__(self, value, unit, registry):\n\"\"\"\n    The SimpleMeasurement class contains the value and unit\n    for a single non-composite measure\n\n    Parameters\n    ----------\n    value : float\n    unit : str\n    \"\"\"\n    super().__init__()\n    self.value = value\n    self.unit = unit\n    self.registry = registry\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher","title":"<code>MeasurementsMatcher</code>","text":"Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>class MeasurementsMatcher:\n    def __init__(\n        self,\n        nlp: spacy.Language,\n        measurements: Optional[\n            Union[\n                List[Union[str, MeasureConfig]],\n                Dict[str, MeasureConfig],\n            ]\n        ] = None,\n        units_config: Dict[str, UnitConfig] = patterns.units_config,\n        number_terms: Dict[str, List[str]] = patterns.number_terms,\n        stopwords: List[str] = patterns.stopwords,\n        unit_divisors: List[str] = patterns.unit_divisors,\n        name: str = \"measurements\",\n        ignore_excluded: bool = True,\n        compose_units: bool = True,\n        attr: str = \"NORM\",\n        extract_ranges: bool = False,\n        range_patterns: List[\n            Tuple[Optional[str], Optional[str]]\n        ] = patterns.range_patterns,  # noqa: E501\n        as_ents: bool = False,\n        merge_mode: MergeStrategy = MergeStrategy.union,\n    ):\n\"\"\"\n        Matcher component to extract measurements.\n        A measurements is most often composed of a number and a unit like\n        &gt; 1,26 cm\n        The unit can also be positioned in place of the decimal dot/comma\n        &gt; 1 cm 26\n        Some measurements can be composite\n        &gt; 1,26 cm x 2,34 mm\n        And sometimes they are factorized\n        &gt; Les trois kystes mesurent 1, 2 et 3cm.\n\n        The recognized measurements are stored in the \"measurements\" SpanGroup.\n        Each span has a `Measurement` object stored in the \"value\" extension attribute.\n\n        Parameters\n        ----------\n        nlp : Language\n            The SpaCy object.\n        measurements : Optional[Union[List[Union[str, MeasureConfig]],Dict[str, MeasureConfig]]]\n            A mapping from measure names to MeasureConfig\n            Each measure's configuration has the following shape:\n            {\n                \"unit\": str, # the unit of the measure (like \"kg\"),\n                \"unitless_patterns\": { # optional patterns to handle unitless cases\n                    \"terms\": List[str], # list of preceding terms used to trigger the\n                    measure\n                    # Mapping from ranges to unit to handle cases like\n                    # (\"Taille: 1.2\" -&gt; 1.20 m vs \"Taille: 120\" -&gt; 120cm)\n                    \"ranges\": List[{\n                        \"min\": int,\n                        \"max\": int,\n                        \"unit\": str,\n                    }, {\n                        \"min\": int,\n                        \"unit\": str,\n                    }, ...],\n                }\n        number_terms: Dict[str, List[str]\n            A mapping of numbers to their lexical variants\n        stopwords: List[str]\n            A list of stopwords that do not matter when placed between a unitless\n            trigger\n            and a number\n        unit_divisors: List[str]\n            A list of terms used to divide two units (like: m / s)\n        attr : str\n            Whether to match on the text ('TEXT') or on the normalized text ('NORM')\n        ignore_excluded : bool\n            Whether to exclude pollution patterns when matching in the text\n        compose_units: bool\n            Whether to compose units (like \"m/s\" or \"m.s-1\")\n        extract_ranges: bool\n            Whether to extract ranges (like \"entre 1 et 2 cm\")\n        range_patterns: List[Tuple[str, str]]\n            A list of \"{FROM} xx {TO} yy\" patterns to match range measurements\n        \"\"\"  # noqa E501\n\n        if measurements is None:\n            measurements = [\n                {**m, \"name\": k} for k, m in patterns.common_measurements.items()\n            ]\n        elif isinstance(measurements, (list, tuple)):\n            measurements = [\n                m\n                if isinstance(m, dict)\n                else {**patterns.common_measurements[m], \"name\": m}\n                for m in measurements\n            ]\n        elif isinstance(measurements, dict):\n            measurements = [{\"name\": k, **m} for k, m in measurements.items()]\n\n        self.nlp = nlp\n        self.name = name\n        self.unit_registry = UnitRegistry(units_config)\n        self.regex_matcher = RegexMatcher(\n            attr=attr,\n            ignore_excluded=True,\n        )\n        self.term_matcher = EDSPhraseMatcher(\n            nlp.vocab,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=True,\n        )\n        self.unitless_patterns: Dict[str, UnitlessPatternConfigWithName] = {}\n        self.unit_part_label_hashes: Set[int] = set()\n        self.unitless_label_hashes: Set[int] = set()\n        self.unit_followers: Dict[str, str] = {}\n        self.measure_names: Dict[str, str] = {}\n        self.as_ents = as_ents\n        self.compose_units = compose_units\n        self.extract_ranges = extract_ranges\n        self.range_patterns = range_patterns\n        self.merge_mode = merge_mode\n\n        # NUMBER PATTERNS\n        one_plus = \"[1-9][0-9]*\"\n        self.regex_matcher.add(\n            \"number\",\n            [\n                rf\"(?&lt;![0-9][.,]?){one_plus}([ ]\\d{{3}})*[ ]+(?:[,.][ ]+\\d+)?\",\n                rf\"(?&lt;![0-9][.,]?){one_plus}([ ]\\d{{3}})*(?:[,.]\\d+)?\",\n                rf\"(?&lt;![0-9][.,]?){one_plus}([ ]/[ ]|/){one_plus}\",\n                r\"(?&lt;![0-9][.,]?)00?\",\n            ],\n        )\n        self.number_label_hashes = {nlp.vocab.strings[\"number\"]}\n        for number, terms in number_terms.items():\n            self.term_matcher.build_patterns(nlp, {number: terms})\n            self.number_label_hashes.add(nlp.vocab.strings[number])\n\n        # UNIT PATTERNS\n        for unit_name, unit_config in units_config.items():\n            self.term_matcher.build_patterns(nlp, {unit_name: unit_config[\"terms\"]})\n            if unit_config.get(\"followed_by\") is not None:\n                self.unit_followers[unit_name] = unit_config[\"followed_by\"]\n            self.unit_part_label_hashes.add(nlp.vocab.strings[unit_name])\n\n        self.unit_part_label_hashes.add(nlp.vocab.strings[\"per\"])\n        self.term_matcher.build_patterns(\n            nlp,\n            {\n                \"per\": unit_divisors,\n                \"stopword\": stopwords,\n                \"unitless_stopword\": [\":\"],\n            },\n        )\n\n        # MEASURES\n        for measure_config in measurements:\n            name = measure_config[\"name\"]\n            unit = measure_config[\"unit\"]\n            self.measure_names[self.unit_registry.parse_unit(unit)[0]] = name\n            if \"unitless_patterns\" in measure_config:\n                for pattern in measure_config[\"unitless_patterns\"]:\n                    pattern_name = f\"unitless_{len(self.unitless_patterns)}\"\n                    self.term_matcher.build_patterns(\n                        nlp,\n                        terms={\n                            pattern_name: pattern[\"terms\"],\n                        },\n                    )\n                    self.unitless_label_hashes.add(nlp.vocab.strings[pattern_name])\n                    self.unitless_patterns[pattern_name] = {\"name\": name, **pattern}\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\"\"\"\n        Set extensions for the measurements pipeline.\n        \"\"\"\n\n        if not Span.has_extension(\"value\"):\n            Span.set_extension(\"value\", default=None)\n\n    def extract_units(self, term_matches: Iterable[Span]) -&gt; Iterable[Span]:\n\"\"\"\n        Extracts unit spans from the document by extracting unit atoms (declared in the\n        units_config parameter) and aggregating them automatically\n        Ex: \"il faut 2 g par jour\"\n        =&gt; we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day)\n        =&gt; we aggregate these adjacent matches together to compose a new unit g_per_day\n\n\n        Parameters\n        ----------\n        term_matches: Iterable[Span]\n\n        Returns\n        -------\n        Iterable[Span]\n        \"\"\"\n        last = None\n        units = []\n        current = []\n        unit_label_hashes = set()\n        for unit_part in filter_spans(term_matches):\n            if unit_part.label not in self.unit_part_label_hashes:\n                continue\n            if last is not None and (\n                (\n                    unit_part.doc[last.end : unit_part.start].text.strip() != \"\"\n                    and len(current)\n                )\n                or (\n                    not self.compose_units\n                    and len(current)\n                    and current[-1].label_ != \"per\"\n                )\n            ):\n                doc = current[0].doc\n                # Last non \"per\" match: we don't want our units to be like `g_per`\n                end = next(\n                    (i for i, e in list(enumerate(current))[::-1] if e.label_ != \"per\"),\n                    None,\n                )\n                if end is not None:\n                    unit = \"_\".join(part.label_ for part in current[: end + 1])\n                    units.append(Span(doc, current[0].start, current[end].end, unit))\n                    unit_label_hashes.add(units[-1].label)\n                current = []\n                last = None\n            current.append(unit_part)\n            last = unit_part\n\n        end = next(\n            (i for i, e in list(enumerate(current))[::-1] if e.label_ != \"per\"), None\n        )\n        if end is not None:\n            doc = current[0].doc\n            unit = \"_\".join(part.label_ for part in current[: end + 1])\n            units.append(Span(doc, current[0].start, current[end].end, unit))\n            unit_label_hashes.add(units[-1].label)\n\n        return units\n\n    @classmethod\n    def make_pseudo_sentence(\n        cls,\n        doclike: Union[Doc, Span],\n        matches: List[Tuple[Span, bool]],\n        pseudo_mapping: Dict[int, str],\n    ) -&gt; Tuple[str, List[int]]:\n\"\"\"\n        Creates a pseudo sentence (one letter per entity)\n        to extract higher order patterns\n        Ex: the sentence\n        \"Il font {1}{,} {2} {et} {3} {cm} de long{.}\" is transformed into \"wn,n,nuw.\"\n\n        Parameters\n        ----------\n        doclike: Union[Doc, Span]\n            The document or span to transform\n        matches: List[(Span, bool)]\n            List of tuple of span and whether the span represents a sentence end\n        pseudo_mapping: Dict[int, str]\n            A mapping from label to char in the pseudo sentence\n\n        Returns\n        -------\n        (str, List[int])\n            - the pseudo sentence\n            - a list of offsets to convert match indices into pseudo sent char indices\n        \"\"\"\n        pseudo = []\n        snippet = doclike if isinstance(doclike, Span) else doclike[:]\n        last = snippet.start\n        offsets = []\n        for ent, is_sent_split in matches:\n            if (\n                ent.start != last\n                and not doclike.doc[last : ent.start].text.strip() == \"\"\n            ):\n                pseudo.append(\"w\")\n            offsets.append(len(pseudo))\n            pseudo.append(pseudo_mapping.get(ent.label, \".\" if is_sent_split else \"w\"))\n            last = ent.end\n        if snippet.end != last and doclike.doc[last : snippet.end].text.strip() == \"\":\n            pseudo.append(\"w\")\n        pseudo = \"\".join(pseudo)\n\n        return pseudo, offsets\n\n    def get_matches(self, doc):\n\"\"\"\n        Extract and filter regex and phrase matches in the document\n        to prepare the measurement extraction.\n        Returns the matches and a list of hashes to quickly find unit matches\n\n        Parameters\n        ----------\n        doc: Doc\n\n        Returns\n        -------\n        Tuple[List[(Span, bool)], Set[int]]\n            - List of tuples of spans and whether the spans represents a sentence end\n            - List of hash label to distinguish unit from other matches\n        \"\"\"\n        sent_ends = [doc[i : i + 1] for i in range(len(doc)) if doc[i].is_sent_end]\n\n        regex_matches = list(self.regex_matcher(doc, as_spans=True))\n        term_matches = list(self.term_matcher(doc, as_spans=True))\n\n        # Detect unit parts and compose them into units\n        units = self.extract_units(term_matches)\n        unit_label_hashes = {unit.label for unit in units}\n\n        # Filter matches to prevent matches over dates or doc entities\n        non_unit_terms = [\n            term\n            for term in term_matches\n            if term.label not in self.unit_part_label_hashes\n        ]\n\n        # Filter out measurement-related spans that overlap already matched\n        # entities (in doc.ents or doc.spans[\"dates\"])\n        # Note: we also include sentence ends tokens as 1-token spans in those matches\n        # Prevent from matching over ents that are not measurement related\n        ents = (e for e in doc.ents if e.label_ not in self.measure_names.values())\n        spans__keep__is_sent_end = filter_spans(\n            [\n                # Tuples (span, keep = is measurement related, is sentence end)\n                *zip(get_span_group(doc, \"dates\"), repeat(False), repeat(False)),\n                *zip(regex_matches, repeat(True), repeat(False)),\n                *zip(non_unit_terms, repeat(True), repeat(False)),\n                *zip(units, repeat(True), repeat(False)),\n                *zip(ents, repeat(False), repeat(False)),\n                *zip(sent_ends, repeat(True), repeat(True)),\n            ]\n        )\n\n        # Remove non-measurement related spans (keep = False) and sort the matches\n        matches_and_is_sentence_end: List[(Span, bool)] = sorted(\n            [\n                (span, is_sent_end)\n                for span, keep, is_sent_end in spans__keep__is_sent_end\n                # and remove entities that are not relevant to this pipeline\n                if keep\n            ]\n        )\n\n        return matches_and_is_sentence_end, unit_label_hashes\n\n    def extract_measurements(self, doclike: Doc):\n\"\"\"\n        Extracts measure entities from the document\n\n        Parameters\n        ----------\n        doclike: Doc\n\n        Returns\n        -------\n        List[Span]\n        \"\"\"\n        doc = doclike.doc if isinstance(doclike, Span) else doclike\n        matches, unit_label_hashes = self.get_matches(doclike)\n\n        # Make match slice function to query them\n        def get_matches_after(i):\n            anchor = matches[i][0]\n            for j, (ent, is_sent_end) in enumerate(matches[i + 1 :]):\n                if not is_sent_end and ent.start &gt; anchor.end + AFTER_SNIPPET_LIMIT:\n                    return\n                yield j + i + 1, ent\n\n        def get_matches_before(i):\n            anchor = matches[i][0]\n            for j, (ent, is_sent_end) in enumerate(matches[i::-1]):\n                if not is_sent_end and ent.end &lt; anchor.start - BEFORE_SNIPPET_LIMIT:\n                    return\n                yield i - j, ent\n\n        # Make a pseudo sentence to query higher order patterns in the main loop\n        # `offsets` is a mapping from matches indices (ie match n\u00b0i) to\n        # char indices in the pseudo sentence\n        pseudo, offsets = self.make_pseudo_sentence(\n            doclike,\n            matches,\n            {\n                self.nlp.vocab.strings[\"stopword\"]: \",\",\n                self.nlp.vocab.strings[\"unitless_stopword\"]: \":\",\n                self.nlp.vocab.strings[\"number\"]: \"n\",\n                **{name: \"u\" for name in unit_label_hashes},\n                **{name: \"n\" for name in self.number_label_hashes},\n            },\n        )\n\n        measurements = []\n        matched_unit_indices = set()\n        matched_number_indices = set()\n\n        # Iterate through the number matches\n        for number_idx, (number, is_sent_split) in enumerate(matches):\n            if not is_sent_split and number.label not in self.number_label_hashes:\n                continue\n\n            # Detect the measure value\n            try:\n                if number.label_ == \"number\":\n                    number_text = (\n                        number.text.replace(\" \", \"\")\n                        .replace(\",\", \".\")\n                        .replace(\" \", \"\")\n                        .lstrip(\"0\")\n                    )\n                    value = eval(number_text or \"0\")\n                else:\n                    value = eval(number.label_)\n            except (ValueError, SyntaxError):\n                continue\n\n            unit_idx = unit_text = unit_norm = None\n\n            # Find the closest unit after the number\n            try:\n                unit_idx, unit_text = next(\n                    (j, ent)\n                    for j, ent in get_matches_after(number_idx)\n                    if ent.label in unit_label_hashes\n                )\n                unit_norm = unit_text.label_\n            except (AttributeError, StopIteration):\n                pass\n\n            # Try to pair the number with this next unit if the two are only separated\n            # by numbers and separators alternatively (as in [1][,] [2] [and] [3] cm)\n            try:\n                pseudo_sent = pseudo[offsets[number_idx] + 1 : offsets[unit_idx]]\n                if not re.fullmatch(r\"(,n)*\", pseudo_sent):\n                    unit_text, unit_norm = None, None\n            except TypeError:\n                pass\n\n            # Otherwise, try to infer the unit from the preceding unit to handle cases\n            # like (1 meter 50)\n            if unit_norm is None and number_idx - 1 in matched_unit_indices:\n                try:\n                    unit_before = matches[number_idx - 1][0]\n                    if unit_before.end == number.start:\n                        unit_norm = self.unit_followers[unit_before.label_]\n                except (KeyError, AttributeError, IndexError):\n                    pass\n\n            # If no unit was matched, try to detect unitless patterns before\n            # the number to handle cases like (\"Weight: 63, Height: 170\")\n            if not unit_norm:\n                try:\n                    (unitless_idx, unitless_text) = next(\n                        (j, e)\n                        for j, e in get_matches_before(number_idx)\n                        if e.label in self.unitless_label_hashes\n                    )\n                    unit_norm = None\n                    if re.fullmatch(\n                        r\"[,:n]*\",\n                        pseudo[offsets[unitless_idx] + 1 : offsets[number_idx]],\n                    ):\n                        unitless_pattern = self.unitless_patterns[unitless_text.label_]\n                        unit_norm = next(\n                            scope[\"unit\"]\n                            for scope in unitless_pattern[\"ranges\"]\n                            if (\"min\" not in scope or value &gt;= scope[\"min\"])\n                            and (\"max\" not in scope or value &lt; scope[\"max\"])\n                        )\n                except StopIteration:\n                    pass\n\n            # Otherwise, skip this number\n            if not unit_norm:\n                continue\n\n            # Compute the final entity\n            # TODO: handle this part better without .text.strip(), with cases for\n            #  stopwords, etc\n            if (\n                unit_text\n                and number.start &lt;= unit_text.end\n                and doc[number.end : unit_text.start].text.strip() == \"\"\n            ):\n                ent = doc[number.start : unit_text.end]\n            elif (\n                unit_text\n                and unit_text.start &lt;= number.end\n                and doc[unit_text.end : number.start].text.strip() == \"\"\n            ):\n                ent = doc[unit_text.start : number.end]\n            else:\n                ent = number\n\n            # Compute the dimensionality of the parsed unit\n            try:\n                dims = self.unit_registry.parse_unit(unit_norm)[0]\n            except KeyError:\n                continue\n\n            # If the measure was not requested, dismiss it\n            # Otherwise, relabel the entity and create the value attribute\n            if dims not in self.measure_names:\n                continue\n\n            ent._.value = SimpleMeasurement(value, unit_norm, self.unit_registry)\n            ent.label_ = self.measure_names[dims]\n\n            measurements.append(ent)\n\n            if unit_idx is not None:\n                matched_unit_indices.add(unit_idx)\n            if number_idx is not None:\n                matched_number_indices.add(number_idx)\n\n        unmatched = []\n        for idx, (match, _) in enumerate(matches):\n            if (\n                match.label in unit_label_hashes\n                and idx not in matched_unit_indices\n                or match.label in self.number_label_hashes\n                and idx not in matched_number_indices\n            ):\n                unmatched.append(match)\n\n        return measurements, unmatched\n\n    @classmethod\n    def merge_adjacent_measurements(cls, measurements: List[Span]) -&gt; List[Span]:\n\"\"\"\n        Aggregates extracted measurements together when they are adjacent to handle\n        cases like\n        - 1 meter 50 cm\n        - 30\u00b0 4' 54\"\n\n        Parameters\n        ----------\n        measurements: List[Span]\n\n        Returns\n        -------\n        List[Span]\n        \"\"\"\n        merged = measurements[:1]\n        for ent in measurements[1:]:\n            last = merged[-1]\n\n            if last.end == ent.start and last._.value.unit != ent._.value.unit:\n                try:\n                    new_value = last._.value + ent._.value\n                    merged[-1] = last = last.doc[last.start : ent.end]\n                    last._.value = new_value\n                    last.label_ = ent.label_\n                except (AttributeError, TypeError):\n                    merged.append(ent)\n            else:\n                merged.append(ent)\n\n        return merged\n\n    def merge_measurements_in_ranges(self, measurements: List[Span]) -&gt; List[Span]:\n\"\"\"\n        Aggregates extracted measurements together when they are adjacent to handle\n        cases like\n        - 1 meter 50 cm\n        - 30\u00b0 4' 54\"\n\n        Parameters\n        ----------\n        measurements: List[Span]\n\n        Returns\n        -------\n        List[Span]\n        \"\"\"\n        print(\"RANGES\", self.range_patterns)\n        if not self.extract_ranges or not self.range_patterns:\n            print(\"NOT MERGE RANGES\")\n            return measurements\n\n        merged = measurements[:1]\n        for ent in measurements[1:]:\n            last = merged[-1]\n\n            from_text = last.doc[last.start - 1].norm_ if last.start &gt; 0 else None\n            to_text = get_text(last.doc[last.end : ent.start], \"NORM\", True)\n            matching_patterns = [\n                (a, b)\n                for a, b in self.range_patterns\n                if b == to_text and (a is None or a == from_text)\n            ]\n            if len(matching_patterns):\n                try:\n                    new_value = RangeMeasurement.from_measurements(\n                        last._.value, ent._.value\n                    )\n                    merged[-1] = last = last.doc[\n                        last.start\n                        if matching_patterns[0][0] is None\n                        else last.start - 1 : ent.end\n                    ]\n                    last.label_ = ent.label_\n                    last._.value = new_value\n                except (AttributeError, TypeError):\n                    merged.append(ent)\n            else:\n                merged.append(ent)\n\n        return merged\n\n    def merge_with_existing(\n        self,\n        extracted: List[Span],\n        existing: List[Span],\n    ) -&gt; List[Span]:\n\"\"\"\n        Merges the extracted measurements with the existing measurements in the\n        document.\n\n        Parameters\n        ----------\n        extracted: List[Span]\n            The extracted measurements\n        existing: List[Span]\n            The existing measurements in the document\n\n        Returns\n        -------\n        List[Span]\n        \"\"\"\n        if self.merge_mode == \"align\":\n            spans_measurements = align_spans(extracted, existing, sort_by_overlap=True)\n\n            extracted = []\n            for span, span_measurements in zip(existing, spans_measurements):\n                if len(span_measurements):\n                    span._.value = span_measurements[0]._.value\n                    extracted.append(span)\n\n        elif self.merge_mode == \"intersect\":\n            spans_measurements = align_spans(extracted, existing)\n            extracted = []\n            for span, span_measurements in zip(existing, spans_measurements):\n                extracted.extend(span_measurements)\n            extracted = list(dict.fromkeys(extracted))\n\n        else:\n            extracted = [*extracted, *existing]\n\n        return extracted\n\n    def __call__(self, doc):\n\"\"\"\n        Adds measurements to document's \"measurements\" SpanGroup.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for extracted measurements.\n        \"\"\"\n        ent_labels = set(self.measure_names.values())\n        existing = [\n            ent\n            for ent in (*doc.ents, *doc.spans.get(self.name, ()))\n            if ent.label_ in ent_labels\n        ]\n        other_ents = [ent for ent in doc.ents if ent.label_ not in ent_labels]\n        snippets = (\n            dict.fromkeys(ent.sent for ent in existing)\n            if self.merge_mode in (\"intersect\", \"align\")\n            else [doc]\n        )\n        measurements = [m for s in snippets for m in self.extract_measurements(s)[0]]\n        measurements = self.merge_adjacent_measurements(measurements)\n        measurements = self.merge_measurements_in_ranges(measurements)\n        measurements = self.merge_with_existing(measurements, existing)\n\n        if self.as_ents:\n            doc.ents = filter_spans((*other_ents, *measurements))\n\n        doc.spans[self.name] = dedup(\n            (*measurements, *doc.spans.get(self.name, ())),\n            key=lambda x: (x.start, x.end, x.label_),\n        )\n\n        # for backward compatibility\n        if self.name == \"measurements\":\n            doc.spans[\"measures\"] = doc.spans[\"measurements\"]\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__init__","title":"<code>__init__(nlp, measurements=None, units_config=patterns.units_config, number_terms=patterns.number_terms, stopwords=patterns.stopwords, unit_divisors=patterns.unit_divisors, name='measurements', ignore_excluded=True, compose_units=True, attr='NORM', extract_ranges=False, range_patterns=patterns.range_patterns, as_ents=False, merge_mode=MergeStrategy.union)</code>","text":"<p>Matcher component to extract measurements. A measurements is most often composed of a number and a unit like</p> <p>1,26 cm The unit can also be positioned in place of the decimal dot/comma 1 cm 26 Some measurements can be composite 1,26 cm x 2,34 mm And sometimes they are factorized Les trois kystes mesurent 1, 2 et 3cm.</p> <p>The recognized measurements are stored in the \"measurements\" SpanGroup. Each span has a <code>Measurement</code> object stored in the \"value\" extension attribute.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The SpaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>measurements</code> <p>A mapping from measure names to MeasureConfig Each measure's configuration has the following shape: { \"unit\": str, # the unit of the measure (like \"kg\"), \"unitless_patterns\": { # optional patterns to handle unitless cases \"terms\": List[str], # list of preceding terms used to trigger the measure</p> <code>number_terms</code> <p>A mapping of numbers to their lexical variants</p> <p> TYPE: <code>Dict[str, List[str]]</code> DEFAULT: <code>patterns.number_terms</code> </p> <code>stopwords</code> <p>A list of stopwords that do not matter when placed between a unitless trigger and a number</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.stopwords</code> </p> <code>unit_divisors</code> <p>A list of terms used to divide two units (like: m / s)</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.unit_divisors</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to exclude pollution patterns when matching in the text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>compose_units</code> <p>Whether to compose units (like \"m/s\" or \"m.s-1\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>extract_ranges</code> <p>Whether to extract ranges (like \"entre 1 et 2 cm\")</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>range_patterns</code> <p>A list of \"{FROM} xx {TO} yy\" patterns to match range measurements</p> <p> TYPE: <code>List[Tuple[Optional[str], Optional[str]]]</code> DEFAULT: <code>patterns.range_patterns</code> </p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def __init__(\n    self,\n    nlp: spacy.Language,\n    measurements: Optional[\n        Union[\n            List[Union[str, MeasureConfig]],\n            Dict[str, MeasureConfig],\n        ]\n    ] = None,\n    units_config: Dict[str, UnitConfig] = patterns.units_config,\n    number_terms: Dict[str, List[str]] = patterns.number_terms,\n    stopwords: List[str] = patterns.stopwords,\n    unit_divisors: List[str] = patterns.unit_divisors,\n    name: str = \"measurements\",\n    ignore_excluded: bool = True,\n    compose_units: bool = True,\n    attr: str = \"NORM\",\n    extract_ranges: bool = False,\n    range_patterns: List[\n        Tuple[Optional[str], Optional[str]]\n    ] = patterns.range_patterns,  # noqa: E501\n    as_ents: bool = False,\n    merge_mode: MergeStrategy = MergeStrategy.union,\n):\n\"\"\"\n    Matcher component to extract measurements.\n    A measurements is most often composed of a number and a unit like\n    &gt; 1,26 cm\n    The unit can also be positioned in place of the decimal dot/comma\n    &gt; 1 cm 26\n    Some measurements can be composite\n    &gt; 1,26 cm x 2,34 mm\n    And sometimes they are factorized\n    &gt; Les trois kystes mesurent 1, 2 et 3cm.\n\n    The recognized measurements are stored in the \"measurements\" SpanGroup.\n    Each span has a `Measurement` object stored in the \"value\" extension attribute.\n\n    Parameters\n    ----------\n    nlp : Language\n        The SpaCy object.\n    measurements : Optional[Union[List[Union[str, MeasureConfig]],Dict[str, MeasureConfig]]]\n        A mapping from measure names to MeasureConfig\n        Each measure's configuration has the following shape:\n        {\n            \"unit\": str, # the unit of the measure (like \"kg\"),\n            \"unitless_patterns\": { # optional patterns to handle unitless cases\n                \"terms\": List[str], # list of preceding terms used to trigger the\n                measure\n                # Mapping from ranges to unit to handle cases like\n                # (\"Taille: 1.2\" -&gt; 1.20 m vs \"Taille: 120\" -&gt; 120cm)\n                \"ranges\": List[{\n                    \"min\": int,\n                    \"max\": int,\n                    \"unit\": str,\n                }, {\n                    \"min\": int,\n                    \"unit\": str,\n                }, ...],\n            }\n    number_terms: Dict[str, List[str]\n        A mapping of numbers to their lexical variants\n    stopwords: List[str]\n        A list of stopwords that do not matter when placed between a unitless\n        trigger\n        and a number\n    unit_divisors: List[str]\n        A list of terms used to divide two units (like: m / s)\n    attr : str\n        Whether to match on the text ('TEXT') or on the normalized text ('NORM')\n    ignore_excluded : bool\n        Whether to exclude pollution patterns when matching in the text\n    compose_units: bool\n        Whether to compose units (like \"m/s\" or \"m.s-1\")\n    extract_ranges: bool\n        Whether to extract ranges (like \"entre 1 et 2 cm\")\n    range_patterns: List[Tuple[str, str]]\n        A list of \"{FROM} xx {TO} yy\" patterns to match range measurements\n    \"\"\"  # noqa E501\n\n    if measurements is None:\n        measurements = [\n            {**m, \"name\": k} for k, m in patterns.common_measurements.items()\n        ]\n    elif isinstance(measurements, (list, tuple)):\n        measurements = [\n            m\n            if isinstance(m, dict)\n            else {**patterns.common_measurements[m], \"name\": m}\n            for m in measurements\n        ]\n    elif isinstance(measurements, dict):\n        measurements = [{\"name\": k, **m} for k, m in measurements.items()]\n\n    self.nlp = nlp\n    self.name = name\n    self.unit_registry = UnitRegistry(units_config)\n    self.regex_matcher = RegexMatcher(\n        attr=attr,\n        ignore_excluded=True,\n    )\n    self.term_matcher = EDSPhraseMatcher(\n        nlp.vocab,\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=True,\n    )\n    self.unitless_patterns: Dict[str, UnitlessPatternConfigWithName] = {}\n    self.unit_part_label_hashes: Set[int] = set()\n    self.unitless_label_hashes: Set[int] = set()\n    self.unit_followers: Dict[str, str] = {}\n    self.measure_names: Dict[str, str] = {}\n    self.as_ents = as_ents\n    self.compose_units = compose_units\n    self.extract_ranges = extract_ranges\n    self.range_patterns = range_patterns\n    self.merge_mode = merge_mode\n\n    # NUMBER PATTERNS\n    one_plus = \"[1-9][0-9]*\"\n    self.regex_matcher.add(\n        \"number\",\n        [\n            rf\"(?&lt;![0-9][.,]?){one_plus}([ ]\\d{{3}})*[ ]+(?:[,.][ ]+\\d+)?\",\n            rf\"(?&lt;![0-9][.,]?){one_plus}([ ]\\d{{3}})*(?:[,.]\\d+)?\",\n            rf\"(?&lt;![0-9][.,]?){one_plus}([ ]/[ ]|/){one_plus}\",\n            r\"(?&lt;![0-9][.,]?)00?\",\n        ],\n    )\n    self.number_label_hashes = {nlp.vocab.strings[\"number\"]}\n    for number, terms in number_terms.items():\n        self.term_matcher.build_patterns(nlp, {number: terms})\n        self.number_label_hashes.add(nlp.vocab.strings[number])\n\n    # UNIT PATTERNS\n    for unit_name, unit_config in units_config.items():\n        self.term_matcher.build_patterns(nlp, {unit_name: unit_config[\"terms\"]})\n        if unit_config.get(\"followed_by\") is not None:\n            self.unit_followers[unit_name] = unit_config[\"followed_by\"]\n        self.unit_part_label_hashes.add(nlp.vocab.strings[unit_name])\n\n    self.unit_part_label_hashes.add(nlp.vocab.strings[\"per\"])\n    self.term_matcher.build_patterns(\n        nlp,\n        {\n            \"per\": unit_divisors,\n            \"stopword\": stopwords,\n            \"unitless_stopword\": [\":\"],\n        },\n    )\n\n    # MEASURES\n    for measure_config in measurements:\n        name = measure_config[\"name\"]\n        unit = measure_config[\"unit\"]\n        self.measure_names[self.unit_registry.parse_unit(unit)[0]] = name\n        if \"unitless_patterns\" in measure_config:\n            for pattern in measure_config[\"unitless_patterns\"]:\n                pattern_name = f\"unitless_{len(self.unitless_patterns)}\"\n                self.term_matcher.build_patterns(\n                    nlp,\n                    terms={\n                        pattern_name: pattern[\"terms\"],\n                    },\n                )\n                self.unitless_label_hashes.add(nlp.vocab.strings[pattern_name])\n                self.unitless_patterns[pattern_name] = {\"name\": name, **pattern}\n\n    self.set_extensions()\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__init__--mapping-from-ranges-to-unit-to-handle-cases-like","title":"Mapping from ranges to unit to handle cases like","text":""},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__init__--taille-12-120-m-vs-taille-120-120cm","title":"(\"Taille: 1.2\" -&gt; 1.20 m vs \"Taille: 120\" -&gt; 120cm)","text":"<p>\"ranges\": List[{ \"min\": int, \"max\": int, \"unit\": str, }, { \"min\": int, \"unit\": str, }, ...], }</p> <p> TYPE: <code>Optional[Union[List[Union[str, MeasureConfig]], Dict[str, MeasureConfig]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.set_extensions","title":"<code>set_extensions()</code>  <code>classmethod</code>","text":"<p>Set extensions for the measurements pipeline.</p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>@classmethod\ndef set_extensions(cls) -&gt; None:\n\"\"\"\n    Set extensions for the measurements pipeline.\n    \"\"\"\n\n    if not Span.has_extension(\"value\"):\n        Span.set_extension(\"value\", default=None)\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.extract_units","title":"<code>extract_units(term_matches)</code>","text":"<p>Extracts unit spans from the document by extracting unit atoms (declared in the units_config parameter) and aggregating them automatically Ex: \"il faut 2 g par jour\" =&gt; we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day) =&gt; we aggregate these adjacent matches together to compose a new unit g_per_day</p> PARAMETER DESCRIPTION <code>term_matches</code> <p> TYPE: <code>Iterable[Span]</code> </p> RETURNS DESCRIPTION <code>Iterable[Span]</code> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def extract_units(self, term_matches: Iterable[Span]) -&gt; Iterable[Span]:\n\"\"\"\n    Extracts unit spans from the document by extracting unit atoms (declared in the\n    units_config parameter) and aggregating them automatically\n    Ex: \"il faut 2 g par jour\"\n    =&gt; we extract [g]=unit(g), [par]=divisor(per), [jour]=unit(day)\n    =&gt; we aggregate these adjacent matches together to compose a new unit g_per_day\n\n\n    Parameters\n    ----------\n    term_matches: Iterable[Span]\n\n    Returns\n    -------\n    Iterable[Span]\n    \"\"\"\n    last = None\n    units = []\n    current = []\n    unit_label_hashes = set()\n    for unit_part in filter_spans(term_matches):\n        if unit_part.label not in self.unit_part_label_hashes:\n            continue\n        if last is not None and (\n            (\n                unit_part.doc[last.end : unit_part.start].text.strip() != \"\"\n                and len(current)\n            )\n            or (\n                not self.compose_units\n                and len(current)\n                and current[-1].label_ != \"per\"\n            )\n        ):\n            doc = current[0].doc\n            # Last non \"per\" match: we don't want our units to be like `g_per`\n            end = next(\n                (i for i, e in list(enumerate(current))[::-1] if e.label_ != \"per\"),\n                None,\n            )\n            if end is not None:\n                unit = \"_\".join(part.label_ for part in current[: end + 1])\n                units.append(Span(doc, current[0].start, current[end].end, unit))\n                unit_label_hashes.add(units[-1].label)\n            current = []\n            last = None\n        current.append(unit_part)\n        last = unit_part\n\n    end = next(\n        (i for i, e in list(enumerate(current))[::-1] if e.label_ != \"per\"), None\n    )\n    if end is not None:\n        doc = current[0].doc\n        unit = \"_\".join(part.label_ for part in current[: end + 1])\n        units.append(Span(doc, current[0].start, current[end].end, unit))\n        unit_label_hashes.add(units[-1].label)\n\n    return units\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.make_pseudo_sentence","title":"<code>make_pseudo_sentence(doclike, matches, pseudo_mapping)</code>  <code>classmethod</code>","text":"<p>Creates a pseudo sentence (one letter per entity) to extract higher order patterns Ex: the sentence \"Il font {1}{,} {2} {et} {3} {cm} de long{.}\" is transformed into \"wn,n,nuw.\"</p> PARAMETER DESCRIPTION <code>doclike</code> <p>The document or span to transform</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>matches</code> <p>List of tuple of span and whether the span represents a sentence end</p> <p> TYPE: <code>List[Tuple[Span, bool]]</code> </p> <code>pseudo_mapping</code> <p>A mapping from label to char in the pseudo sentence</p> <p> TYPE: <code>Dict[int, str]</code> </p> RETURNS DESCRIPTION <code>str, List[int]</code> <ul> <li>the pseudo sentence</li> <li>a list of offsets to convert match indices into pseudo sent char indices</li> </ul> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>@classmethod\ndef make_pseudo_sentence(\n    cls,\n    doclike: Union[Doc, Span],\n    matches: List[Tuple[Span, bool]],\n    pseudo_mapping: Dict[int, str],\n) -&gt; Tuple[str, List[int]]:\n\"\"\"\n    Creates a pseudo sentence (one letter per entity)\n    to extract higher order patterns\n    Ex: the sentence\n    \"Il font {1}{,} {2} {et} {3} {cm} de long{.}\" is transformed into \"wn,n,nuw.\"\n\n    Parameters\n    ----------\n    doclike: Union[Doc, Span]\n        The document or span to transform\n    matches: List[(Span, bool)]\n        List of tuple of span and whether the span represents a sentence end\n    pseudo_mapping: Dict[int, str]\n        A mapping from label to char in the pseudo sentence\n\n    Returns\n    -------\n    (str, List[int])\n        - the pseudo sentence\n        - a list of offsets to convert match indices into pseudo sent char indices\n    \"\"\"\n    pseudo = []\n    snippet = doclike if isinstance(doclike, Span) else doclike[:]\n    last = snippet.start\n    offsets = []\n    for ent, is_sent_split in matches:\n        if (\n            ent.start != last\n            and not doclike.doc[last : ent.start].text.strip() == \"\"\n        ):\n            pseudo.append(\"w\")\n        offsets.append(len(pseudo))\n        pseudo.append(pseudo_mapping.get(ent.label, \".\" if is_sent_split else \"w\"))\n        last = ent.end\n    if snippet.end != last and doclike.doc[last : snippet.end].text.strip() == \"\":\n        pseudo.append(\"w\")\n    pseudo = \"\".join(pseudo)\n\n    return pseudo, offsets\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.get_matches","title":"<code>get_matches(doc)</code>","text":"<p>Extract and filter regex and phrase matches in the document to prepare the measurement extraction. Returns the matches and a list of hashes to quickly find unit matches</p> PARAMETER DESCRIPTION <code>doc</code> <p> </p> RETURNS DESCRIPTION <code>Tuple[List[Span, bool], Set[int]]</code> <ul> <li>List of tuples of spans and whether the spans represents a sentence end</li> <li>List of hash label to distinguish unit from other matches</li> </ul> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def get_matches(self, doc):\n\"\"\"\n    Extract and filter regex and phrase matches in the document\n    to prepare the measurement extraction.\n    Returns the matches and a list of hashes to quickly find unit matches\n\n    Parameters\n    ----------\n    doc: Doc\n\n    Returns\n    -------\n    Tuple[List[(Span, bool)], Set[int]]\n        - List of tuples of spans and whether the spans represents a sentence end\n        - List of hash label to distinguish unit from other matches\n    \"\"\"\n    sent_ends = [doc[i : i + 1] for i in range(len(doc)) if doc[i].is_sent_end]\n\n    regex_matches = list(self.regex_matcher(doc, as_spans=True))\n    term_matches = list(self.term_matcher(doc, as_spans=True))\n\n    # Detect unit parts and compose them into units\n    units = self.extract_units(term_matches)\n    unit_label_hashes = {unit.label for unit in units}\n\n    # Filter matches to prevent matches over dates or doc entities\n    non_unit_terms = [\n        term\n        for term in term_matches\n        if term.label not in self.unit_part_label_hashes\n    ]\n\n    # Filter out measurement-related spans that overlap already matched\n    # entities (in doc.ents or doc.spans[\"dates\"])\n    # Note: we also include sentence ends tokens as 1-token spans in those matches\n    # Prevent from matching over ents that are not measurement related\n    ents = (e for e in doc.ents if e.label_ not in self.measure_names.values())\n    spans__keep__is_sent_end = filter_spans(\n        [\n            # Tuples (span, keep = is measurement related, is sentence end)\n            *zip(get_span_group(doc, \"dates\"), repeat(False), repeat(False)),\n            *zip(regex_matches, repeat(True), repeat(False)),\n            *zip(non_unit_terms, repeat(True), repeat(False)),\n            *zip(units, repeat(True), repeat(False)),\n            *zip(ents, repeat(False), repeat(False)),\n            *zip(sent_ends, repeat(True), repeat(True)),\n        ]\n    )\n\n    # Remove non-measurement related spans (keep = False) and sort the matches\n    matches_and_is_sentence_end: List[(Span, bool)] = sorted(\n        [\n            (span, is_sent_end)\n            for span, keep, is_sent_end in spans__keep__is_sent_end\n            # and remove entities that are not relevant to this pipeline\n            if keep\n        ]\n    )\n\n    return matches_and_is_sentence_end, unit_label_hashes\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.extract_measurements","title":"<code>extract_measurements(doclike)</code>","text":"<p>Extracts measure entities from the document</p> PARAMETER DESCRIPTION <code>doclike</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def extract_measurements(self, doclike: Doc):\n\"\"\"\n    Extracts measure entities from the document\n\n    Parameters\n    ----------\n    doclike: Doc\n\n    Returns\n    -------\n    List[Span]\n    \"\"\"\n    doc = doclike.doc if isinstance(doclike, Span) else doclike\n    matches, unit_label_hashes = self.get_matches(doclike)\n\n    # Make match slice function to query them\n    def get_matches_after(i):\n        anchor = matches[i][0]\n        for j, (ent, is_sent_end) in enumerate(matches[i + 1 :]):\n            if not is_sent_end and ent.start &gt; anchor.end + AFTER_SNIPPET_LIMIT:\n                return\n            yield j + i + 1, ent\n\n    def get_matches_before(i):\n        anchor = matches[i][0]\n        for j, (ent, is_sent_end) in enumerate(matches[i::-1]):\n            if not is_sent_end and ent.end &lt; anchor.start - BEFORE_SNIPPET_LIMIT:\n                return\n            yield i - j, ent\n\n    # Make a pseudo sentence to query higher order patterns in the main loop\n    # `offsets` is a mapping from matches indices (ie match n\u00b0i) to\n    # char indices in the pseudo sentence\n    pseudo, offsets = self.make_pseudo_sentence(\n        doclike,\n        matches,\n        {\n            self.nlp.vocab.strings[\"stopword\"]: \",\",\n            self.nlp.vocab.strings[\"unitless_stopword\"]: \":\",\n            self.nlp.vocab.strings[\"number\"]: \"n\",\n            **{name: \"u\" for name in unit_label_hashes},\n            **{name: \"n\" for name in self.number_label_hashes},\n        },\n    )\n\n    measurements = []\n    matched_unit_indices = set()\n    matched_number_indices = set()\n\n    # Iterate through the number matches\n    for number_idx, (number, is_sent_split) in enumerate(matches):\n        if not is_sent_split and number.label not in self.number_label_hashes:\n            continue\n\n        # Detect the measure value\n        try:\n            if number.label_ == \"number\":\n                number_text = (\n                    number.text.replace(\" \", \"\")\n                    .replace(\",\", \".\")\n                    .replace(\" \", \"\")\n                    .lstrip(\"0\")\n                )\n                value = eval(number_text or \"0\")\n            else:\n                value = eval(number.label_)\n        except (ValueError, SyntaxError):\n            continue\n\n        unit_idx = unit_text = unit_norm = None\n\n        # Find the closest unit after the number\n        try:\n            unit_idx, unit_text = next(\n                (j, ent)\n                for j, ent in get_matches_after(number_idx)\n                if ent.label in unit_label_hashes\n            )\n            unit_norm = unit_text.label_\n        except (AttributeError, StopIteration):\n            pass\n\n        # Try to pair the number with this next unit if the two are only separated\n        # by numbers and separators alternatively (as in [1][,] [2] [and] [3] cm)\n        try:\n            pseudo_sent = pseudo[offsets[number_idx] + 1 : offsets[unit_idx]]\n            if not re.fullmatch(r\"(,n)*\", pseudo_sent):\n                unit_text, unit_norm = None, None\n        except TypeError:\n            pass\n\n        # Otherwise, try to infer the unit from the preceding unit to handle cases\n        # like (1 meter 50)\n        if unit_norm is None and number_idx - 1 in matched_unit_indices:\n            try:\n                unit_before = matches[number_idx - 1][0]\n                if unit_before.end == number.start:\n                    unit_norm = self.unit_followers[unit_before.label_]\n            except (KeyError, AttributeError, IndexError):\n                pass\n\n        # If no unit was matched, try to detect unitless patterns before\n        # the number to handle cases like (\"Weight: 63, Height: 170\")\n        if not unit_norm:\n            try:\n                (unitless_idx, unitless_text) = next(\n                    (j, e)\n                    for j, e in get_matches_before(number_idx)\n                    if e.label in self.unitless_label_hashes\n                )\n                unit_norm = None\n                if re.fullmatch(\n                    r\"[,:n]*\",\n                    pseudo[offsets[unitless_idx] + 1 : offsets[number_idx]],\n                ):\n                    unitless_pattern = self.unitless_patterns[unitless_text.label_]\n                    unit_norm = next(\n                        scope[\"unit\"]\n                        for scope in unitless_pattern[\"ranges\"]\n                        if (\"min\" not in scope or value &gt;= scope[\"min\"])\n                        and (\"max\" not in scope or value &lt; scope[\"max\"])\n                    )\n            except StopIteration:\n                pass\n\n        # Otherwise, skip this number\n        if not unit_norm:\n            continue\n\n        # Compute the final entity\n        # TODO: handle this part better without .text.strip(), with cases for\n        #  stopwords, etc\n        if (\n            unit_text\n            and number.start &lt;= unit_text.end\n            and doc[number.end : unit_text.start].text.strip() == \"\"\n        ):\n            ent = doc[number.start : unit_text.end]\n        elif (\n            unit_text\n            and unit_text.start &lt;= number.end\n            and doc[unit_text.end : number.start].text.strip() == \"\"\n        ):\n            ent = doc[unit_text.start : number.end]\n        else:\n            ent = number\n\n        # Compute the dimensionality of the parsed unit\n        try:\n            dims = self.unit_registry.parse_unit(unit_norm)[0]\n        except KeyError:\n            continue\n\n        # If the measure was not requested, dismiss it\n        # Otherwise, relabel the entity and create the value attribute\n        if dims not in self.measure_names:\n            continue\n\n        ent._.value = SimpleMeasurement(value, unit_norm, self.unit_registry)\n        ent.label_ = self.measure_names[dims]\n\n        measurements.append(ent)\n\n        if unit_idx is not None:\n            matched_unit_indices.add(unit_idx)\n        if number_idx is not None:\n            matched_number_indices.add(number_idx)\n\n    unmatched = []\n    for idx, (match, _) in enumerate(matches):\n        if (\n            match.label in unit_label_hashes\n            and idx not in matched_unit_indices\n            or match.label in self.number_label_hashes\n            and idx not in matched_number_indices\n        ):\n            unmatched.append(match)\n\n    return measurements, unmatched\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.merge_adjacent_measurements","title":"<code>merge_adjacent_measurements(measurements)</code>  <code>classmethod</code>","text":"<p>Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\"</p> PARAMETER DESCRIPTION <code>measurements</code> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>@classmethod\ndef merge_adjacent_measurements(cls, measurements: List[Span]) -&gt; List[Span]:\n\"\"\"\n    Aggregates extracted measurements together when they are adjacent to handle\n    cases like\n    - 1 meter 50 cm\n    - 30\u00b0 4' 54\"\n\n    Parameters\n    ----------\n    measurements: List[Span]\n\n    Returns\n    -------\n    List[Span]\n    \"\"\"\n    merged = measurements[:1]\n    for ent in measurements[1:]:\n        last = merged[-1]\n\n        if last.end == ent.start and last._.value.unit != ent._.value.unit:\n            try:\n                new_value = last._.value + ent._.value\n                merged[-1] = last = last.doc[last.start : ent.end]\n                last._.value = new_value\n                last.label_ = ent.label_\n            except (AttributeError, TypeError):\n                merged.append(ent)\n        else:\n            merged.append(ent)\n\n    return merged\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.merge_measurements_in_ranges","title":"<code>merge_measurements_in_ranges(measurements)</code>","text":"<p>Aggregates extracted measurements together when they are adjacent to handle cases like - 1 meter 50 cm - 30\u00b0 4' 54\"</p> PARAMETER DESCRIPTION <code>measurements</code> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def merge_measurements_in_ranges(self, measurements: List[Span]) -&gt; List[Span]:\n\"\"\"\n    Aggregates extracted measurements together when they are adjacent to handle\n    cases like\n    - 1 meter 50 cm\n    - 30\u00b0 4' 54\"\n\n    Parameters\n    ----------\n    measurements: List[Span]\n\n    Returns\n    -------\n    List[Span]\n    \"\"\"\n    print(\"RANGES\", self.range_patterns)\n    if not self.extract_ranges or not self.range_patterns:\n        print(\"NOT MERGE RANGES\")\n        return measurements\n\n    merged = measurements[:1]\n    for ent in measurements[1:]:\n        last = merged[-1]\n\n        from_text = last.doc[last.start - 1].norm_ if last.start &gt; 0 else None\n        to_text = get_text(last.doc[last.end : ent.start], \"NORM\", True)\n        matching_patterns = [\n            (a, b)\n            for a, b in self.range_patterns\n            if b == to_text and (a is None or a == from_text)\n        ]\n        if len(matching_patterns):\n            try:\n                new_value = RangeMeasurement.from_measurements(\n                    last._.value, ent._.value\n                )\n                merged[-1] = last = last.doc[\n                    last.start\n                    if matching_patterns[0][0] is None\n                    else last.start - 1 : ent.end\n                ]\n                last.label_ = ent.label_\n                last._.value = new_value\n            except (AttributeError, TypeError):\n                merged.append(ent)\n        else:\n            merged.append(ent)\n\n    return merged\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.merge_with_existing","title":"<code>merge_with_existing(extracted, existing)</code>","text":"<p>Merges the extracted measurements with the existing measurements in the document.</p> PARAMETER DESCRIPTION <code>extracted</code> <p>The extracted measurements</p> <p> TYPE: <code>List[Span]</code> </p> <code>existing</code> <p>The existing measurements in the document</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def merge_with_existing(\n    self,\n    extracted: List[Span],\n    existing: List[Span],\n) -&gt; List[Span]:\n\"\"\"\n    Merges the extracted measurements with the existing measurements in the\n    document.\n\n    Parameters\n    ----------\n    extracted: List[Span]\n        The extracted measurements\n    existing: List[Span]\n        The existing measurements in the document\n\n    Returns\n    -------\n    List[Span]\n    \"\"\"\n    if self.merge_mode == \"align\":\n        spans_measurements = align_spans(extracted, existing, sort_by_overlap=True)\n\n        extracted = []\n        for span, span_measurements in zip(existing, spans_measurements):\n            if len(span_measurements):\n                span._.value = span_measurements[0]._.value\n                extracted.append(span)\n\n    elif self.merge_mode == \"intersect\":\n        spans_measurements = align_spans(extracted, existing)\n        extracted = []\n        for span, span_measurements in zip(existing, spans_measurements):\n            extracted.extend(span_measurements)\n        extracted = list(dict.fromkeys(extracted))\n\n    else:\n        extracted = [*extracted, *existing]\n\n    return extracted\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/measurements/#edsnlp.pipelines.misc.measurements.measurements.MeasurementsMatcher.__call__","title":"<code>__call__(doc)</code>","text":"<p>Adds measurements to document's \"measurements\" SpanGroup.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted measurements.</p> Source code in <code>edsnlp/pipelines/misc/measurements/measurements.py</code> <pre><code>def __call__(self, doc):\n\"\"\"\n    Adds measurements to document's \"measurements\" SpanGroup.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for extracted measurements.\n    \"\"\"\n    ent_labels = set(self.measure_names.values())\n    existing = [\n        ent\n        for ent in (*doc.ents, *doc.spans.get(self.name, ()))\n        if ent.label_ in ent_labels\n    ]\n    other_ents = [ent for ent in doc.ents if ent.label_ not in ent_labels]\n    snippets = (\n        dict.fromkeys(ent.sent for ent in existing)\n        if self.merge_mode in (\"intersect\", \"align\")\n        else [doc]\n    )\n    measurements = [m for s in snippets for m in self.extract_measurements(s)[0]]\n    measurements = self.merge_adjacent_measurements(measurements)\n    measurements = self.merge_measurements_in_ranges(measurements)\n    measurements = self.merge_with_existing(measurements, existing)\n\n    if self.as_ents:\n        doc.ents = filter_spans((*other_ents, *measurements))\n\n    doc.spans[self.name] = dedup(\n        (*measurements, *doc.spans.get(self.name, ())),\n        key=lambda x: (x.start, x.end, x.label_),\n    )\n\n    # for backward compatibility\n    if self.name == \"measurements\":\n        doc.spans[\"measures\"] = doc.spans[\"measurements\"]\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/misc/measurements/patterns/","title":"<code>edsnlp.pipelines.misc.measurements.patterns</code>","text":""},{"location":"reference/pipelines/misc/reason/","title":"<code>edsnlp.pipelines.misc.reason</code>","text":""},{"location":"reference/pipelines/misc/reason/factory/","title":"<code>edsnlp.pipelines.misc.reason.factory</code>","text":""},{"location":"reference/pipelines/misc/reason/patterns/","title":"<code>edsnlp.pipelines.misc.reason.patterns</code>","text":""},{"location":"reference/pipelines/misc/reason/reason/","title":"<code>edsnlp.pipelines.misc.reason.reason</code>","text":""},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason","title":"<code>Reason</code>","text":"<p>         Bases: <code>GenericMatcher</code></p> <p>Pipeline to identify the reason of the hospitalisation.</p> <p>It declares a Span extension called <code>ents_reason</code> and adds the key <code>reasons</code> to doc.spans.</p> <p>It also declares the boolean extension <code>is_reason</code>. This extension is set to True for the Reason Spans but also for the entities that overlap the reason span.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>reasons</code> <p>The terminology of reasons.</p> <p> TYPE: <code>Optional[Dict[str, Union[List[str], str]]]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>use_sections</code> <p>whether or not use the <code>sections</code> pipeline to improve results.</p> <p> TYPE: <code>bool</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/misc/reason/reason.py</code> <pre><code>class Reason(GenericMatcher):\n\"\"\"Pipeline to identify the reason of the hospitalisation.\n\n    It declares a Span extension called `ents_reason` and adds\n    the key `reasons` to doc.spans.\n\n    It also declares the boolean extension `is_reason`.\n    This extension is set to True for the Reason Spans but also\n    for the entities that overlap the reason span.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    reasons : Optional[Dict[str, Union[List[str], str]]]\n        The terminology of reasons.\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\", or a dict with\n        the key 'term_attr'. We can also add a key for each regex.\n    use_sections : bool,\n        whether or not use the `sections` pipeline to improve results.\n    ignore_excluded : bool\n        Whether to skip excluded tokens.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        reasons: Optional[Dict[str, Union[List[str], str]]],\n        attr: Union[Dict[str, str], str],\n        use_sections: bool,\n        ignore_excluded: bool,\n    ):\n\n        if reasons is None:\n            reasons = patterns.reasons\n\n        super().__init__(\n            nlp,\n            terms=None,\n            regex=reasons,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n        )\n\n        self.use_sections = use_sections and (\n            \"eds.sections\" in self.nlp.pipe_names or \"sections\" in self.nlp.pipe_names\n        )\n        if use_sections and not self.use_sections:\n            logger.warning(\n                \"You have requested that the pipeline use annotations \"\n                \"provided by the `eds.section` pipeline, but it was not set. \"\n                \"Skipping that step.\"\n            )\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\n        if not Span.has_extension(\"ents_reason\"):\n            Span.set_extension(\"ents_reason\", default=None)\n\n        if not Span.has_extension(\"is_reason\"):\n            Span.set_extension(\"is_reason\", default=False)\n\n    def _enhance_with_sections(self, sections: Iterable, reasons: Iterable) -&gt; List:\n\"\"\"Enhance the list of reasons with the section information.\n        If the reason overlaps with history, so it will be removed from the list\n\n        Parameters\n        ----------\n        sections : Iterable\n            Spans of sections identified with the `sections` pipeline\n        reasons : Iterable\n            Reasons list identified by the regex\n\n        Returns\n        -------\n        List\n            Updated list of spans reasons\n        \"\"\"\n\n        for section in sections:\n            if section.label_ in patterns.sections_reason:\n                reasons.append(section)\n\n            if section.label_ in patterns.section_exclude:\n                for reason in reasons:\n                    if check_inclusion(reason, section.start, section.end):\n                        reasons.remove(reason)\n\n        return reasons\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"Find spans related to the reasons of the hospitalisation\n\n        Parameters\n        ----------\n        doc : Doc\n\n        Returns\n        -------\n        Doc\n        \"\"\"\n        matches = self.process(doc)\n        reasons = get_spans(matches, \"reasons\")\n\n        if self.use_sections:\n            sections = doc.spans[\"sections\"]\n            reasons = self._enhance_with_sections(sections=sections, reasons=reasons)\n\n        doc.spans[\"reasons\"] = reasons\n\n        # Entities\n        if len(doc.ents) &gt; 0:\n            for reason in reasons:  # TODO optimize this iteration\n                ent_list = []\n                for ent in doc.ents:\n                    if check_inclusion(ent, reason.start, reason.end):\n                        ent_list.append(ent)\n                        ent._.is_reason = True\n\n                reason._.ents_reason = ent_list\n                reason._.is_reason = True\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/misc/reason/reason/#edsnlp.pipelines.misc.reason.reason.Reason.__call__","title":"<code>__call__(doc)</code>","text":"<p>Find spans related to the reasons of the hospitalisation</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> Source code in <code>edsnlp/pipelines/misc/reason/reason.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"Find spans related to the reasons of the hospitalisation\n\n    Parameters\n    ----------\n    doc : Doc\n\n    Returns\n    -------\n    Doc\n    \"\"\"\n    matches = self.process(doc)\n    reasons = get_spans(matches, \"reasons\")\n\n    if self.use_sections:\n        sections = doc.spans[\"sections\"]\n        reasons = self._enhance_with_sections(sections=sections, reasons=reasons)\n\n    doc.spans[\"reasons\"] = reasons\n\n    # Entities\n    if len(doc.ents) &gt; 0:\n        for reason in reasons:  # TODO optimize this iteration\n            ent_list = []\n            for ent in doc.ents:\n                if check_inclusion(ent, reason.start, reason.end):\n                    ent_list.append(ent)\n                    ent._.is_reason = True\n\n            reason._.ents_reason = ent_list\n            reason._.is_reason = True\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/misc/sections/","title":"<code>edsnlp.pipelines.misc.sections</code>","text":""},{"location":"reference/pipelines/misc/sections/factory/","title":"<code>edsnlp.pipelines.misc.sections.factory</code>","text":""},{"location":"reference/pipelines/misc/sections/patterns/","title":"<code>edsnlp.pipelines.misc.sections.patterns</code>","text":"<p>These section titles were extracted from a work performed by Ivan Lerner at AP-HP. It supplied a number of documents annotated for section titles.</p> <p>The section titles were reviewed by Gilles Chatellier, who gave meaningful insights.</p> <p>See sections/section-dataset notebook for detail.</p>"},{"location":"reference/pipelines/misc/sections/sections/","title":"<code>edsnlp.pipelines.misc.sections.sections</code>","text":""},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections","title":"<code>Sections</code>","text":"<p>         Bases: <code>GenericMatcher</code></p> <p>Divides the document into sections.</p> <p>By default, we are using a dataset of documents annotated for section titles, using the work done by Ivan Lerner, reviewed by Gilles Chatellier.</p> <p>Detected sections are :</p> <ul> <li>allergies ;</li> <li>ant\u00e9c\u00e9dents ;</li> <li>ant\u00e9c\u00e9dents familiaux ;</li> <li>traitements entr\u00e9e ;</li> <li>conclusion ;</li> <li>conclusion entr\u00e9e ;</li> <li>habitus ;</li> <li>correspondants ;</li> <li>diagnostic ;</li> <li>donn\u00e9es biom\u00e9triques entr\u00e9e ;</li> <li>examens ;</li> <li>examens compl\u00e9mentaires ;</li> <li>facteurs de risques ;</li> <li>histoire de la maladie ;</li> <li>actes ;</li> <li>motif ;</li> <li>prescriptions ;</li> <li>traitements sortie.</li> </ul> <p>The component looks for section titles within the document, and stores them in the <code>section_title</code> extension.</p> <p>For ease-of-use, the component also populates a <code>section</code> extension, which contains a list of spans corresponding to the \"sections\" of the document. These span from the start of one section title to the next, which can introduce obvious bias should an intermediate section title goes undetected.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy pipeline object.</p> <p> TYPE: <code>Language</code> </p> <code>sections</code> <p>Dictionary of terms to look for.</p> <p> TYPE: <code>Dict[str, List[str]]</code> </p> <code>attr</code> <p>Default attribute to match on.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/misc/sections/sections.py</code> <pre><code>class Sections(GenericMatcher):\n\"\"\"\n    Divides the document into sections.\n\n    By default, we are using a dataset of documents annotated for section titles,\n    using the work done by Ivan Lerner, reviewed by Gilles Chatellier.\n\n    Detected sections are :\n\n    - allergies ;\n    - ant\u00e9c\u00e9dents ;\n    - ant\u00e9c\u00e9dents familiaux ;\n    - traitements entr\u00e9e ;\n    - conclusion ;\n    - conclusion entr\u00e9e ;\n    - habitus ;\n    - correspondants ;\n    - diagnostic ;\n    - donn\u00e9es biom\u00e9triques entr\u00e9e ;\n    - examens ;\n    - examens compl\u00e9mentaires ;\n    - facteurs de risques ;\n    - histoire de la maladie ;\n    - actes ;\n    - motif ;\n    - prescriptions ;\n    - traitements sortie.\n\n    The component looks for section titles within the document,\n    and stores them in the `section_title` extension.\n\n    For ease-of-use, the component also populates a `section` extension,\n    which contains a list of spans corresponding to the \"sections\" of the\n    document. These span from the start of one section title to the next,\n    which can introduce obvious bias should an intermediate section title\n    goes undetected.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy pipeline object.\n    sections : Dict[str, List[str]]\n        Dictionary of terms to look for.\n    attr : str\n        Default attribute to match on.\n    ignore_excluded : bool\n        Whether to skip excluded tokens.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        sections: Dict[str, List[str]],\n        add_patterns: bool,\n        attr: str,\n        ignore_excluded: bool,\n    ):\n\n        logger.warning(\n            \"The component Sections is still in Beta. Use at your own risks.\"\n        )\n\n        if sections is None:\n            sections = patterns.sections\n        sections = dict(sections)\n\n        self.add_patterns = add_patterns\n        if add_patterns:\n\n            for k, v in sections.items():\n\n                sections[k] = [\n                    r\"(?&lt;=(?:\\n|^)[^\\n]{0,5})\" + ent + r\"(?=[^\\n]{0,5}\\n)\" for ent in v\n                ]\n\n        super().__init__(\n            nlp,\n            terms=None,\n            regex=sections,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n        )\n\n        self.set_extensions()\n\n        if not nlp.has_pipe(\"normalizer\") and not nlp.has_pipe(\"eds.normalizer\"):\n            logger.warning(\"You should add pipe `eds.normalizer`\")\n\n    @classmethod\n    def set_extensions(cls):\n\n        if not Span.has_extension(\"section_title\"):\n            Span.set_extension(\"section_title\", default=None)\n\n        if not Span.has_extension(\"section\"):\n            Span.set_extension(\"section\", default=None)\n\n    @classmethod\n    def tag_ents(cls, sections: List[Span]):\n        for section in sections:\n            for e in section.ents:\n                e._.section = section.label_\n\n    # noinspection PyProtectedMember\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Divides the doc into sections\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for sections\n        \"\"\"\n        titles = filter_spans(self.process(doc))\n\n        sections = []\n\n        for t1, t2 in zip(titles[:-1], titles[1:]):\n            section = Span(doc, t1.start, t2.start, label=t1.label)\n            section._.section_title = t1\n            sections.append(section)\n\n        if titles:\n            t = titles[-1]\n            section = Span(doc, t.start, len(doc), label=t.label)\n            section._.section_title = t\n            sections.append(section)\n\n        doc.spans[\"sections\"] = sections\n        doc.spans[\"section_titles\"] = titles\n\n        self.tag_ents(sections)\n        return doc\n</code></pre>"},{"location":"reference/pipelines/misc/sections/sections/#edsnlp.pipelines.misc.sections.sections.Sections.__call__","title":"<code>__call__(doc)</code>","text":"<p>Divides the doc into sections</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for sections</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/misc/sections/sections.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Divides the doc into sections\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for sections\n    \"\"\"\n    titles = filter_spans(self.process(doc))\n\n    sections = []\n\n    for t1, t2 in zip(titles[:-1], titles[1:]):\n        section = Span(doc, t1.start, t2.start, label=t1.label)\n        section._.section_title = t1\n        sections.append(section)\n\n    if titles:\n        t = titles[-1]\n        section = Span(doc, t.start, len(doc), label=t.label)\n        section._.section_title = t\n        sections.append(section)\n\n    doc.spans[\"sections\"] = sections\n    doc.spans[\"section_titles\"] = titles\n\n    self.tag_ents(sections)\n    return doc\n</code></pre>"},{"location":"reference/pipelines/misc/tables/","title":"<code>edsnlp.pipelines.misc.tables</code>","text":""},{"location":"reference/pipelines/misc/tables/factory/","title":"<code>edsnlp.pipelines.misc.tables.factory</code>","text":""},{"location":"reference/pipelines/misc/tables/patterns/","title":"<code>edsnlp.pipelines.misc.tables.patterns</code>","text":""},{"location":"reference/pipelines/misc/tables/tables/","title":"<code>edsnlp.pipelines.misc.tables.tables</code>","text":""},{"location":"reference/pipelines/misc/tables/tables/#edsnlp.pipelines.misc.tables.tables.TablesMatcher","title":"<code>TablesMatcher</code>","text":"<p>         Bases: <code>GenericMatcher</code></p> <p>Pipeline to identify the Tables.</p> <p>It adds the key <code>tables</code> to doc.spans.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>tables_pattern</code> <p>The regex pattern to identify tables. The key of dictionary should be <code>tables</code></p> <p> TYPE: <code>Optional[Dict[str, str]]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'. We can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/misc/tables/tables.py</code> <pre><code>class TablesMatcher(GenericMatcher):\n\"\"\"Pipeline to identify the Tables.\n\n    It adds the key `tables` to doc.spans.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    tables_pattern : Optional[Dict[str, str]]\n        The regex pattern to identify tables.\n        The key of dictionary should be `tables`\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\", or a dict with\n        the key 'term_attr'. We can also add a key for each regex.\n    ignore_excluded : bool\n        Whether to skip excluded tokens.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        tables_pattern: Optional[Dict[str, str]],\n        sep_pattern: Optional[str],\n        attr: Union[Dict[str, str], str],\n        ignore_excluded: bool,\n    ):\n\n        if tables_pattern is None:\n            self.tables_pattern = patterns.regex\n        else:\n            self.tables_pattern = tables_pattern\n\n        if sep_pattern is None:\n            self.sep = patterns.sep\n        else:\n            self.sep = sep_pattern\n\n        super().__init__(\n            nlp,\n            terms=None,\n            regex=self.tables_pattern,\n            attr=attr,\n            ignore_excluded=ignore_excluded,\n        )\n\n        if not Span.has_extension(\"to_pd_table\"):\n            Span.set_extension(\"to_pd_table\", method=self.to_pd_table)\n\n        self.set_extensions()\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"Find spans that contain tables\n\n        Parameters\n        ----------\n        doc : Doc\n\n        Returns\n        -------\n        Doc\n        \"\"\"\n        matches = self.process(doc)\n        tables = get_spans(matches, \"tables\")\n        # parsed = self.parse(tables=tables)\n\n        doc.spans[\"tables\"] = tables\n\n        return doc\n\n    def to_pd_table(self, span) -&gt; pd.DataFrame:\n        table_str_io = StringIO(span.text)\n        parsed = pd.read_csv(\n            table_str_io,\n            sep=self.sep,\n            engine=\"python\",\n            header=None,\n            on_bad_lines=\"skip\",\n        )\n        return parsed\n</code></pre>"},{"location":"reference/pipelines/misc/tables/tables/#edsnlp.pipelines.misc.tables.tables.TablesMatcher.__call__","title":"<code>__call__(doc)</code>","text":"<p>Find spans that contain tables</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>Doc</code> Source code in <code>edsnlp/pipelines/misc/tables/tables.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"Find spans that contain tables\n\n    Parameters\n    ----------\n    doc : Doc\n\n    Returns\n    -------\n    Doc\n    \"\"\"\n    matches = self.process(doc)\n    tables = get_spans(matches, \"tables\")\n    # parsed = self.parse(tables=tables)\n\n    doc.spans[\"tables\"] = tables\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/ner/","title":"<code>edsnlp.pipelines.ner</code>","text":""},{"location":"reference/pipelines/ner/adicap/","title":"<code>edsnlp.pipelines.ner.adicap</code>","text":""},{"location":"reference/pipelines/ner/adicap/adicap/","title":"<code>edsnlp.pipelines.ner.adicap.adicap</code>","text":"<p><code>eds.adicap</code> pipeline</p>"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap","title":"<code>Adicap</code>","text":"<p>         Bases: <code>ContextualMatcher</code></p> Source code in <code>edsnlp/pipelines/ner/adicap/adicap.py</code> <pre><code>class Adicap(ContextualMatcher):\n    def __init__(self, nlp, pattern, attr, prefix, window):\n\n        self.nlp = nlp\n        if pattern is None:\n            pattern = patterns.base_code\n\n        if prefix is None:\n            prefix = patterns.adicap_prefix\n\n        adicap_pattern = dict(\n            source=\"adicap\",\n            regex=prefix,\n            regex_attr=attr,\n            assign=[\n                dict(\n                    name=\"code\",\n                    regex=pattern,\n                    window=window,\n                    replace_entity=True,\n                    reduce_mode=None,\n                ),\n            ],\n        )\n\n        super().__init__(\n            nlp=nlp,\n            name=\"adicap\",\n            attr=attr,\n            patterns=adicap_pattern,\n            ignore_excluded=False,\n            regex_flags=0,\n            alignment_mode=\"expand\",\n            include_assigned=False,\n            assign_as_span=False,\n        )\n\n        self.decode_dict = get_adicap_dict()\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        super().set_extensions()\n        if not Span.has_extension(\"adicap\"):\n            Span.set_extension(\"adicap\", default=None)\n        if not Span.has_extension(\"value\"):\n            Span.set_extension(\"value\", default=None)\n\n    def decode(self, code):\n        code = re.sub(\"[^A-Za-z0-9 ]+\", \"\", code)\n        exploded = list(code)\n        adicap = AdicapCode(\n            code=code,\n            sampling_mode=self.decode_dict[\"D1\"][\"codes\"].get(exploded[0]),\n            technic=self.decode_dict[\"D2\"][\"codes\"].get(exploded[1]),\n            organ=self.decode_dict[\"D3\"][\"codes\"].get(\"\".join(exploded[2:4])),\n        )\n\n        for d in [\"D4\", \"D5\", \"D6\", \"D7\"]:\n            adicap_short = self.decode_dict[d][\"codes\"].get(\"\".join(exploded[4:8]))\n            adicap_long = self.decode_dict[d][\"codes\"].get(\"\".join(exploded[2:8]))\n\n            if (adicap_short is not None) | (adicap_long is not None):\n                adicap.pathology = self.decode_dict[d][\"label\"]\n                adicap.behaviour_type = self.decode_dict[d][\"codes\"].get(exploded[5])\n\n                if adicap_short is not None:\n                    adicap.pathology_type = adicap_short\n\n                else:\n                    adicap.pathology_type = adicap_long\n\n        return adicap\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Tags ADICAP mentions.\n\n        Parameters\n        ----------\n        doc : Doc\n            spaCy Doc object\n\n        Returns\n        -------\n        doc : Doc\n            spaCy Doc object, annotated for ADICAP\n        \"\"\"\n        spans = self.process(doc)\n        spans = filter_spans(spans)\n\n        for span in spans:\n            span._.adicap = self.decode(span._.assigned[\"code\"])\n            span._.value = span._.adicap\n            span._.assigned = None\n\n        doc.spans[\"adicap\"] = spans\n\n        ents, discarded = filter_spans(list(doc.ents) + spans, return_discarded=True)\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/ner/adicap/adicap/#edsnlp.pipelines.ner.adicap.adicap.Adicap.__call__","title":"<code>__call__(doc)</code>","text":"<p>Tags ADICAP mentions.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for ADICAP</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/ner/adicap/adicap.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Tags ADICAP mentions.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy Doc object\n\n    Returns\n    -------\n    doc : Doc\n        spaCy Doc object, annotated for ADICAP\n    \"\"\"\n    spans = self.process(doc)\n    spans = filter_spans(spans)\n\n    for span in spans:\n        span._.adicap = self.decode(span._.assigned[\"code\"])\n        span._.value = span._.adicap\n        span._.assigned = None\n\n    doc.spans[\"adicap\"] = spans\n\n    ents, discarded = filter_spans(list(doc.ents) + spans, return_discarded=True)\n\n    doc.ents = ents\n\n    if \"discarded\" not in doc.spans:\n        doc.spans[\"discarded\"] = []\n    doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/ner/adicap/factory/","title":"<code>edsnlp.pipelines.ner.adicap.factory</code>","text":""},{"location":"reference/pipelines/ner/adicap/factory/#edsnlp.pipelines.ner.adicap.factory.create_component","title":"<code>create_component(nlp, name='eds.adicap', pattern=base_code, prefix=adicap_prefix, window=500, attr='TEXT')</code>","text":"<p>Create a new component to recognize and normalize ADICAP codes in documents.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.adicap'</code> </p> <code>pattern</code> <p>The regex pattern to use for matching ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>base_code</code> </p> <code>prefix</code> <p>The regex pattern to use for matching the prefix before ADICAP codes</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> DEFAULT: <code>adicap_prefix</code> </p> <code>window</code> <p>Number of tokens to look for prefix. It will never go further the start of the sentence</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> Source code in <code>edsnlp/pipelines/ner/adicap/factory.py</code> <pre><code>@Language.factory(\n    \"eds.adicap\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.adicap\",\n    pattern: Optional[Union[List[str], str]] = base_code,\n    prefix: Optional[Union[List[str], str]] = adicap_prefix,\n    window: int = 500,\n    attr: str = \"TEXT\",\n):\n\"\"\"\n    Create a new component to recognize and normalize ADICAP codes in documents.\n\n    Parameters\n    ----------\n    nlp: Language\n        spaCy `Language` object.\n    name: str\n        The name of the pipe\n    pattern: Optional[Union[List[str], str]]\n        The regex pattern to use for matching ADICAP codes\n    prefix: Optional[Union[List[str], str]]\n        The regex pattern to use for matching the prefix before ADICAP codes\n    window: int\n        Number of tokens to look for prefix. It will never go further the start of\n        the sentence\n    attr: str\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n\n    Returns\n    -------\n\n    \"\"\"\n\n    return Adicap(nlp, pattern=pattern, attr=attr, prefix=prefix, window=window)\n</code></pre>"},{"location":"reference/pipelines/ner/adicap/models/","title":"<code>edsnlp.pipelines.ner.adicap.models</code>","text":""},{"location":"reference/pipelines/ner/adicap/patterns/","title":"<code>edsnlp.pipelines.ner.adicap.patterns</code>","text":"<p>Source : https://esante.gouv.fr/sites/default/files/media_entity/documents/cgts_sem_adicap_fiche-detaillee.pdf</p>"},{"location":"reference/pipelines/ner/cim10/","title":"<code>edsnlp.pipelines.ner.cim10</code>","text":""},{"location":"reference/pipelines/ner/cim10/factory/","title":"<code>edsnlp.pipelines.ner.cim10.factory</code>","text":""},{"location":"reference/pipelines/ner/cim10/factory/#edsnlp.pipelines.ner.cim10.factory.create_component","title":"<code>create_component(nlp, name='eds.cim10', attr='NORM', ignore_excluded=False, ignore_space_tokens=False, term_matcher=TerminologyTermMatcher.exact, term_matcher_config={})</code>","text":"<p>Create a factory that returns new a component to recognize and normalize CIM10 codes and concepts in documents.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.cim10'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either <code>TerminologyTermMatcher.exact</code> or <code>TerminologyTermMatcher.simstring</code></p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code> Source code in <code>edsnlp/pipelines/ner/cim10/factory.py</code> <pre><code>@Language.factory(\n    \"eds.cim10\", default_config=DEFAULT_CONFIG, assigns=[\"doc.ents\", \"doc.spans\"]\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.cim10\",\n    attr: Union[str, Dict[str, str]] = \"NORM\",\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    term_matcher: TerminologyTermMatcher = TerminologyTermMatcher.exact,\n    term_matcher_config: Dict[str, Any] = {},\n):\n\"\"\"\n    Create a factory that returns new a component to recognize and normalize CIM10 codes\n    and concepts in documents.\n\n    Parameters\n    ----------\n    nlp: Language\n        spaCy `Language` object.\n    name: str\n        The name of the pipe\n    attr: Union[str, Dict[str, str]]\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n    ignore_excluded: bool\n        Whether to skip excluded tokens during matching.\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n    term_matcher: TerminologyTermMatcher\n        The term matcher to use, either `TerminologyTermMatcher.exact` or\n        `TerminologyTermMatcher.simstring`\n    term_matcher_config: Dict[str, Any]\n        The configuration for the term matcher\n\n    Returns\n    -------\n    TerminologyMatcher\n    \"\"\"\n\n    return TerminologyMatcher(\n        nlp,\n        label=\"cim10\",\n        regex=None,\n        terms=patterns.get_patterns(),\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        term_matcher=term_matcher,\n        term_matcher_config=term_matcher_config,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/cim10/patterns/","title":"<code>edsnlp.pipelines.ner.cim10.patterns</code>","text":""},{"location":"reference/pipelines/ner/covid/","title":"<code>edsnlp.pipelines.ner.covid</code>","text":""},{"location":"reference/pipelines/ner/covid/factory/","title":"<code>edsnlp.pipelines.ner.covid.factory</code>","text":""},{"location":"reference/pipelines/ner/covid/factory/#edsnlp.pipelines.ner.covid.factory.create_component","title":"<code>create_component(nlp, name='eds.covid', attr='LOWER', ignore_excluded=False, ignore_space_tokens=False)</code>","text":"<p>Create a factory that returns new GenericMatcher with patterns for covid</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.covid'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'LOWER'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>GenericMatcher</code> Source code in <code>edsnlp/pipelines/ner/covid/factory.py</code> <pre><code>@Language.factory(\n    \"eds.covid\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.covid\",\n    attr: Union[str, Dict[str, str]] = \"LOWER\",\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n):\n\"\"\"\n    Create a factory that returns new GenericMatcher with patterns for covid\n\n    Parameters\n    ----------\n    nlp: Language\n        spaCy `Language` object.\n    name: str\n        The name of the pipe\n    attr: Union[str, Dict[str, str]]\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n    ignore_excluded: bool\n        Whether to skip excluded tokens during matching.\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n\n    Returns\n    -------\n    GenericMatcher\n    \"\"\"\n\n    return GenericMatcher(\n        nlp,\n        terms=None,\n        regex=dict(covid=patterns.pattern),\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/covid/patterns/","title":"<code>edsnlp.pipelines.ner.covid.patterns</code>","text":""},{"location":"reference/pipelines/ner/drugs/","title":"<code>edsnlp.pipelines.ner.drugs</code>","text":""},{"location":"reference/pipelines/ner/drugs/factory/","title":"<code>edsnlp.pipelines.ner.drugs.factory</code>","text":""},{"location":"reference/pipelines/ner/drugs/factory/#edsnlp.pipelines.ner.drugs.factory.create_component","title":"<code>create_component(nlp, name='eds.drugs', attr='NORM', ignore_excluded=False, ignore_space_tokens=False, term_matcher=TerminologyTermMatcher.exact, term_matcher_config={})</code>","text":"<p>Create a new component to recognize and normalize drugs in documents. The terminology is based on Romedi (see documentation) and the drugs are normalized to the ATC codes.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.drugs'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either <code>TerminologyTermMatcher.exact</code> or <code>TerminologyTermMatcher.simstring</code></p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>TerminologyMatcher</code> Source code in <code>edsnlp/pipelines/ner/drugs/factory.py</code> <pre><code>@Language.factory(\n    \"eds.drugs\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.drugs\",\n    attr: str = \"NORM\",\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    term_matcher: TerminologyTermMatcher = TerminologyTermMatcher.exact,\n    term_matcher_config: Dict[str, Any] = {},\n):\n\"\"\"\n    Create a new component to recognize and normalize drugs in documents.\n    The terminology is based on Romedi (see documentation) and the\n    drugs are normalized to the ATC codes.\n\n    Parameters\n    ----------\n    nlp: Language\n        spaCy `Language` object.\n    name: str\n        The name of the pipe\n    attr: Union[str, Dict[str, str]]\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n    ignore_excluded: bool\n        Whether to skip excluded tokens during matching.\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n    term_matcher: TerminologyTermMatcher\n        The term matcher to use, either `TerminologyTermMatcher.exact` or\n        `TerminologyTermMatcher.simstring`\n    term_matcher_config: Dict[str, Any]\n        The configuration for the term matcher\n\n    Returns\n    -------\n    TerminologyMatcher\n    \"\"\"\n    return TerminologyMatcher(\n        nlp,\n        label=\"drug\",\n        terms=patterns.get_patterns(),\n        regex=dict(),\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        term_matcher=term_matcher,\n        term_matcher_config=term_matcher_config,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/drugs/patterns/","title":"<code>edsnlp.pipelines.ner.drugs.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/","title":"<code>edsnlp.pipelines.ner.scores</code>","text":""},{"location":"reference/pipelines/ner/scores/base_score/","title":"<code>edsnlp.pipelines.ner.scores.base_score</code>","text":""},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score","title":"<code>Score</code>","text":"<p>         Bases: <code>ContextualMatcher</code></p> <p>Matcher component to extract a numeric score</p> Source code in <code>edsnlp/pipelines/ner/scores/base_score.py</code> <pre><code>class Score(ContextualMatcher):\n\"\"\"Matcher component to extract a numeric score\"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        score_name: str,\n        regex: List[str],\n        attr: str,\n        value_extract: Union[str, Dict[str, str], List[Dict[str, str]]],\n        score_normalization: Union[str, Callable[[Union[str, None]], Any]],\n        window: int,\n        ignore_excluded: bool,\n        ignore_space_tokens: bool,\n        flags: Union[re.RegexFlag, int],\n    ):\n\"\"\"\n        Parameters\n        ----------\n        nlp : Language\n            The spaCy object.\n        score_name : str\n            The name of the extracted score\n        regex : List[str]\n            A list of regexes to identify the score\n        attr : str\n            Whether to match on the text ('TEXT') or on the normalized text ('NORM')\n        value_extract : str\n            Regex with capturing group to get the score value\n        score_normalization : Callable[[Union[str,None]], Any]\n            Function that takes the \"raw\" value extracted from the `value_extract`\n            regex and should return:\n\n            - None if no score could be extracted\n            - The desired score value else\n        window : int\n            Number of token to include after the score's mention to find the\n            score's value\n        ignore_excluded : bool\n            Whether to ignore excluded spans when matching\n        ignore_space_tokens : bool\n            Whether to ignore space tokens when matching\n        flags : Union[re.RegexFlag, int]\n            Regex flags to use when matching\n        \"\"\"\n        if isinstance(value_extract, str):\n            value_extract = dict(\n                name=\"value\",\n                regex=value_extract,\n                window=window,\n            )\n\n        if isinstance(value_extract, dict):\n            value_extract = [value_extract]\n\n        value_exists = False\n        for i, extract in enumerate(value_extract):\n            extract[\"window\"] = extract.get(\"window\", window)\n            if extract.get(\"name\", None) == \"value\":\n                value_exists = True\n                extract[\"replace_entity\"] = True\n                extract[\"reduce_mode\"] = \"keep_first\"\n            value_extract[i] = extract\n\n        assert value_exists, \"You should provide a `value` regex in the `assign` dict.\"\n\n        patterns = dict(\n            source=score_name,\n            regex=regex,\n            assign=value_extract,\n        )\n\n        super().__init__(\n            nlp=nlp,\n            name=score_name,\n            patterns=patterns,\n            assign_as_span=False,\n            alignment_mode=\"expand\",\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n            attr=attr,\n            regex_flags=flags,\n            include_assigned=False,\n        )\n\n        self.score_name = score_name\n\n        if isinstance(score_normalization, str):\n            self.score_normalization = registry.get(\"misc\", score_normalization)\n        else:\n            self.score_normalization = score_normalization\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        super(Score, Score).set_extensions()\n        if not Span.has_extension(\"score_name\"):\n            Span.set_extension(\"score_name\", default=None)\n        if not Span.has_extension(\"score_value\"):\n            Span.set_extension(\"score_value\", default=None)\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Adds spans to document.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for extracted terms.\n        \"\"\"\n\n        ents = self.process(doc)\n        ents = self.score_filtering(ents)\n\n        ents, discarded = filter_spans(\n            list(doc.ents) + list(ents), return_discarded=True\n        )\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n\n    def score_filtering(self, ents: List[Span]) -&gt; List[Span]:\n\"\"\"\n        Extracts, if available, the value of the score.\n        Normalizes the score via the provided `self.score_normalization` method.\n\n        Parameters\n        ----------\n        ents: List[Span]\n            List of spaCy's spans extracted by the score matcher\n\n        Returns\n        -------\n        ents: List[Span]\n            List of spaCy's spans, with, if found, an added `score_value` extension\n        \"\"\"\n\n        for ent in ents:\n            value = ent._.assigned.get(\"value\", None)\n            if value is None:\n                continue\n            normalized_value = self.score_normalization(value)\n            if normalized_value is not None:\n                ent._.score_name = self.score_name\n                ent._.score_value = normalized_value\n\n                yield ent\n</code></pre>"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.__init__","title":"<code>__init__(nlp, score_name, regex, attr, value_extract, score_normalization, window, ignore_excluded, ignore_space_tokens, flags)</code>","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>score_name</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/base_score.py</code> <pre><code>def __init__(\n    self,\n    nlp: Language,\n    score_name: str,\n    regex: List[str],\n    attr: str,\n    value_extract: Union[str, Dict[str, str], List[Dict[str, str]]],\n    score_normalization: Union[str, Callable[[Union[str, None]], Any]],\n    window: int,\n    ignore_excluded: bool,\n    ignore_space_tokens: bool,\n    flags: Union[re.RegexFlag, int],\n):\n\"\"\"\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    score_name : str\n        The name of the extracted score\n    regex : List[str]\n        A list of regexes to identify the score\n    attr : str\n        Whether to match on the text ('TEXT') or on the normalized text ('NORM')\n    value_extract : str\n        Regex with capturing group to get the score value\n    score_normalization : Callable[[Union[str,None]], Any]\n        Function that takes the \"raw\" value extracted from the `value_extract`\n        regex and should return:\n\n        - None if no score could be extracted\n        - The desired score value else\n    window : int\n        Number of token to include after the score's mention to find the\n        score's value\n    ignore_excluded : bool\n        Whether to ignore excluded spans when matching\n    ignore_space_tokens : bool\n        Whether to ignore space tokens when matching\n    flags : Union[re.RegexFlag, int]\n        Regex flags to use when matching\n    \"\"\"\n    if isinstance(value_extract, str):\n        value_extract = dict(\n            name=\"value\",\n            regex=value_extract,\n            window=window,\n        )\n\n    if isinstance(value_extract, dict):\n        value_extract = [value_extract]\n\n    value_exists = False\n    for i, extract in enumerate(value_extract):\n        extract[\"window\"] = extract.get(\"window\", window)\n        if extract.get(\"name\", None) == \"value\":\n            value_exists = True\n            extract[\"replace_entity\"] = True\n            extract[\"reduce_mode\"] = \"keep_first\"\n        value_extract[i] = extract\n\n    assert value_exists, \"You should provide a `value` regex in the `assign` dict.\"\n\n    patterns = dict(\n        source=score_name,\n        regex=regex,\n        assign=value_extract,\n    )\n\n    super().__init__(\n        nlp=nlp,\n        name=score_name,\n        patterns=patterns,\n        assign_as_span=False,\n        alignment_mode=\"expand\",\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        attr=attr,\n        regex_flags=flags,\n        include_assigned=False,\n    )\n\n    self.score_name = score_name\n\n    if isinstance(score_normalization, str):\n        self.score_normalization = registry.get(\"misc\", score_normalization)\n    else:\n        self.score_normalization = score_normalization\n\n    self.set_extensions()\n</code></pre>"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.__call__","title":"<code>__call__(doc)</code>","text":"<p>Adds spans to document.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for extracted terms.</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/base_score.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Adds spans to document.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for extracted terms.\n    \"\"\"\n\n    ents = self.process(doc)\n    ents = self.score_filtering(ents)\n\n    ents, discarded = filter_spans(\n        list(doc.ents) + list(ents), return_discarded=True\n    )\n\n    doc.ents = ents\n\n    if \"discarded\" not in doc.spans:\n        doc.spans[\"discarded\"] = []\n    doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/ner/scores/base_score/#edsnlp.pipelines.ner.scores.base_score.Score.score_filtering","title":"<code>score_filtering(ents)</code>","text":"<p>Extracts, if available, the value of the score. Normalizes the score via the provided <code>self.score_normalization</code> method.</p> PARAMETER DESCRIPTION <code>ents</code> <p>List of spaCy's spans extracted by the score matcher</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>ents</code> <p>List of spaCy's spans, with, if found, an added <code>score_value</code> extension</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/base_score.py</code> <pre><code>def score_filtering(self, ents: List[Span]) -&gt; List[Span]:\n\"\"\"\n    Extracts, if available, the value of the score.\n    Normalizes the score via the provided `self.score_normalization` method.\n\n    Parameters\n    ----------\n    ents: List[Span]\n        List of spaCy's spans extracted by the score matcher\n\n    Returns\n    -------\n    ents: List[Span]\n        List of spaCy's spans, with, if found, an added `score_value` extension\n    \"\"\"\n\n    for ent in ents:\n        value = ent._.assigned.get(\"value\", None)\n        if value is None:\n            continue\n        normalized_value = self.score_normalization(value)\n        if normalized_value is not None:\n            ent._.score_name = self.score_name\n            ent._.score_value = normalized_value\n\n            yield ent\n</code></pre>"},{"location":"reference/pipelines/ner/scores/factory/","title":"<code>edsnlp.pipelines.ner.scores.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/factory/#edsnlp.pipelines.ner.scores.factory.create_component","title":"<code>create_component(nlp, name='eds.score', score_name=None, regex=None, value_extract=None, score_normalization=None, attr='NORM', window=7, flags=0, ignore_excluded=False, ignore_space_tokens=False)</code>","text":"PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.score'</code> </p> <code>score_name</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>regex</code> <p>A list of regexes to identify the score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex with capturing group to get the score value</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return:</p> <ul> <li>None if no score could be extracted</li> <li>The desired score value else</li> </ul> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> DEFAULT: <code>None</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens when matching</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Regex flags to use when matching</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/factory.py</code> <pre><code>@deprecated_factory(\n    \"score\",\n    \"eds.score\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\n@Language.factory(\n    \"eds.score\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.score\",\n    score_name: str = None,\n    regex: List[str] = None,\n    value_extract: str = None,\n    score_normalization: Union[str, Callable[[Union[str, None]], Any]] = None,\n    attr: str = \"NORM\",\n    window: int = 7,\n    flags: Union[re.RegexFlag, int] = 0,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n):\n\"\"\"\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    name : str\n        The name of the component.\n    score_name : str\n        The name of the extracted score\n    regex : List[str]\n        A list of regexes to identify the score\n    attr : str\n        Whether to match on the text ('TEXT') or on the normalized text ('NORM')\n    value_extract : str\n        Regex with capturing group to get the score value\n    score_normalization : Callable[[Union[str,None]], Any]\n        Function that takes the \"raw\" value extracted from the `value_extract` regex,\n        and should return:\n\n        - None if no score could be extracted\n        - The desired score value else\n    window : int\n        Number of token to include after the score's mention to find the\n        score's value\n    ignore_excluded : bool\n        Whether to ignore excluded spans when matching\n    ignore_space_tokens : bool\n        Whether to ignore space tokens when matching\n    flags : Union[re.RegexFlag, int]\n        Regex flags to use when matching\n    \"\"\"\n    return Score(\n        nlp,\n        score_name=score_name,\n        regex=regex,\n        value_extract=value_extract,\n        score_normalization=score_normalization,\n        attr=attr,\n        flags=flags,\n        window=window,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/scores/charlson/","title":"<code>edsnlp.pipelines.ner.scores.charlson</code>","text":""},{"location":"reference/pipelines/ner/scores/charlson/factory/","title":"<code>edsnlp.pipelines.ner.scores.charlson.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/charlson/patterns/","title":"<code>edsnlp.pipelines.ner.scores.charlson.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/charlson/patterns/#edsnlp.pipelines.ner.scores.charlson.patterns.score_normalization","title":"<code>score_normalization(extracted_score)</code>","text":"<p>Charlson score normalization. If available, returns the integer value of the Charlson score.</p> Source code in <code>edsnlp/pipelines/ner/scores/charlson/patterns.py</code> <pre><code>@spacy.registry.misc(score_normalization_str)\ndef score_normalization(extracted_score: Union[str, None]):\n\"\"\"\n    Charlson score normalization.\n    If available, returns the integer value of the Charlson score.\n    \"\"\"\n    score_range = list(range(0, 30))\n    try:\n        if (extracted_score is not None) and (int(extracted_score) in score_range):\n            return int(extracted_score)\n    except ValueError:\n        return None\n</code></pre>"},{"location":"reference/pipelines/ner/scores/elstonellis/","title":"<code>edsnlp.pipelines.ner.scores.elstonellis</code>","text":""},{"location":"reference/pipelines/ner/scores/elstonellis/factory/","title":"<code>edsnlp.pipelines.ner.scores.elstonellis.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/elstonellis/factory/#edsnlp.pipelines.ner.scores.elstonellis.factory.create_component","title":"<code>create_component(nlp, name, regex=patterns.regex, value_extract=patterns.value_extract, score_normalization=patterns.score_normalization_str, attr='TEXT', window=20, ignore_excluded=False, ignore_space_tokens=False, flags=0)</code>","text":"<p>Matcher for the Elston-Ellis score.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy Language object</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>regex</code> <p>The regex patterns to match</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.regex</code> </p> <code>value_extract</code> <p>The regex pattern to extract the value from the matched text</p> <p> TYPE: <code>str</code> DEFAULT: <code>patterns.value_extract</code> </p> <code>score_normalization</code> <p>The normalization function to apply to the extracted value</p> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>patterns.score_normalization_str</code> </p> <code>attr</code> <p>The token attribute to match on (e.g. \"TEXT\" or \"NORM\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>window</code> <p>The window size to search for the regex pattern</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>The regex flags to use</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>Returns</code> <p> </p> <code>Score</code> <p> </p> Source code in <code>edsnlp/pipelines/ner/scores/elstonellis/factory.py</code> <pre><code>@Language.factory(\n    \"eds.elston-ellis\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str,\n    regex: List[str] = patterns.regex,\n    value_extract: str = patterns.value_extract,\n    score_normalization: Union[\n        str, Callable[[Union[str, None]], Any]\n    ] = patterns.score_normalization_str,\n    attr: str = \"TEXT\",\n    window: int = 20,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    flags: Union[re.RegexFlag, int] = 0,\n):\n\"\"\"\n    Matcher for the Elston-Ellis score.\n\n    Parameters\n    ----------\n    nlp: Language\n        The spaCy Language object\n    name: str\n        The name of the component\n    regex: List[str]\n        The regex patterns to match\n    value_extract: str\n        The regex pattern to extract the value from the matched text\n    score_normalization: Union[str, Callable[[Union[str, None]], Any]]\n        The normalization function to apply to the extracted value\n    attr: str\n        The token attribute to match on (e.g. \"TEXT\" or \"NORM\")\n    window: int\n        The window size to search for the regex pattern\n    ignore_excluded: bool\n        Whether to ignore excluded tokens\n    ignore_space_tokens: bool\n        Whether to ignore space tokens\n    flags: Union[re.RegexFlag, int]\n        The regex flags to use\n    Returns\n    -------\n    Score\n    \"\"\"\n    return Score(\n        nlp,\n        score_name=name,\n        regex=regex,\n        value_extract=value_extract,\n        score_normalization=score_normalization,\n        attr=attr,\n        window=window,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        flags=flags,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/","title":"<code>edsnlp.pipelines.ner.scores.elstonellis.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/elstonellis/patterns/#edsnlp.pipelines.ner.scores.elstonellis.patterns.score_normalization","title":"<code>score_normalization(extracted_score)</code>","text":"<p>Elston and Ellis score normalization. If available, returns the integer value of the Elston and Ellis score.</p> Source code in <code>edsnlp/pipelines/ner/scores/elstonellis/patterns.py</code> <pre><code>@spacy.registry.misc(score_normalization_str)\ndef score_normalization(extracted_score: Union[str, None]):\n\"\"\"\n    Elston and Ellis score normalization.\n    If available, returns the integer value of the Elston and Ellis score.\n    \"\"\"\n    try:\n        x = 0\n        for i in re.findall(r\"[0-3]\", extracted_score):\n            x += int(i)\n\n        if x &lt;= 5:\n            return 1\n\n        elif x &lt;= 7:\n            return 2\n\n        else:\n            return 3\n\n    except ValueError:\n        return None\n</code></pre>"},{"location":"reference/pipelines/ner/scores/emergency/","title":"<code>edsnlp.pipelines.ner.scores.emergency</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/ccmu/","title":"<code>edsnlp.pipelines.ner.scores.emergency.ccmu</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/","title":"<code>edsnlp.pipelines.ner.scores.emergency.ccmu.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/ccmu/factory/#edsnlp.pipelines.ner.scores.emergency.ccmu.factory.create_component","title":"<code>create_component(nlp, name='eds.emergency.ccmu', regex=patterns.regex, value_extract=patterns.value_extract, score_normalization=patterns.score_normalization_str, attr='NORM', window=20, ignore_excluded=False, ignore_space_tokens=False, flags=0)</code>","text":"<p>Matcher for the Emergency CCMU score.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy Language object</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.emergency.ccmu'</code> </p> <code>regex</code> <p>The regex patterns to match</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.regex</code> </p> <code>value_extract</code> <p>The regex pattern to extract the value from the matched text</p> <p> TYPE: <code>str</code> DEFAULT: <code>patterns.value_extract</code> </p> <code>score_normalization</code> <p>The normalization function to apply to the extracted value</p> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>patterns.score_normalization_str</code> </p> <code>attr</code> <p>The token attribute to match on (e.g. \"TEXT\" or \"NORM\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>window</code> <p>The window size to search for the regex pattern</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>The regex flags to use</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>Returns</code> <p> </p> <code>Score</code> <p> </p> Source code in <code>edsnlp/pipelines/ner/scores/emergency/ccmu/factory.py</code> <pre><code>@deprecated_factory(\n    \"emergency.ccmu\",\n    \"eds.emergency.ccmu\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\n@Language.factory(\n    \"eds.emergency.ccmu\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.emergency.ccmu\",\n    regex: List[str] = patterns.regex,\n    value_extract: str = patterns.value_extract,\n    score_normalization: Union[\n        str, Callable[[Union[str, None]], Any]\n    ] = patterns.score_normalization_str,\n    attr: str = \"NORM\",\n    window: int = 20,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    flags: Union[re.RegexFlag, int] = 0,\n):\n\"\"\"\n    Matcher for the Emergency CCMU score.\n\n    Parameters\n    ----------\n    nlp: Language\n        The spaCy Language object\n    name: str\n        The name of the component\n    regex: List[str]\n        The regex patterns to match\n    value_extract: str\n        The regex pattern to extract the value from the matched text\n    score_normalization: Union[str, Callable[[Union[str, None]], Any]]\n        The normalization function to apply to the extracted value\n    attr: str\n        The token attribute to match on (e.g. \"TEXT\" or \"NORM\")\n    window: int\n        The window size to search for the regex pattern\n    ignore_excluded: bool\n        Whether to ignore excluded tokens\n    ignore_space_tokens: bool\n        Whether to ignore space tokens\n    flags: Union[re.RegexFlag, int]\n        The regex flags to use\n    Returns\n    -------\n    Score\n    \"\"\"\n    return Score(\n        nlp,\n        score_name=name,\n        regex=regex,\n        value_extract=value_extract,\n        score_normalization=score_normalization,\n        attr=attr,\n        window=window,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        flags=flags,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/","title":"<code>edsnlp.pipelines.ner.scores.emergency.ccmu.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/ccmu/patterns/#edsnlp.pipelines.ner.scores.emergency.ccmu.patterns.score_normalization","title":"<code>score_normalization(extracted_score)</code>","text":"<p>CCMU score normalization. If available, returns the integer value of the CCMU score.</p> Source code in <code>edsnlp/pipelines/ner/scores/emergency/ccmu/patterns.py</code> <pre><code>@spacy.registry.misc(score_normalization_str)\ndef score_normalization(extracted_score: Union[str, None]):\n\"\"\"\n    CCMU score normalization.\n    If available, returns the integer value of the CCMU score.\n    \"\"\"\n    score_range = [1, 2, 3, 4, 5]\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n</code></pre>"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/","title":"<code>edsnlp.pipelines.ner.scores.emergency.gemsa</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/","title":"<code>edsnlp.pipelines.ner.scores.emergency.gemsa.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/gemsa/factory/#edsnlp.pipelines.ner.scores.emergency.gemsa.factory.create_component","title":"<code>create_component(nlp, name='eds.emergency.gemsa', regex=patterns.regex, value_extract=patterns.value_extract, score_normalization=patterns.score_normalization_str, attr='NORM', window=20, ignore_excluded=False, ignore_space_tokens=False, flags=0)</code>","text":"<p>Matcher for the Emergency CCMU score.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy Language object</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.emergency.gemsa'</code> </p> <code>regex</code> <p>The regex patterns to match</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.regex</code> </p> <code>value_extract</code> <p>The regex pattern to extract the value from the matched text</p> <p> TYPE: <code>str</code> DEFAULT: <code>patterns.value_extract</code> </p> <code>score_normalization</code> <p>The normalization function to apply to the extracted value</p> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>patterns.score_normalization_str</code> </p> <code>attr</code> <p>The token attribute to match on (e.g. \"TEXT\" or \"NORM\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>window</code> <p>The window size to search for the regex pattern</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>The regex flags to use</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>Returns</code> <p> </p> <code>Score</code> <p> </p> Source code in <code>edsnlp/pipelines/ner/scores/emergency/gemsa/factory.py</code> <pre><code>@deprecated_factory(\n    \"emergency.gemsa\",\n    \"eds.emergency.gemsa\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\n@Language.factory(\n    \"eds.emergency.gemsa\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.emergency.gemsa\",\n    regex: List[str] = patterns.regex,\n    value_extract: str = patterns.value_extract,\n    score_normalization: Union[\n        str, Callable[[Union[str, None]], Any]\n    ] = patterns.score_normalization_str,\n    attr: str = \"NORM\",\n    window: int = 20,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    flags: Union[re.RegexFlag, int] = 0,\n):\n\"\"\"\n    Matcher for the Emergency CCMU score.\n\n    Parameters\n    ----------\n    nlp: Language\n        The spaCy Language object\n    name: str\n        The name of the component\n    regex: List[str]\n        The regex patterns to match\n    value_extract: str\n        The regex pattern to extract the value from the matched text\n    score_normalization: Union[str, Callable[[Union[str, None]], Any]]\n        The normalization function to apply to the extracted value\n    attr: str\n        The token attribute to match on (e.g. \"TEXT\" or \"NORM\")\n    window: int\n        The window size to search for the regex pattern\n    ignore_excluded: bool\n        Whether to ignore excluded tokens\n    ignore_space_tokens: bool\n        Whether to ignore space tokens\n    flags: Union[re.RegexFlag, int]\n        The regex flags to use\n    Returns\n    -------\n    Score\n    \"\"\"\n    return Score(\n        nlp,\n        score_name=name,\n        regex=regex,\n        value_extract=value_extract,\n        score_normalization=score_normalization,\n        attr=attr,\n        window=window,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        flags=flags,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/","title":"<code>edsnlp.pipelines.ner.scores.emergency.gemsa.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/gemsa/patterns/#edsnlp.pipelines.ner.scores.emergency.gemsa.patterns.score_normalization","title":"<code>score_normalization(extracted_score)</code>","text":"<p>GEMSA score normalization. If available, returns the integer value of the GEMSA score.</p> Source code in <code>edsnlp/pipelines/ner/scores/emergency/gemsa/patterns.py</code> <pre><code>@spacy.registry.misc(score_normalization_str)\ndef score_normalization(extracted_score: Union[str, None]):\n\"\"\"\n    GEMSA score normalization.\n    If available, returns the integer value of the GEMSA score.\n    \"\"\"\n    score_range = [1, 2, 3, 4, 5, 6]\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n</code></pre>"},{"location":"reference/pipelines/ner/scores/emergency/priority/","title":"<code>edsnlp.pipelines.ner.scores.emergency.priority</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/","title":"<code>edsnlp.pipelines.ner.scores.emergency.priority.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/priority/factory/#edsnlp.pipelines.ner.scores.emergency.priority.factory.create_component","title":"<code>create_component(nlp, name='emergency.priority', regex=patterns.regex, value_extract=patterns.value_extract, score_normalization=patterns.score_normalization_str, attr='NORM', window=7, ignore_excluded=False, ignore_space_tokens=False, flags=0)</code>","text":"<p>Matcher for the Emergency Priority score.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy Language object</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'emergency.priority'</code> </p> <code>regex</code> <p>The regex patterns to match</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.regex</code> </p> <code>value_extract</code> <p>The regex pattern to extract the value from the matched text</p> <p> TYPE: <code>str</code> DEFAULT: <code>patterns.value_extract</code> </p> <code>score_normalization</code> <p>The normalization function to apply to the extracted value</p> <p> TYPE: <code>Union[str, Callable[[Union[str, None]], Any]]</code> DEFAULT: <code>patterns.score_normalization_str</code> </p> <code>attr</code> <p>The token attribute to match on (e.g. \"TEXT\" or \"NORM\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>window</code> <p>The window size to search for the regex pattern</p> <p> TYPE: <code>int</code> DEFAULT: <code>7</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>The regex flags to use</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> <code>Returns</code> <p> </p> <code>Score</code> <p> </p> Source code in <code>edsnlp/pipelines/ner/scores/emergency/priority/factory.py</code> <pre><code>@deprecated_factory(\n    \"emergency.priority\",\n    \"eds.emergency.priority\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\n@Language.factory(\n    \"eds.emergency.priority\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"emergency.priority\",\n    regex: List[str] = patterns.regex,\n    value_extract: str = patterns.value_extract,\n    score_normalization: Union[\n        str, Callable[[Union[str, None]], Any]\n    ] = patterns.score_normalization_str,\n    attr: str = \"NORM\",\n    window: int = 7,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    flags: Union[re.RegexFlag, int] = 0,\n):\n\"\"\"\n    Matcher for the Emergency Priority score.\n\n    Parameters\n    ----------\n    nlp: Language\n        The spaCy Language object\n    name: str\n        The name of the component\n    regex: List[str]\n        The regex patterns to match\n    value_extract: str\n        The regex pattern to extract the value from the matched text\n    score_normalization: Union[str, Callable[[Union[str, None]], Any]]\n        The normalization function to apply to the extracted value\n    attr: str\n        The token attribute to match on (e.g. \"TEXT\" or \"NORM\")\n    window: int\n        The window size to search for the regex pattern\n    ignore_excluded: bool\n        Whether to ignore excluded tokens\n    ignore_space_tokens: bool\n        Whether to ignore space tokens\n    flags: Union[re.RegexFlag, int]\n        The regex flags to use\n    Returns\n    -------\n    Score\n    \"\"\"\n    return Score(\n        nlp,\n        score_name=name,\n        regex=regex,\n        value_extract=value_extract,\n        score_normalization=score_normalization,\n        attr=attr,\n        window=window,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        flags=flags,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/","title":"<code>edsnlp.pipelines.ner.scores.emergency.priority.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/emergency/priority/patterns/#edsnlp.pipelines.ner.scores.emergency.priority.patterns.score_normalization","title":"<code>score_normalization(extracted_score)</code>","text":"<p>Priority score normalization. If available, returns the integer value of the priority score.</p> Source code in <code>edsnlp/pipelines/ner/scores/emergency/priority/patterns.py</code> <pre><code>@spacy.registry.misc(score_normalization_str)\ndef score_normalization(extracted_score: Union[str, None]):\n\"\"\"\n    Priority score normalization.\n    If available, returns the integer value of the priority score.\n    \"\"\"\n    score_range = list(range(0, 6))\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n</code></pre>"},{"location":"reference/pipelines/ner/scores/sofa/","title":"<code>edsnlp.pipelines.ner.scores.sofa</code>","text":""},{"location":"reference/pipelines/ner/scores/sofa/factory/","title":"<code>edsnlp.pipelines.ner.scores.sofa.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/sofa/factory/#edsnlp.pipelines.ner.scores.sofa.factory.create_component","title":"<code>create_component(nlp, name, regex=patterns.regex, value_extract=patterns.value_extract, score_normalization=patterns.score_normalization_str, attr='NORM', window=10, ignore_excluded=False, ignore_space_tokens=False, flags=0)</code>","text":"<p>Matcher component to extract the SOFA score</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the extracted score</p> <p> TYPE: <code>str</code> </p> <code>regex</code> <p>A list of regexes to identify the SOFA score</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>patterns.regex</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')</p> <p> TYPE: <code>str</code> DEFAULT: <code>'NORM'</code> </p> <code>value_extract</code> <p>Regex to extract the score value</p> <p> TYPE: <code>Dict[str, str]</code> DEFAULT: <code>patterns.value_extract</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return - None if no score could be extracted - The desired score value else</p> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> DEFAULT: <code>patterns.score_normalization_str</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>flags</code> <p>Flags to pass to the regex</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> DEFAULT: <code>0</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/sofa/factory.py</code> <pre><code>@deprecated_factory(\n    \"SOFA\",\n    \"eds.SOFA\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\n@Language.factory(\n    \"eds.SOFA\",\n    default_config=DEFAULT_CONFIG,\n    assigns=[\"doc.ents\", \"doc.spans\"],\n)\ndef create_component(\n    nlp: Language,\n    name: str,\n    regex: List[str] = patterns.regex,\n    value_extract: List[Dict[str, str]] = patterns.value_extract,\n    score_normalization: Union[\n        str, Callable[[Union[str, None]], Any]\n    ] = patterns.score_normalization_str,\n    attr: str = \"NORM\",\n    window: int = 10,\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    flags: Union[re.RegexFlag, int] = 0,\n):\n\"\"\"\n    Matcher component to extract the SOFA score\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    name : str\n        The name of the extracted score\n    regex : List[str]\n        A list of regexes to identify the SOFA score\n    attr : str\n        Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')\n    value_extract : Dict[str, str]\n        Regex to extract the score value\n    score_normalization : Callable[[Union[str,None]], Any]\n        Function that takes the \"raw\" value extracted from the `value_extract` regex,\n        and should return\n        - None if no score could be extracted\n        - The desired score value else\n    window : int\n        Number of token to include after the score's mention to find the\n        score's value\n    ignore_excluded : bool\n        Whether to ignore excluded spans\n    ignore_space_tokens : bool\n        Whether to ignore space tokens\n    flags : Union[re.RegexFlag, int]\n        Flags to pass to the regex\n    \"\"\"\n    return Sofa(\n        nlp,\n        score_name=name,\n        regex=regex,\n        value_extract=value_extract,\n        score_normalization=score_normalization,\n        attr=attr,\n        window=window,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        flags=flags,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/scores/sofa/patterns/","title":"<code>edsnlp.pipelines.ner.scores.sofa.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/sofa/patterns/#edsnlp.pipelines.ner.scores.sofa.patterns.score_normalization","title":"<code>score_normalization(extracted_score)</code>","text":"<p>Sofa score normalization. If available, returns the integer value of the SOFA score.</p> Source code in <code>edsnlp/pipelines/ner/scores/sofa/patterns.py</code> <pre><code>@spacy.registry.misc(score_normalization_str)\ndef score_normalization(extracted_score: Union[str, None]):\n\n\"\"\"\n    Sofa score normalization.\n    If available, returns the integer value of the SOFA score.\n    \"\"\"\n    score_range = list(range(0, 30))\n    if (extracted_score is not None) and (int(extracted_score) in score_range):\n        return int(extracted_score)\n</code></pre>"},{"location":"reference/pipelines/ner/scores/sofa/sofa/","title":"<code>edsnlp.pipelines.ner.scores.sofa.sofa</code>","text":""},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa","title":"<code>Sofa</code>","text":"<p>         Bases: <code>Score</code></p> <p>Matcher component to extract the SOFA score</p> PARAMETER DESCRIPTION <code>nlp</code> <p>The spaCy object.</p> <p> TYPE: <code>Language</code> </p> <code>regex</code> <p>A list of regexes to identify the SOFA score</p> <p> TYPE: <code>List[str]</code> </p> <code>attr</code> <p>Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')</p> <p> TYPE: <code>str</code> </p> <code>value_extract</code> <p>Regex to extract the score value</p> <p> TYPE: <code>Dict[str, str]</code> </p> <code>score_normalization</code> <p>Function that takes the \"raw\" value extracted from the <code>value_extract</code> regex, and should return - None if no score could be extracted - The desired score value else</p> <p> TYPE: <code>Callable[[Union[str, None]], Any]</code> </p> <code>window</code> <p>Number of token to include after the score's mention to find the score's value</p> <p> TYPE: <code>int</code> </p> <code>ignore_excluded</code> <p>Whether to ignore excluded spans</p> <p> TYPE: <code>bool</code> </p> <code>ignore_space_tokens</code> <p>Whether to ignore space tokens</p> <p> TYPE: <code>bool</code> </p> <code>flags</code> <p>Flags to pass to the regex</p> <p> TYPE: <code>Union[re.RegexFlag, int]</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/sofa/sofa.py</code> <pre><code>class Sofa(Score):\n\"\"\"\n    Matcher component to extract the SOFA score\n\n    Parameters\n    ----------\n    nlp : Language\n        The spaCy object.\n    regex : List[str]\n        A list of regexes to identify the SOFA score\n    attr : str\n        Whether to match on the text ('TEXT') or on the normalized text ('CUSTOM_NORM')\n    value_extract : Dict[str, str]\n        Regex to extract the score value\n    score_normalization : Callable[[Union[str,None]], Any]\n        Function that takes the \"raw\" value extracted from the `value_extract` regex,\n        and should return\n        - None if no score could be extracted\n        - The desired score value else\n    window : int\n        Number of token to include after the score's mention to find the\n        score's value\n    ignore_excluded : bool\n        Whether to ignore excluded spans\n    ignore_space_tokens : bool\n        Whether to ignore space tokens\n    flags : Union[re.RegexFlag, int]\n        Flags to pass to the regex\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Language,\n        score_name: str,\n        regex: List[str],\n        attr: str,\n        value_extract: List[Dict[str, str]],\n        score_normalization: Union[str, Callable[[Union[str, None]], Any]],\n        window: int,\n        flags: Union[re.RegexFlag, int],\n        ignore_excluded: bool,\n        ignore_space_tokens: bool,\n    ):\n\n        super().__init__(\n            nlp,\n            score_name=score_name,\n            regex=regex,\n            value_extract=value_extract,\n            score_normalization=score_normalization,\n            attr=attr,\n            window=window,\n            flags=flags,\n            ignore_excluded=ignore_excluded,\n            ignore_space_tokens=ignore_space_tokens,\n        )\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        super(Sofa, Sofa).set_extensions()\n        if not Span.has_extension(\"score_method\"):\n            Span.set_extension(\"score_method\", default=None)\n\n    def score_filtering(self, ents: List[Span]) -&gt; List[Span]:\n\"\"\"\n        Extracts, if available, the value of the score.\n        Normalizes the score via the provided `self.score_normalization` method.\n\n        Parameters\n        ----------\n        ents: List[Span]\n            List of spaCy's spans extracted by the score matcher\n\n        Returns\n        -------\n        ents: List[Span]\n            List of spaCy's spans, with, if found, an added `score_value` extension\n        \"\"\"\n\n        for ent in ents:\n            assigned = ent._.assigned\n            if not assigned:\n                continue\n            if assigned.get(\"method_max\") is not None:\n                method = \"Maximum\"\n            elif assigned.get(\"method_24h\") is not None:\n                method = \"24H\"\n            elif assigned.get(\"method_adm\") is not None:\n                method = \"A l'admission\"\n            else:\n                method = \"Non pr\u00e9cis\u00e9e\"\n\n            normalized_value = self.score_normalization(assigned[\"value\"])\n\n            if normalized_value is not None:\n                ent._.score_name = self.score_name\n                ent._.score_value = int(normalized_value)\n                ent._.score_method = method\n\n                yield ent\n</code></pre>"},{"location":"reference/pipelines/ner/scores/sofa/sofa/#edsnlp.pipelines.ner.scores.sofa.sofa.Sofa.score_filtering","title":"<code>score_filtering(ents)</code>","text":"<p>Extracts, if available, the value of the score. Normalizes the score via the provided <code>self.score_normalization</code> method.</p> PARAMETER DESCRIPTION <code>ents</code> <p>List of spaCy's spans extracted by the score matcher</p> <p> TYPE: <code>List[Span]</code> </p> RETURNS DESCRIPTION <code>ents</code> <p>List of spaCy's spans, with, if found, an added <code>score_value</code> extension</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/sofa/sofa.py</code> <pre><code>def score_filtering(self, ents: List[Span]) -&gt; List[Span]:\n\"\"\"\n    Extracts, if available, the value of the score.\n    Normalizes the score via the provided `self.score_normalization` method.\n\n    Parameters\n    ----------\n    ents: List[Span]\n        List of spaCy's spans extracted by the score matcher\n\n    Returns\n    -------\n    ents: List[Span]\n        List of spaCy's spans, with, if found, an added `score_value` extension\n    \"\"\"\n\n    for ent in ents:\n        assigned = ent._.assigned\n        if not assigned:\n            continue\n        if assigned.get(\"method_max\") is not None:\n            method = \"Maximum\"\n        elif assigned.get(\"method_24h\") is not None:\n            method = \"24H\"\n        elif assigned.get(\"method_adm\") is not None:\n            method = \"A l'admission\"\n        else:\n            method = \"Non pr\u00e9cis\u00e9e\"\n\n        normalized_value = self.score_normalization(assigned[\"value\"])\n\n        if normalized_value is not None:\n            ent._.score_name = self.score_name\n            ent._.score_value = int(normalized_value)\n            ent._.score_method = method\n\n            yield ent\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/","title":"<code>edsnlp.pipelines.ner.scores.tnm</code>","text":""},{"location":"reference/pipelines/ner/scores/tnm/factory/","title":"<code>edsnlp.pipelines.ner.scores.tnm.factory</code>","text":""},{"location":"reference/pipelines/ner/scores/tnm/models/","title":"<code>edsnlp.pipelines.ner.scores.tnm.models</code>","text":""},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM","title":"<code>TNM</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/models.py</code> <pre><code>class TNM(BaseModel):\n\n    prefix: Optional[Prefix] = None\n    tumour: Optional[Tumour] = None\n    tumour_specification: Optional[Specification] = None\n    tumour_suffix: Optional[str] = None\n    node: Optional[Node] = None\n    node_specification: Optional[Specification] = None\n    node_suffix: Optional[str] = None\n    metastasis: Optional[Metastasis] = None\n    resection_completeness: Optional[int] = None\n    version: Optional[str] = None\n    version_year: Optional[int] = None\n\n    @validator(\"*\", pre=True)\n    def coerce_o(cls, v):\n        if isinstance(v, str):\n            v = v.replace(\"o\", \"0\")\n        return v\n\n    @validator(\"version_year\")\n    def validate_year(cls, v):\n        if v is None:\n            return v\n\n        if v &lt; 40:\n            v += 2000\n        elif v &lt; 100:\n            v += 1900\n\n        return v\n\n    def norm(self) -&gt; str:\n        norm = []\n\n        if self.prefix is not None:\n            norm.append(str(self.prefix))\n\n        if (\n            (self.tumour is not None)\n            | (self.tumour_specification is not None)\n            | (self.tumour_suffix is not None)\n        ):\n            norm.append(f\"T{str(self.tumour or '')}\")\n            norm.append(f\"{str(self.tumour_specification or '')}\")\n            norm.append(f\"{str(self.tumour_suffix or '')}\")\n\n        if (\n            (self.node is not None)\n            | (self.node_specification is not None)\n            | (self.node_suffix is not None)\n        ):\n            norm.append(f\"N{str(self.node or '')}\")\n            norm.append(f\"{str(self.node_specification or '')}\")\n            norm.append(f\"{str(self.node_suffix or '')}\")\n\n        if self.metastasis is not None:\n            norm.append(f\"M{self.metastasis}\")\n\n        if self.resection_completeness is not None:\n            norm.append(f\"R{self.resection_completeness}\")\n\n        if self.version is not None and self.version_year is not None:\n            norm.append(f\" ({self.version.upper()} {self.version_year})\")\n\n        return \"\".join(norm)\n\n    def dict(\n        self,\n        *,\n        include: Union[\"AbstractSetIntStr\", \"MappingIntStrAny\"] = None,\n        exclude: Union[\"AbstractSetIntStr\", \"MappingIntStrAny\"] = None,\n        by_alias: bool = False,\n        skip_defaults: bool = None,\n        exclude_unset: bool = False,\n        exclude_defaults: bool = False,\n        exclude_none: bool = False,\n    ) -&gt; \"DictStrAny\":\n\"\"\"\n        Generate a dictionary representation of the model,\n        optionally specifying which fields to include or exclude.\n\n        \"\"\"\n        if skip_defaults is not None:\n            warnings.warn(\n                f\"\"\"{self.__class__.__name__}.dict(): \"skip_defaults\"\n                is deprecated and replaced by \"exclude_unset\" \"\"\",\n                DeprecationWarning,\n            )\n            exclude_unset = skip_defaults\n\n        d = dict(\n            self._iter(\n                to_dict=True,\n                by_alias=by_alias,\n                include=include,\n                exclude=exclude,\n                exclude_unset=exclude_unset,\n                exclude_defaults=exclude_defaults,\n                exclude_none=exclude_none,\n            )\n        )\n        set_keys = set(d.keys())\n        for k in set_keys.intersection(\n            {\n                \"prefix\",\n                \"tumour\",\n                \"node\",\n                \"metastasis\",\n                \"tumour_specification\",\n                \"node_specification\",\n                \"tumour_suffix\",\n                \"node_suffix\",\n            }\n        ):\n            v = d[k]\n            if isinstance(v, TnmEnum):\n                d[k] = v.value\n\n        return d\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/models/#edsnlp.pipelines.ner.scores.tnm.models.TNM.dict","title":"<code>dict(*, include=None, exclude=None, by_alias=False, skip_defaults=None, exclude_unset=False, exclude_defaults=False, exclude_none=False)</code>","text":"<p>Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.</p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/models.py</code> <pre><code>def dict(\n    self,\n    *,\n    include: Union[\"AbstractSetIntStr\", \"MappingIntStrAny\"] = None,\n    exclude: Union[\"AbstractSetIntStr\", \"MappingIntStrAny\"] = None,\n    by_alias: bool = False,\n    skip_defaults: bool = None,\n    exclude_unset: bool = False,\n    exclude_defaults: bool = False,\n    exclude_none: bool = False,\n) -&gt; \"DictStrAny\":\n\"\"\"\n    Generate a dictionary representation of the model,\n    optionally specifying which fields to include or exclude.\n\n    \"\"\"\n    if skip_defaults is not None:\n        warnings.warn(\n            f\"\"\"{self.__class__.__name__}.dict(): \"skip_defaults\"\n            is deprecated and replaced by \"exclude_unset\" \"\"\",\n            DeprecationWarning,\n        )\n        exclude_unset = skip_defaults\n\n    d = dict(\n        self._iter(\n            to_dict=True,\n            by_alias=by_alias,\n            include=include,\n            exclude=exclude,\n            exclude_unset=exclude_unset,\n            exclude_defaults=exclude_defaults,\n            exclude_none=exclude_none,\n        )\n    )\n    set_keys = set(d.keys())\n    for k in set_keys.intersection(\n        {\n            \"prefix\",\n            \"tumour\",\n            \"node\",\n            \"metastasis\",\n            \"tumour_specification\",\n            \"node_specification\",\n            \"tumour_suffix\",\n            \"node_suffix\",\n        }\n    ):\n        v = d[k]\n        if isinstance(v, TnmEnum):\n            d[k] = v.value\n\n    return d\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/patterns/","title":"<code>edsnlp.pipelines.ner.scores.tnm.patterns</code>","text":""},{"location":"reference/pipelines/ner/scores/tnm/tnm/","title":"<code>edsnlp.pipelines.ner.scores.tnm.tnm</code>","text":"<p><code>eds.tnm</code> pipeline.</p>"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM","title":"<code>TNM</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Tags and normalizes TNM mentions.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>Language pipeline object</p> <p> TYPE: <code>spacy.language.Language</code> </p> <code>pattern</code> <p>List of regular expressions for TNM mentions.</p> <p> TYPE: <code>Optional[Union[List[str], str]]</code> </p> <code>attr</code> <p>spaCy attribute to use</p> <p> TYPE: <code>str</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/tnm.py</code> <pre><code>class TNM(BaseComponent):\n\"\"\"\n    Tags and normalizes TNM mentions.\n\n    Parameters\n    ----------\n    nlp : spacy.language.Language\n        Language pipeline object\n    pattern : Optional[Union[List[str], str]]\n        List of regular expressions for TNM mentions.\n    attr : str\n        spaCy attribute to use\n    \"\"\"\n\n    # noinspection PyProtectedMember\n    def __init__(\n        self,\n        nlp: Language,\n        pattern: Optional[Union[List[str], str]],\n        attr: str,\n    ):\n\n        self.nlp = nlp\n\n        if pattern is None:\n            pattern = patterns.tnm_pattern\n\n        if isinstance(pattern, str):\n            pattern = [pattern]\n\n        self.regex_matcher = RegexMatcher(attr=attr, alignment_mode=\"strict\")\n        self.regex_matcher.add(\"tnm\", pattern)\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\"\"\"\n        Set extensions for the dates pipeline.\n        \"\"\"\n\n        if not Span.has_extension(\"value\"):\n            Span.set_extension(\"value\", default=None)\n\n    def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Find TNM mentions in doc.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        spans:\n            list of tnm spans\n        \"\"\"\n\n        spans = self.regex_matcher(\n            doc,\n            as_spans=True,\n            return_groupdict=True,\n        )\n\n        spans = filter_spans(spans)\n\n        return spans\n\n    def parse(self, spans: List[Tuple[Span, Dict[str, str]]]) -&gt; List[Span]:\n\"\"\"\n        Parse dates using the groupdict returned by the matcher.\n\n        Parameters\n        ----------\n        spans : List[Tuple[Span, Dict[str, str]]]\n            List of tuples containing the spans and groupdict\n            returned by the matcher.\n\n        Returns\n        -------\n        List[Span]\n            List of processed spans, with the date parsed.\n        \"\"\"\n\n        for span, groupdict in spans:\n            try:\n                span._.value = models.TNM.parse_obj(groupdict)\n            except ValidationError:\n                span._.value = models.TNM.parse_obj({})\n\n            span.kb_id_ = span._.value.norm()\n\n        return [span for span, _ in spans]\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Tags TNM mentions.\n\n        Parameters\n        ----------\n        doc : Doc\n            spaCy Doc object\n\n        Returns\n        -------\n        doc : Doc\n            spaCy Doc object, annotated for TNM\n        \"\"\"\n        spans = self.process(doc)\n        spans = filter_spans(spans)\n\n        spans = self.parse(spans)\n\n        doc.spans[\"tnm\"] = spans\n\n        ents, discarded = filter_spans(list(doc.ents) + spans, return_discarded=True)\n\n        doc.ents = ents\n\n        if \"discarded\" not in doc.spans:\n            doc.spans[\"discarded\"] = []\n        doc.spans[\"discarded\"].extend(discarded)\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.set_extensions","title":"<code>set_extensions()</code>  <code>classmethod</code>","text":"<p>Set extensions for the dates pipeline.</p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/tnm.py</code> <pre><code>@classmethod\ndef set_extensions(cls) -&gt; None:\n\"\"\"\n    Set extensions for the dates pipeline.\n    \"\"\"\n\n    if not Span.has_extension(\"value\"):\n        Span.set_extension(\"value\", default=None)\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.process","title":"<code>process(doc)</code>","text":"<p>Find TNM mentions in doc.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>spans</code> <p>list of tnm spans</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/tnm.py</code> <pre><code>def process(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Find TNM mentions in doc.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    spans:\n        list of tnm spans\n    \"\"\"\n\n    spans = self.regex_matcher(\n        doc,\n        as_spans=True,\n        return_groupdict=True,\n    )\n\n    spans = filter_spans(spans)\n\n    return spans\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.parse","title":"<code>parse(spans)</code>","text":"<p>Parse dates using the groupdict returned by the matcher.</p> PARAMETER DESCRIPTION <code>spans</code> <p>List of tuples containing the spans and groupdict returned by the matcher.</p> <p> TYPE: <code>List[Tuple[Span, Dict[str, str]]]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of processed spans, with the date parsed.</p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/tnm.py</code> <pre><code>def parse(self, spans: List[Tuple[Span, Dict[str, str]]]) -&gt; List[Span]:\n\"\"\"\n    Parse dates using the groupdict returned by the matcher.\n\n    Parameters\n    ----------\n    spans : List[Tuple[Span, Dict[str, str]]]\n        List of tuples containing the spans and groupdict\n        returned by the matcher.\n\n    Returns\n    -------\n    List[Span]\n        List of processed spans, with the date parsed.\n    \"\"\"\n\n    for span, groupdict in spans:\n        try:\n            span._.value = models.TNM.parse_obj(groupdict)\n        except ValidationError:\n            span._.value = models.TNM.parse_obj({})\n\n        span.kb_id_ = span._.value.norm()\n\n    return [span for span, _ in spans]\n</code></pre>"},{"location":"reference/pipelines/ner/scores/tnm/tnm/#edsnlp.pipelines.ner.scores.tnm.tnm.TNM.__call__","title":"<code>__call__(doc)</code>","text":"<p>Tags TNM mentions.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for TNM</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/ner/scores/tnm/tnm.py</code> <pre><code>def __call__(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Tags TNM mentions.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy Doc object\n\n    Returns\n    -------\n    doc : Doc\n        spaCy Doc object, annotated for TNM\n    \"\"\"\n    spans = self.process(doc)\n    spans = filter_spans(spans)\n\n    spans = self.parse(spans)\n\n    doc.spans[\"tnm\"] = spans\n\n    ents, discarded = filter_spans(list(doc.ents) + spans, return_discarded=True)\n\n    doc.ents = ents\n\n    if \"discarded\" not in doc.spans:\n        doc.spans[\"discarded\"] = []\n    doc.spans[\"discarded\"].extend(discarded)\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/ner/umls/","title":"<code>edsnlp.pipelines.ner.umls</code>","text":""},{"location":"reference/pipelines/ner/umls/factory/","title":"<code>edsnlp.pipelines.ner.umls.factory</code>","text":""},{"location":"reference/pipelines/ner/umls/factory/#edsnlp.pipelines.ner.umls.factory.create_component","title":"<code>create_component(nlp, name='eds.umls', attr='NORM', ignore_excluded=False, ignore_space_tokens=False, term_matcher=TerminologyTermMatcher.exact, term_matcher_config={}, pattern_config=dict(languages=['FRE'], sources=None))</code>","text":"<p>Create a component to recognize and normalize terms in document that normalize to UMLS concepts.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy <code>Language</code> object.</p> <p> TYPE: <code>Language</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'eds.umls'</code> </p> <code>attr</code> <p>Attribute to match on, eg <code>TEXT</code>, <code>NORM</code>, etc.</p> <p> TYPE: <code>Union[str, Dict[str, str]]</code> DEFAULT: <code>'NORM'</code> </p> <code>ignore_excluded</code> <p>Whether to skip excluded tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_space_tokens</code> <p>Whether to skip space tokens during matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>term_matcher</code> <p>The term matcher to use, either <code>TerminologyTermMatcher.exact</code> or <code>TerminologyTermMatcher.simstring</code></p> <p> TYPE: <code>TerminologyTermMatcher</code> DEFAULT: <code>TerminologyTermMatcher.exact</code> </p> <code>term_matcher_config</code> <p>The configuration for the term matcher</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>pattern_config</code> <p>The pattern retriever configuration</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>dict(languages=['FRE'], sources=None)</code> </p> Source code in <code>edsnlp/pipelines/ner/umls/factory.py</code> <pre><code>@Language.factory(\n    \"eds.umls\", default_config=DEFAULT_CONFIG, assigns=[\"doc.ents\", \"doc.spans\"]\n)\ndef create_component(\n    nlp: Language,\n    name: str = \"eds.umls\",\n    attr: Union[str, Dict[str, str]] = \"NORM\",\n    ignore_excluded: bool = False,\n    ignore_space_tokens: bool = False,\n    term_matcher: TerminologyTermMatcher = TerminologyTermMatcher.exact,\n    term_matcher_config: Dict[str, Any] = {},\n    pattern_config: Dict[str, Any] = dict(\n        languages=[\"FRE\"],\n        sources=None,\n    ),\n):\n\"\"\"\n    Create a component to recognize and normalize terms in document that\n    normalize to UMLS concepts.\n\n    Parameters\n    ----------\n    nlp: Language\n        spaCy `Language` object.\n    name: str\n        The name of the pipe\n    attr: Union[str, Dict[str, str]]\n        Attribute to match on, eg `TEXT`, `NORM`, etc.\n    ignore_excluded: bool\n        Whether to skip excluded tokens during matching.\n    ignore_space_tokens: bool\n        Whether to skip space tokens during matching.\n    term_matcher: TerminologyTermMatcher\n        The term matcher to use, either `TerminologyTermMatcher.exact` or\n        `TerminologyTermMatcher.simstring`\n    term_matcher_config: Dict[str, Any]\n        The configuration for the term matcher\n    pattern_config: Dict[str, Any]\n        The pattern retriever configuration\n\n    Returns\n    -------\n\n    \"\"\"\n\n    return TerminologyMatcher(\n        nlp,\n        label=\"umls\",\n        regex=None,\n        terms=patterns.get_patterns(pattern_config),\n        attr=attr,\n        ignore_excluded=ignore_excluded,\n        ignore_space_tokens=ignore_space_tokens,\n        term_matcher=term_matcher,\n        term_matcher_config=term_matcher_config,\n    )\n</code></pre>"},{"location":"reference/pipelines/ner/umls/patterns/","title":"<code>edsnlp.pipelines.ner.umls.patterns</code>","text":""},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_patterns","title":"<code>get_patterns(config)</code>","text":"<p>Load the UMLS terminology patterns.</p> PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_patterns--return","title":"Return","text":"<p>patterns : dict[list]     The mapping between CUI codes and their synonyms.</p>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_patterns--notes","title":"Notes","text":"<p>When run for the first time, this method will download the entire UMLS file and store it at ~/.data/bio/umls/2022AA/. Therefore the second run will be significantly faster than the first one.</p> Source code in <code>edsnlp/pipelines/ner/umls/patterns.py</code> <pre><code>def get_patterns(config: Dict[str, Any]) -&gt; Dict[str, List[str]]:\n\"\"\"Load the UMLS terminology patterns.\n\n    Parameters\n    ----------\n    config : dict[list]\n        Languages and sources to select from the whole terminology.\n        For both keys, None will select all values.\n\n    Return\n    ------\n    patterns : dict[list]\n        The mapping between CUI codes and their synonyms.\n\n    Notes\n    -----\n    When run for the first time, this method will download the\n    entire UMLS file and store it at ~/.data/bio/umls/2022AA/.\n    Therefore the second run will be significantly faster than\n    the first one.\n    \"\"\"\n\n    path, module, filename = get_path(config)\n\n    if path.exists():\n        print(f\"Loading {filename} from {module.base}\")\n        return module.load_pickle(name=filename)\n    else:\n        patterns = download_and_agg_umls(config)\n        module.dump_pickle(name=filename, obj=patterns)\n        print(f\"Saved patterns into {module.base / filename}\")\n        return patterns\n</code></pre>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_path","title":"<code>get_path(config)</code>","text":"<p>Get the path, module and filename of the UMLS file.</p> PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_path--return","title":"Return","text":"<p>path, module, filename : pathlib.Path, pystow.module, str</p>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.get_path--notes","title":"Notes","text":"<p><code>get_path</code> will convert the config dict into a pretty filename.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\"languages\": [\"FRE\", \"ENG\"], \"sources\": None}\n&gt;&gt;&gt; print(get_path(config))\n.data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\"\n</code></pre> Source code in <code>edsnlp/pipelines/ner/umls/patterns.py</code> <pre><code>def get_path(config: Dict[str, Any]) -&gt; Tuple[Path, pystow.impl.Module, str]:\n\"\"\"Get the path, module and filename of the UMLS file.\n\n    Parameters\n    ----------\n    config : dict[list]\n        Languages and sources to select from the whole terminology.\n        For both keys, None will select all values.\n\n    Return\n    ------\n    path, module, filename : pathlib.Path, pystow.module, str\n\n    Notes\n    -----\n    `get_path` will convert the config dict into a pretty filename.\n\n    Examples\n    --------\n    &gt;&gt;&gt; config = {\"languages\": [\"FRE\", \"ENG\"], \"sources\": None}\n    &gt;&gt;&gt; print(get_path(config))\n    .data/bio/umls/2022AA/languagesFRE-ENG_sourcesNone.pkl\"\n    \"\"\"\n    config_txt = \"\"\n    for k, v in config.items():\n        if isinstance(v, (list, tuple)):\n            v = \"-\".join(v)\n        _config_txt = f\"{k}{v}\"\n        if config_txt:\n            _config_txt = f\"_{_config_txt}\"\n        config_txt += _config_txt\n\n    filename = f\"{PATTERN_VERSION}_{config_txt}.pkl\"\n    module = pystow.module(\"bio\", \"umls\", UMLS_VERSION)\n    path = module.base / filename\n\n    return path, module, filename\n</code></pre>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.download_and_agg_umls","title":"<code>download_and_agg_umls(config)</code>","text":"<p>Download the UMLS if not exist and create a mapping between CUI code and synonyms.</p> PARAMETER DESCRIPTION <code>config</code> <p>Languages and sources to select from the whole terminology. For both keys, None will select all values.</p> <p> TYPE: <code>dict[list]</code> </p>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.download_and_agg_umls--return","title":"Return","text":"<p>patterns : dict[list]     The mapping between CUI codes and their synonyms.</p>"},{"location":"reference/pipelines/ner/umls/patterns/#edsnlp.pipelines.ner.umls.patterns.download_and_agg_umls--notes","title":"Notes","text":"<p>Performs filtering on the returned mapping only, not the downloaded resource.</p> Source code in <code>edsnlp/pipelines/ner/umls/patterns.py</code> <pre><code>def download_and_agg_umls(config) -&gt; Dict[str, List[str]]:\n\"\"\"Download the UMLS if not exist and create a mapping\n    between CUI code and synonyms.\n\n    Parameters\n    ----------\n    config : dict[list]\n        Languages and sources to select from the whole terminology.\n        For both keys, None will select all values.\n\n    Return\n    ------\n    patterns : dict[list]\n        The mapping between CUI codes and their synonyms.\n\n    Notes\n    -----\n    Performs filtering on the returned mapping only, not the downloaded\n    resource.\n    \"\"\"\n\n    api_key = os.getenv(\"UMLS_API_KEY\")\n    if not api_key:\n        warnings.warn(\n            \"You need to define UMLS_API_KEY to download the UMLS. \"\n            \"Get a key by creating an account at \"\n            \"https://uts.nlm.nih.gov/uts/signup-login\"\n        )\n\n    path = download_umls(version=UMLS_VERSION, api_key=api_key)\n\n    # https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.concept_names_and_sources_file_mr/ # noqa\n    header = [\n        \"CUI\",\n        \"LAT\",\n        \"TS\",\n        \"LUI\",\n        \"STT\",\n        \"SUI\",\n        \"ISPREF\",\n        \"AUI\",\n        \"SAUI\",\n        \"SCUI\",\n        \"SDUI\",\n        \"SAB\",\n        \"TTY\",\n        \"CODE\",\n        \"STR\",\n        \"STRL\",\n        \"SUPPRESS\",\n    ]\n    header_to_idx = dict(zip(header, range(len(header))))\n    patterns = defaultdict(list)\n\n    languages = config.get(\"languages\")\n    sources = config.get(\"sources\")\n\n    with zipfile.ZipFile(path) as zip_file:\n        with zip_file.open(\"MRCONSO.RRF\", mode=\"r\") as file:\n            for row in tqdm(\n                file.readlines(), desc=\"Loading 'MRCONSO.RRF' into a dictionnary\"\n            ):\n                row = row.decode(\"utf-8\").split(\"|\")\n\n                if (languages is None or row[header_to_idx[\"LAT\"]] in languages) and (\n                    sources is None or row[header_to_idx[\"SAB\"]] in sources\n                ):\n                    cui = row[header_to_idx[\"CUI\"]]\n                    synonym = row[header_to_idx[\"STR\"]]\n                    patterns[cui].append(synonym)\n\n    return patterns\n</code></pre>"},{"location":"reference/pipelines/qualifiers/","title":"<code>edsnlp.pipelines.qualifiers</code>","text":""},{"location":"reference/pipelines/qualifiers/base/","title":"<code>edsnlp.pipelines.qualifiers.base</code>","text":""},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier","title":"<code>Qualifier</code>","text":"<p>         Bases: <code>BaseComponent</code></p> <p>Implements the NegEx algorithm.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <p> TYPE: <code>bool</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> <code>**terms</code> <p>Terms to look for.</p> <p> TYPE: <code>Dict[str, Optional[List[str]]]</code> DEFAULT: <code>{}</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/base.py</code> <pre><code>class Qualifier(BaseComponent):\n\"\"\"\n    Implements the NegEx algorithm.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'\n        we can also add a key for each regex.\n    on_ents_only : bool\n        Whether to look for matches around detected entities only.\n        Useful for faster inference in downstream tasks.\n    explain : bool\n        Whether to keep track of cues for each entity.\n    **terms : Dict[str, Optional[List[str]]]\n        Terms to look for.\n    \"\"\"\n\n    defaults = dict()\n\n    def __init__(\n        self,\n        nlp: Language,\n        attr: str,\n        on_ents_only: bool,\n        explain: bool,\n        **terms: Dict[str, Optional[List[str]]],\n    ):\n\n        if attr.upper() == \"NORM\":\n            check_normalizer(nlp)\n\n        self.phrase_matcher = EDSPhraseMatcher(vocab=nlp.vocab, attr=attr)\n        self.phrase_matcher.build_patterns(nlp=nlp, terms=terms)\n\n        self.on_ents_only = on_ents_only\n        self.explain = explain\n\n    def get_defaults(\n        self, **kwargs: Dict[str, Optional[List[str]]]\n    ) -&gt; Dict[str, List[str]]:\n\"\"\"\n        Merge terms with their defaults. Null keys are replaced with defaults.\n\n        Returns\n        -------\n        Dict[str, List[str]]\n            Merged dictionary\n        \"\"\"\n        # Filter out empty keys\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n        # Update defaults\n        terms = self.defaults.copy()\n        terms.update(kwargs)\n\n        return terms\n\n    def get_matches(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n        Extract matches.\n\n        Parameters\n        ----------\n        doc : Doc\n            spaCy `Doc` object.\n\n        Returns\n        -------\n        List[Span]\n            List of detected spans\n        \"\"\"\n        if self.on_ents_only:\n\n            sents = set([ent.sent for ent in doc.ents])\n            match_iterator = map(\n                lambda sent: self.phrase_matcher(sent, as_spans=True), sents\n            )\n\n            matches = chain.from_iterable(match_iterator)\n\n        else:\n            matches = self.phrase_matcher(doc, as_spans=True)\n\n        return list(matches)\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n        return self.process(doc)\n</code></pre>"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.get_defaults","title":"<code>get_defaults(**kwargs)</code>","text":"<p>Merge terms with their defaults. Null keys are replaced with defaults.</p> RETURNS DESCRIPTION <code>Dict[str, List[str]]</code> <p>Merged dictionary</p> Source code in <code>edsnlp/pipelines/qualifiers/base.py</code> <pre><code>def get_defaults(\n    self, **kwargs: Dict[str, Optional[List[str]]]\n) -&gt; Dict[str, List[str]]:\n\"\"\"\n    Merge terms with their defaults. Null keys are replaced with defaults.\n\n    Returns\n    -------\n    Dict[str, List[str]]\n        Merged dictionary\n    \"\"\"\n    # Filter out empty keys\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    # Update defaults\n    terms = self.defaults.copy()\n    terms.update(kwargs)\n\n    return terms\n</code></pre>"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.Qualifier.get_matches","title":"<code>get_matches(doc)</code>","text":"<p>Extract matches.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy <code>Doc</code> object.</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of detected spans</p> Source code in <code>edsnlp/pipelines/qualifiers/base.py</code> <pre><code>def get_matches(self, doc: Doc) -&gt; List[Span]:\n\"\"\"\n    Extract matches.\n\n    Parameters\n    ----------\n    doc : Doc\n        spaCy `Doc` object.\n\n    Returns\n    -------\n    List[Span]\n        List of detected spans\n    \"\"\"\n    if self.on_ents_only:\n\n        sents = set([ent.sent for ent in doc.ents])\n        match_iterator = map(\n            lambda sent: self.phrase_matcher(sent, as_spans=True), sents\n        )\n\n        matches = chain.from_iterable(match_iterator)\n\n    else:\n        matches = self.phrase_matcher(doc, as_spans=True)\n\n    return list(matches)\n</code></pre>"},{"location":"reference/pipelines/qualifiers/base/#edsnlp.pipelines.qualifiers.base.get_qualifier_extensions","title":"<code>get_qualifier_extensions(nlp)</code>","text":"<p>Check for all qualifiers present in the pipe and return its corresponding extension</p> Source code in <code>edsnlp/pipelines/qualifiers/base.py</code> <pre><code>def get_qualifier_extensions(nlp: Language):\n\"\"\"\n    Check for all qualifiers present in the pipe and return its corresponding extension\n    \"\"\"\n    return {\n        name: nlp.get_pipe_meta(name).assigns[0].split(\"span.\")[-1]\n        for name, pipe in nlp.pipeline\n        if isinstance(pipe, Qualifier)\n    }\n</code></pre>"},{"location":"reference/pipelines/qualifiers/factories/","title":"<code>edsnlp.pipelines.qualifiers.factories</code>","text":""},{"location":"reference/pipelines/qualifiers/family/","title":"<code>edsnlp.pipelines.qualifiers.family</code>","text":""},{"location":"reference/pipelines/qualifiers/family/factory/","title":"<code>edsnlp.pipelines.qualifiers.family.factory</code>","text":""},{"location":"reference/pipelines/qualifiers/family/family/","title":"<code>edsnlp.pipelines.qualifiers.family.family</code>","text":""},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext","title":"<code>FamilyContext</code>","text":"<p>         Bases: <code>Qualifier</code></p> <p>Implements a family context detection algorithm.</p> <p>The components looks for terms indicating family references in the text.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>family</code> <p>List of terms indicating family reference.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>terminations</code> <p>List of termination terms, to separate syntagmas.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <p> TYPE: <code>bool</code> </p> <code>regex</code> <p>A dictionnary of regex patterns.</p> <p> TYPE: <code>Optional[Dict[str, Union[List[str], str]]]</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> <code>use_sections</code> <p>Whether to use annotated sections (namely <code>ant\u00e9c\u00e9dents familiaux</code>).</p> <p> TYPE: <code>bool, by default </code> </p> Source code in <code>edsnlp/pipelines/qualifiers/family/family.py</code> <pre><code>class FamilyContext(Qualifier):\n\"\"\"\n    Implements a family context detection algorithm.\n\n    The components looks for terms indicating family references in the text.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    family : Optional[List[str]]\n        List of terms indicating family reference.\n    terminations : Optional[List[str]]\n        List of termination terms, to separate syntagmas.\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'\n        we can also add a key for each regex.\n    on_ents_only : bool\n        Whether to look for matches around detected entities only.\n        Useful for faster inference in downstream tasks.\n    regex : Optional[Dict[str, Union[List[str], str]]]\n        A dictionnary of regex patterns.\n    explain : bool\n        Whether to keep track of cues for each entity.\n    use_sections : bool, by default `False`\n        Whether to use annotated sections (namely `ant\u00e9c\u00e9dents familiaux`).\n    \"\"\"\n\n    defaults = dict(\n        family=family,\n        termination=termination,\n    )\n\n    def __init__(\n        self,\n        nlp: Language,\n        attr: str,\n        family: Optional[List[str]],\n        termination: Optional[List[str]],\n        use_sections: bool,\n        explain: bool,\n        on_ents_only: bool,\n    ):\n\n        terms = self.get_defaults(\n            family=family,\n            termination=termination,\n        )\n\n        super().__init__(\n            nlp=nlp,\n            attr=attr,\n            on_ents_only=on_ents_only,\n            explain=explain,\n            **terms,\n        )\n\n        self.set_extensions()\n\n        self.sections = use_sections and (\n            \"eds.sections\" in nlp.pipe_names or \"sections\" in nlp.pipe_names\n        )\n        if use_sections and not self.sections:\n            logger.warning(\n                \"You have requested that the pipeline use annotations \"\n                \"provided by the `section` pipeline, but it was not set. \"\n                \"Skipping that step.\"\n            )\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        if not Token.has_extension(\"family\"):\n            Token.set_extension(\"family\", default=False)\n\n        if not Token.has_extension(\"family_\"):\n            Token.set_extension(\n                \"family_\",\n                getter=lambda token: \"FAMILY\" if token._.family else \"PATIENT\",\n            )\n\n        if not Span.has_extension(\"family\"):\n            Span.set_extension(\"family\", default=False)\n\n        if not Span.has_extension(\"family_\"):\n            Span.set_extension(\n                \"family_\",\n                getter=lambda span: \"FAMILY\" if span._.family else \"PATIENT\",\n            )\n\n        if not Span.has_extension(\"family_cues\"):\n            Span.set_extension(\"family_cues\", default=[])\n\n        if not Doc.has_extension(\"family\"):\n            Doc.set_extension(\"family\", default=[])\n\n    def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Finds entities related to family context.\n\n        Parameters\n        ----------\n        doc: spaCy Doc object\n\n        Returns\n        -------\n        doc: spaCy Doc object, annotated for context\n        \"\"\"\n        matches = self.get_matches(doc)\n\n        terminations = get_spans(matches, \"termination\")\n        boundaries = self._boundaries(doc, terminations)\n\n        # Removes duplicate matches and pseudo-expressions in one statement\n        matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n        entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n        ents = None\n\n        sections = []\n\n        if self.sections:\n            sections = [\n                Span(doc, section.start, section.end, label=\"FAMILY\")\n                for section in doc.spans[\"sections\"]\n                if section.label_ == \"ant\u00e9c\u00e9dents familiaux\"\n            ]\n\n        for start, end in boundaries:\n\n            ents, entities = consume_spans(\n                entities,\n                filter=lambda s: check_inclusion(s, start, end),\n                second_chance=ents,\n            )\n\n            sub_matches, matches = consume_spans(\n                matches, lambda s: start &lt;= s.start &lt; end\n            )\n\n            sub_sections, sections = consume_spans(sections, lambda s: doc[start] in s)\n\n            if self.on_ents_only and not ents:\n                continue\n\n            cues = get_spans(sub_matches, \"family\")\n            cues += sub_sections\n\n            if not cues:\n                continue\n\n            family = bool(cues)\n\n            if not family:\n                continue\n\n            if not self.on_ents_only:\n                for token in doc[start:end]:\n                    token._.family = True\n\n            for ent in ents:\n                ent._.family = True\n                if self.explain:\n                    ent._.family_cues += cues\n                if not self.on_ents_only:\n                    for token in ent:\n                        token._.family = True\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/family/family/#edsnlp.pipelines.qualifiers.family.family.FamilyContext.process","title":"<code>process(doc)</code>","text":"<p>Finds entities related to family context.</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy Doc object, annotated for context</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/family/family.py</code> <pre><code>def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Finds entities related to family context.\n\n    Parameters\n    ----------\n    doc: spaCy Doc object\n\n    Returns\n    -------\n    doc: spaCy Doc object, annotated for context\n    \"\"\"\n    matches = self.get_matches(doc)\n\n    terminations = get_spans(matches, \"termination\")\n    boundaries = self._boundaries(doc, terminations)\n\n    # Removes duplicate matches and pseudo-expressions in one statement\n    matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n    entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n    ents = None\n\n    sections = []\n\n    if self.sections:\n        sections = [\n            Span(doc, section.start, section.end, label=\"FAMILY\")\n            for section in doc.spans[\"sections\"]\n            if section.label_ == \"ant\u00e9c\u00e9dents familiaux\"\n        ]\n\n    for start, end in boundaries:\n\n        ents, entities = consume_spans(\n            entities,\n            filter=lambda s: check_inclusion(s, start, end),\n            second_chance=ents,\n        )\n\n        sub_matches, matches = consume_spans(\n            matches, lambda s: start &lt;= s.start &lt; end\n        )\n\n        sub_sections, sections = consume_spans(sections, lambda s: doc[start] in s)\n\n        if self.on_ents_only and not ents:\n            continue\n\n        cues = get_spans(sub_matches, \"family\")\n        cues += sub_sections\n\n        if not cues:\n            continue\n\n        family = bool(cues)\n\n        if not family:\n            continue\n\n        if not self.on_ents_only:\n            for token in doc[start:end]:\n                token._.family = True\n\n        for ent in ents:\n            ent._.family = True\n            if self.explain:\n                ent._.family_cues += cues\n            if not self.on_ents_only:\n                for token in ent:\n                    token._.family = True\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/family/patterns/","title":"<code>edsnlp.pipelines.qualifiers.family.patterns</code>","text":""},{"location":"reference/pipelines/qualifiers/history/","title":"<code>edsnlp.pipelines.qualifiers.history</code>","text":""},{"location":"reference/pipelines/qualifiers/history/factory/","title":"<code>edsnlp.pipelines.qualifiers.history.factory</code>","text":""},{"location":"reference/pipelines/qualifiers/history/history/","title":"<code>edsnlp.pipelines.qualifiers.history.history</code>","text":""},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History","title":"<code>History</code>","text":"<p>         Bases: <code>Qualifier</code></p> <p>Implements an history detection algorithm.</p> <p>The components looks for terms indicating history in the text.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>history</code> <p>List of terms indicating medical history reference.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>termination</code> <p>List of syntagme termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>use_sections</code> <p>Whether to use section pipeline to detect medical history section.</p> <p> TYPE: <code>bool</code> </p> <code>use_dates</code> <p>Whether to use dates pipeline to detect if the event occurs a long time before the document date.</p> <p> TYPE: <code>bool</code> </p> <code>history_limit</code> <p>The number of days after which the event is considered as history.</p> <p> TYPE: <code>int</code> </p> <code>exclude_birthdate</code> <p>Whether to exclude the birth date from history dates.</p> <p> TYPE: <code>bool</code> </p> <code>closest_dates_only</code> <p>Whether to include the closest dates only.</p> <p> TYPE: <code>bool</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <p> TYPE: <code>bool</code> </p> <code>regex</code> <p>A dictionary of regex patterns.</p> <p> TYPE: <code>Optional[Dict[str, Union[List[str], str]]]</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/history/history.py</code> <pre><code>class History(Qualifier):\n\"\"\"\n    Implements an history detection algorithm.\n\n    The components looks for terms indicating history in the text.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    history : Optional[List[str]]\n        List of terms indicating medical history reference.\n    termination : Optional[List[str]]\n        List of syntagme termination terms.\n    use_sections : bool\n        Whether to use section pipeline to detect medical history section.\n    use_dates : bool\n        Whether to use dates pipeline to detect if the event occurs\n         a long time before the document date.\n    history_limit : int\n        The number of days after which the event is considered as history.\n    exclude_birthdate : bool\n        Whether to exclude the birth date from history dates.\n    closest_dates_only : bool\n        Whether to include the closest dates only.\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'\n        we can also add a key for each regex.\n    on_ents_only : bool\n        Whether to look for matches around detected entities only.\n        Useful for faster inference in downstream tasks.\n    regex : Optional[Dict[str, Union[List[str], str]]]\n        A dictionary of regex patterns.\n    explain : bool\n        Whether to keep track of cues for each entity.\n    \"\"\"\n\n    defaults = dict(\n        history=history,\n        termination=termination,\n    )\n\n    def __init__(\n        self,\n        nlp: Language,\n        attr: str,\n        history: Optional[List[str]],\n        termination: Optional[List[str]],\n        use_sections: bool,\n        use_dates: bool,\n        history_limit: int,\n        closest_dates_only: bool,\n        exclude_birthdate: bool,\n        explain: bool,\n        on_ents_only: bool,\n    ):\n\n        terms = self.get_defaults(\n            history=history,\n            termination=termination,\n        )\n\n        super().__init__(\n            nlp=nlp,\n            attr=attr,\n            on_ents_only=on_ents_only,\n            explain=explain,\n            **terms,\n        )\n\n        self.set_extensions()\n\n        self.history_limit = timedelta(history_limit)\n        self.exclude_birthdate = exclude_birthdate\n        self.closest_dates_only = closest_dates_only\n\n        self.sections = use_sections and (\n            \"eds.sections\" in nlp.pipe_names or \"sections\" in nlp.pipe_names\n        )\n        if use_sections and not self.sections:\n            logger.warning(\n                \"You have requested that the pipeline use annotations \"\n                \"provided by the `section` pipeline, but it was not set. \"\n                \"Skipping that step.\"\n            )\n\n        self.dates = use_dates and (\n            \"eds.dates\" in nlp.pipe_names or \"dates\" in nlp.pipe_names\n        )\n        if use_dates:\n            if not self.dates:\n                logger.warning(\n                    \"You have requested that the pipeline use dates \"\n                    \"provided by the `dates` pipeline, but it was not set. \"\n                    \"Skipping that step.\"\n                )\n            elif exclude_birthdate:\n                logger.info(\n                    \"You have requested that the pipeline use date \"\n                    \"and exclude birth dates. \"\n                    \"To make the most of this feature, \"\n                    \"make sur you provide the `birth_datetime` \"\n                    \"context and `note_datetime` context. \"\n                )\n            else:\n                logger.info(\n                    \"You have requested that the pipeline use date \"\n                    \"To make the most of this feature, \"\n                    \"make sure you provide the `note_datetime` \"\n                    \"context. \"\n                )\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\n        if not Token.has_extension(\"history\"):\n            Token.set_extension(\"history\", default=False)\n\n        if not Token.has_extension(\"antecedents\"):\n            Token.set_extension(\n                \"antecedents\",\n                getter=deprecated_getter_factory(\"antecedents\", \"history\"),\n            )\n\n        if not Token.has_extension(\"antecedent\"):\n            Token.set_extension(\n                \"antecedent\",\n                getter=deprecated_getter_factory(\"antecedent\", \"history\"),\n            )\n\n        if not Token.has_extension(\"history_\"):\n            Token.set_extension(\n                \"history_\",\n                getter=lambda token: \"ATCD\" if token._.history else \"CURRENT\",\n            )\n\n        if not Token.has_extension(\"antecedents_\"):\n            Token.set_extension(\n                \"antecedents_\",\n                getter=deprecated_getter_factory(\"antecedents_\", \"history_\"),\n            )\n\n        if not Token.has_extension(\"antecedent_\"):\n            Token.set_extension(\n                \"antecedent_\",\n                getter=deprecated_getter_factory(\"antecedent_\", \"history_\"),\n            )\n\n        if not Span.has_extension(\"history\"):\n            Span.set_extension(\"history\", default=False)\n\n        if not Span.has_extension(\"antecedents\"):\n            Span.set_extension(\n                \"antecedents\",\n                getter=deprecated_getter_factory(\"antecedents\", \"history\"),\n            )\n\n        if not Span.has_extension(\"antecedent\"):\n            Span.set_extension(\n                \"antecedent\",\n                getter=deprecated_getter_factory(\"antecedent\", \"history\"),\n            )\n\n        if not Span.has_extension(\"history_\"):\n            Span.set_extension(\n                \"history_\",\n                getter=lambda span: \"ATCD\" if span._.history else \"CURRENT\",\n            )\n\n        if not Span.has_extension(\"antecedents_\"):\n            Span.set_extension(\n                \"antecedents_\",\n                getter=deprecated_getter_factory(\"antecedents_\", \"history_\"),\n            )\n\n        if not Span.has_extension(\"antecedent_\"):\n            Span.set_extension(\n                \"antecedent_\",\n                getter=deprecated_getter_factory(\"antecedent_\", \"history_\"),\n            )\n        # Store history mentions responsible for the history entity's character\n        if not Span.has_extension(\"history_cues\"):\n            Span.set_extension(\"history_cues\", default=[])\n\n        # Store recent mentions responsible for the non-antecedent entity's character\n        if not Span.has_extension(\"recent_cues\"):\n            Span.set_extension(\"recent_cues\", default=[])\n\n        if not Span.has_extension(\"antecedents_cues\"):\n            Span.set_extension(\n                \"antecedents_cues\",\n                getter=deprecated_getter_factory(\"antecedents_cues\", \"history_cues\"),\n            )\n\n        if not Span.has_extension(\"antecedent_cues\"):\n            Span.set_extension(\n                \"antecedent_cues\",\n                getter=deprecated_getter_factory(\"antecedent_cues\", \"history_cues\"),\n            )\n\n    def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Finds entities related to history.\n\n        Parameters\n        ----------\n        doc:\n            spaCy Doc object\n\n        Returns\n        -------\n        doc:\n            spaCy Doc object, annotated for history\n        \"\"\"\n\n        if doc._.note_datetime is not None:\n            try:\n                note_datetime = pendulum.instance(doc._.note_datetime)\n                note_datetime = note_datetime.set(tz=\"Europe/Paris\")\n            except ValueError:\n                logger.debug(\n                    \"note_datetime must be a datetime objects. \"\n                    \"Skipping history qualification from note_datetime.\"\n                )\n                note_datetime = None\n\n        if doc._.birth_datetime is not None:\n            try:\n                birth_datetime = pendulum.instance(doc._.birth_datetime)\n                birth_datetime = birth_datetime.set(tz=\"Europe/Paris\")\n            except ValueError:\n                logger.debug(\n                    \"birth_datetime must be a datetime objects. \"\n                    \"Skipping history qualification from birth date.\"\n                )\n                birth_datetime = None\n\n        matches = self.get_matches(doc)\n\n        terminations = get_spans(matches, \"termination\")\n        boundaries = self._boundaries(doc, terminations)\n\n        # Removes duplicate matches and pseudo-expressions in one statement\n        matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n        entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n        ents = None\n        sub_sections = None\n        sub_recent_dates = None\n        sub_history_dates = None\n\n        sections = []\n        if self.sections:\n            sections = [\n                Span(doc, section.start, section.end, label=\"ATCD\")\n                for section in doc.spans[\"sections\"]\n                if section.label_ in sections_history\n            ]\n\n        history_dates = []\n        recent_dates = []\n        if self.dates:\n            for date in doc.spans[\"dates\"]:\n                if date.label_ == \"relative\":\n                    if date._.date.direction.value == \"CURRENT\":\n                        if (\n                            (\n                                date._.date.year == 0\n                                and self.history_limit &gt;= timedelta(365)\n                            )\n                            or (\n                                date._.date.month == 0\n                                and self.history_limit &gt;= timedelta(30)\n                            )\n                            or (\n                                date._.date.week == 0\n                                and self.history_limit &gt;= timedelta(7)\n                            )\n                            or (date._.date.day == 0)\n                        ):\n                            recent_dates.append(\n                                Span(doc, date.start, date.end, label=\"relative_date\")\n                            )\n                    elif date._.date.direction.value == \"PAST\":\n                        if -date._.date.to_datetime() &gt;= self.history_limit:\n                            history_dates.append(\n                                Span(doc, date.start, date.end, label=\"relative_date\")\n                            )\n                        else:\n                            recent_dates.append(\n                                Span(doc, date.start, date.end, label=\"relative_date\")\n                            )\n                elif date.label_ == \"absolute\" and doc._.note_datetime:\n                    try:\n                        absolute_date = date._.date.to_datetime(\n                            note_datetime=note_datetime,\n                            infer_from_context=True,\n                            tz=\"Europe/Paris\",\n                            default_day=15,\n                        )\n                    except ValueError as e:\n                        absolute_date = None\n                        logger.warning(\n                            \"In doc {}, the following date {} raises this error: {}. \"\n                            \"Skipping this date.\",\n                            doc._.note_id,\n                            date._.date,\n                            e,\n                        )\n                    if absolute_date:\n                        if note_datetime.diff(absolute_date) &lt; self.history_limit:\n                            recent_dates.append(\n                                Span(doc, date.start, date.end, label=\"absolute_date\")\n                            )\n                        elif not (\n                            self.exclude_birthdate\n                            and birth_datetime\n                            and absolute_date == birth_datetime\n                        ):\n                            history_dates.append(\n                                Span(doc, date.start, date.end, label=\"absolute_date\")\n                            )\n\n        for start, end in boundaries:\n            ents, entities = consume_spans(\n                entities,\n                filter=lambda s: check_inclusion(s, start, end),\n                second_chance=ents,\n            )\n\n            sub_matches, matches = consume_spans(\n                matches, lambda s: start &lt;= s.start &lt; end\n            )\n\n            if self.sections:\n                sub_sections, sections = consume_spans(\n                    sections, lambda s: s.start &lt; end &lt;= s.end, sub_sections\n                )\n            if self.dates:\n                sub_recent_dates, recent_dates = consume_spans(\n                    recent_dates,\n                    lambda s: check_sent_inclusion(s, start, end),\n                    sub_recent_dates,\n                )\n                sub_history_dates, history_dates = consume_spans(\n                    history_dates,\n                    lambda s: check_sent_inclusion(s, start, end),\n                    sub_history_dates,\n                )\n\n                # Filter dates inside the boundaries only\n                if self.closest_dates_only:\n                    close_recent_dates = []\n                    close_history_dates = []\n                    if sub_recent_dates:\n                        close_recent_dates = [\n                            recent_date\n                            for recent_date in sub_recent_dates\n                            if check_inclusion(recent_date, start, end)\n                        ]\n                        if sub_history_dates:\n                            close_history_dates = [\n                                history_date\n                                for history_date in sub_history_dates\n                                if check_inclusion(history_date, start, end)\n                            ]\n                            # If no date inside the boundaries, get the closest\n                            if not close_recent_dates and not close_history_dates:\n                                min_distance_recent_date = min(\n                                    [\n                                        abs(sub_recent_date.start - start)\n                                        for sub_recent_date in sub_recent_dates\n                                    ]\n                                )\n                                min_distance_history_date = min(\n                                    [\n                                        abs(sub_history_date.start - start)\n                                        for sub_history_date in sub_history_dates\n                                    ]\n                                )\n                                if min_distance_recent_date &lt; min_distance_history_date:\n                                    close_recent_dates = [\n                                        min(\n                                            sub_recent_dates,\n                                            key=lambda x: abs(x.start - start),\n                                        )\n                                    ]\n                                else:\n                                    close_history_dates = [\n                                        min(\n                                            sub_history_dates,\n                                            key=lambda x: abs(x.start - start),\n                                        )\n                                    ]\n                        elif not close_recent_dates:\n                            close_recent_dates = [\n                                min(\n                                    sub_recent_dates,\n                                    key=lambda x: abs(x.start - start),\n                                )\n                            ]\n                    elif sub_history_dates:\n                        close_history_dates = [\n                            history_date\n                            for history_date in sub_history_dates\n                            if check_inclusion(history_date, start, end)\n                        ]\n                        # If no date inside the boundaries, get the closest\n                        if not close_history_dates:\n                            close_history_dates = [\n                                min(\n                                    sub_history_dates,\n                                    key=lambda x: abs(x.start - start),\n                                )\n                            ]\n\n            if self.on_ents_only and not ents:\n                continue\n\n            history_cues = get_spans(sub_matches, \"history\")\n            recent_cues = []\n\n            if self.sections:\n                history_cues.extend(sub_sections)\n\n            if self.dates:\n                history_cues.extend(\n                    close_history_dates\n                    if self.closest_dates_only\n                    else sub_history_dates\n                )\n                recent_cues.extend(\n                    close_recent_dates if self.closest_dates_only else sub_recent_dates\n                )\n\n            history = bool(history_cues) and not bool(recent_cues)\n\n            if not self.on_ents_only:\n                for token in doc[start:end]:\n                    token._.history = history\n\n            for ent in ents:\n                ent._.history = ent._.history or history\n\n                if self.explain:\n                    ent._.history_cues += history_cues\n                    ent._.recent_cues += recent_cues\n\n                if not self.on_ents_only and ent._.history:\n                    for token in ent:\n                        token._.history = True\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/history/history/#edsnlp.pipelines.qualifiers.history.history.History.process","title":"<code>process(doc)</code>","text":"<p>Finds entities related to history.</p> PARAMETER DESCRIPTION <code>doc</code> <p>spaCy Doc object</p> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p>spaCy Doc object, annotated for history</p> <p> TYPE: <code>Doc</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/history/history.py</code> <pre><code>def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Finds entities related to history.\n\n    Parameters\n    ----------\n    doc:\n        spaCy Doc object\n\n    Returns\n    -------\n    doc:\n        spaCy Doc object, annotated for history\n    \"\"\"\n\n    if doc._.note_datetime is not None:\n        try:\n            note_datetime = pendulum.instance(doc._.note_datetime)\n            note_datetime = note_datetime.set(tz=\"Europe/Paris\")\n        except ValueError:\n            logger.debug(\n                \"note_datetime must be a datetime objects. \"\n                \"Skipping history qualification from note_datetime.\"\n            )\n            note_datetime = None\n\n    if doc._.birth_datetime is not None:\n        try:\n            birth_datetime = pendulum.instance(doc._.birth_datetime)\n            birth_datetime = birth_datetime.set(tz=\"Europe/Paris\")\n        except ValueError:\n            logger.debug(\n                \"birth_datetime must be a datetime objects. \"\n                \"Skipping history qualification from birth date.\"\n            )\n            birth_datetime = None\n\n    matches = self.get_matches(doc)\n\n    terminations = get_spans(matches, \"termination\")\n    boundaries = self._boundaries(doc, terminations)\n\n    # Removes duplicate matches and pseudo-expressions in one statement\n    matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n    entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n    ents = None\n    sub_sections = None\n    sub_recent_dates = None\n    sub_history_dates = None\n\n    sections = []\n    if self.sections:\n        sections = [\n            Span(doc, section.start, section.end, label=\"ATCD\")\n            for section in doc.spans[\"sections\"]\n            if section.label_ in sections_history\n        ]\n\n    history_dates = []\n    recent_dates = []\n    if self.dates:\n        for date in doc.spans[\"dates\"]:\n            if date.label_ == \"relative\":\n                if date._.date.direction.value == \"CURRENT\":\n                    if (\n                        (\n                            date._.date.year == 0\n                            and self.history_limit &gt;= timedelta(365)\n                        )\n                        or (\n                            date._.date.month == 0\n                            and self.history_limit &gt;= timedelta(30)\n                        )\n                        or (\n                            date._.date.week == 0\n                            and self.history_limit &gt;= timedelta(7)\n                        )\n                        or (date._.date.day == 0)\n                    ):\n                        recent_dates.append(\n                            Span(doc, date.start, date.end, label=\"relative_date\")\n                        )\n                elif date._.date.direction.value == \"PAST\":\n                    if -date._.date.to_datetime() &gt;= self.history_limit:\n                        history_dates.append(\n                            Span(doc, date.start, date.end, label=\"relative_date\")\n                        )\n                    else:\n                        recent_dates.append(\n                            Span(doc, date.start, date.end, label=\"relative_date\")\n                        )\n            elif date.label_ == \"absolute\" and doc._.note_datetime:\n                try:\n                    absolute_date = date._.date.to_datetime(\n                        note_datetime=note_datetime,\n                        infer_from_context=True,\n                        tz=\"Europe/Paris\",\n                        default_day=15,\n                    )\n                except ValueError as e:\n                    absolute_date = None\n                    logger.warning(\n                        \"In doc {}, the following date {} raises this error: {}. \"\n                        \"Skipping this date.\",\n                        doc._.note_id,\n                        date._.date,\n                        e,\n                    )\n                if absolute_date:\n                    if note_datetime.diff(absolute_date) &lt; self.history_limit:\n                        recent_dates.append(\n                            Span(doc, date.start, date.end, label=\"absolute_date\")\n                        )\n                    elif not (\n                        self.exclude_birthdate\n                        and birth_datetime\n                        and absolute_date == birth_datetime\n                    ):\n                        history_dates.append(\n                            Span(doc, date.start, date.end, label=\"absolute_date\")\n                        )\n\n    for start, end in boundaries:\n        ents, entities = consume_spans(\n            entities,\n            filter=lambda s: check_inclusion(s, start, end),\n            second_chance=ents,\n        )\n\n        sub_matches, matches = consume_spans(\n            matches, lambda s: start &lt;= s.start &lt; end\n        )\n\n        if self.sections:\n            sub_sections, sections = consume_spans(\n                sections, lambda s: s.start &lt; end &lt;= s.end, sub_sections\n            )\n        if self.dates:\n            sub_recent_dates, recent_dates = consume_spans(\n                recent_dates,\n                lambda s: check_sent_inclusion(s, start, end),\n                sub_recent_dates,\n            )\n            sub_history_dates, history_dates = consume_spans(\n                history_dates,\n                lambda s: check_sent_inclusion(s, start, end),\n                sub_history_dates,\n            )\n\n            # Filter dates inside the boundaries only\n            if self.closest_dates_only:\n                close_recent_dates = []\n                close_history_dates = []\n                if sub_recent_dates:\n                    close_recent_dates = [\n                        recent_date\n                        for recent_date in sub_recent_dates\n                        if check_inclusion(recent_date, start, end)\n                    ]\n                    if sub_history_dates:\n                        close_history_dates = [\n                            history_date\n                            for history_date in sub_history_dates\n                            if check_inclusion(history_date, start, end)\n                        ]\n                        # If no date inside the boundaries, get the closest\n                        if not close_recent_dates and not close_history_dates:\n                            min_distance_recent_date = min(\n                                [\n                                    abs(sub_recent_date.start - start)\n                                    for sub_recent_date in sub_recent_dates\n                                ]\n                            )\n                            min_distance_history_date = min(\n                                [\n                                    abs(sub_history_date.start - start)\n                                    for sub_history_date in sub_history_dates\n                                ]\n                            )\n                            if min_distance_recent_date &lt; min_distance_history_date:\n                                close_recent_dates = [\n                                    min(\n                                        sub_recent_dates,\n                                        key=lambda x: abs(x.start - start),\n                                    )\n                                ]\n                            else:\n                                close_history_dates = [\n                                    min(\n                                        sub_history_dates,\n                                        key=lambda x: abs(x.start - start),\n                                    )\n                                ]\n                    elif not close_recent_dates:\n                        close_recent_dates = [\n                            min(\n                                sub_recent_dates,\n                                key=lambda x: abs(x.start - start),\n                            )\n                        ]\n                elif sub_history_dates:\n                    close_history_dates = [\n                        history_date\n                        for history_date in sub_history_dates\n                        if check_inclusion(history_date, start, end)\n                    ]\n                    # If no date inside the boundaries, get the closest\n                    if not close_history_dates:\n                        close_history_dates = [\n                            min(\n                                sub_history_dates,\n                                key=lambda x: abs(x.start - start),\n                            )\n                        ]\n\n        if self.on_ents_only and not ents:\n            continue\n\n        history_cues = get_spans(sub_matches, \"history\")\n        recent_cues = []\n\n        if self.sections:\n            history_cues.extend(sub_sections)\n\n        if self.dates:\n            history_cues.extend(\n                close_history_dates\n                if self.closest_dates_only\n                else sub_history_dates\n            )\n            recent_cues.extend(\n                close_recent_dates if self.closest_dates_only else sub_recent_dates\n            )\n\n        history = bool(history_cues) and not bool(recent_cues)\n\n        if not self.on_ents_only:\n            for token in doc[start:end]:\n                token._.history = history\n\n        for ent in ents:\n            ent._.history = ent._.history or history\n\n            if self.explain:\n                ent._.history_cues += history_cues\n                ent._.recent_cues += recent_cues\n\n            if not self.on_ents_only and ent._.history:\n                for token in ent:\n                    token._.history = True\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/history/patterns/","title":"<code>edsnlp.pipelines.qualifiers.history.patterns</code>","text":""},{"location":"reference/pipelines/qualifiers/hypothesis/","title":"<code>edsnlp.pipelines.qualifiers.hypothesis</code>","text":""},{"location":"reference/pipelines/qualifiers/hypothesis/factory/","title":"<code>edsnlp.pipelines.qualifiers.hypothesis.factory</code>","text":""},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/","title":"<code>edsnlp.pipelines.qualifiers.hypothesis.hypothesis</code>","text":""},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis","title":"<code>Hypothesis</code>","text":"<p>         Bases: <code>Qualifier</code></p> <p>Hypothesis detection with spaCy.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li>preceding hypothesis, ie cues that precede a hypothetic expression</li> <li>following hypothesis, ie cues that follow a hypothetic expression</li> <li>pseudo hypothesis : contain a hypothesis cue, but are not hypothesis   (eg \"pas de doute\"/\"no doubt\")</li> <li>hypothetic verbs : verbs indicating hypothesis (eg \"douter\")</li> <li>classic verbs conjugated to the conditional, thus indicating hypothesis</li> </ul> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>pseudo</code> <p>List of pseudo hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>preceding</code> <p>List of preceding hypothesis cues</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>following</code> <p>List of following hypothesis cues.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>verbs_hyp</code> <p>List of hypothetic verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>verbs_eds</code> <p>List of mainstream verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>filter_matches</code> <p>Whether to filter out overlapping matches.</p> <p> TYPE: <code>bool</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <p> TYPE: <code>bool</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> <code>regex</code> <p>A dictionnary of regex patterns.</p> <p> TYPE: <code>Optional[Dict[str, Union[List[str], str]]]</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py</code> <pre><code>class Hypothesis(Qualifier):\n\"\"\"\n    Hypothesis detection with spaCy.\n\n    The component looks for five kinds of expressions in the text :\n\n    - preceding hypothesis, ie cues that precede a hypothetic expression\n    - following hypothesis, ie cues that follow a hypothetic expression\n    - pseudo hypothesis : contain a hypothesis cue, but are not hypothesis\n      (eg \"pas de doute\"/\"no doubt\")\n    - hypothetic verbs : verbs indicating hypothesis (eg \"douter\")\n    - classic verbs conjugated to the conditional, thus indicating hypothesis\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    pseudo : Optional[List[str]]\n        List of pseudo hypothesis cues.\n    preceding : Optional[List[str]]\n        List of preceding hypothesis cues\n    following : Optional[List[str]]\n        List of following hypothesis cues.\n    verbs_hyp : Optional[List[str]]\n        List of hypothetic verbs.\n    verbs_eds : Optional[List[str]]\n        List of mainstream verbs.\n    filter_matches : bool\n        Whether to filter out overlapping matches.\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr'\n        we can also add a key for each regex.\n    on_ents_only : bool\n        Whether to look for matches around detected entities only.\n        Useful for faster inference in downstream tasks.\n    within_ents : bool\n        Whether to consider cues within entities.\n    explain : bool\n        Whether to keep track of cues for each entity.\n    regex : Optional[Dict[str, Union[List[str], str]]]\n        A dictionnary of regex patterns.\n    \"\"\"\n\n    defaults = dict(\n        following=following,\n        preceding=preceding,\n        pseudo=pseudo,\n        termination=termination,\n        verbs_eds=verbs_eds,\n        verbs_hyp=verbs_hyp,\n    )\n\n    def __init__(\n        self,\n        nlp: Language,\n        attr: str,\n        pseudo: Optional[List[str]],\n        preceding: Optional[List[str]],\n        following: Optional[List[str]],\n        termination: Optional[List[str]],\n        verbs_eds: Optional[List[str]],\n        verbs_hyp: Optional[List[str]],\n        on_ents_only: bool,\n        within_ents: bool,\n        explain: bool,\n    ):\n\n        terms = self.get_defaults(\n            pseudo=pseudo,\n            preceding=preceding,\n            following=following,\n            termination=termination,\n            verbs_eds=verbs_eds,\n            verbs_hyp=verbs_hyp,\n        )\n        terms[\"verbs_preceding\"], terms[\"verbs_following\"] = self.load_verbs(\n            verbs_hyp=terms.pop(\"verbs_hyp\"),\n            verbs_eds=terms.pop(\"verbs_eds\"),\n        )\n\n        super().__init__(\n            nlp=nlp,\n            attr=attr,\n            on_ents_only=on_ents_only,\n            explain=explain,\n            **terms,\n        )\n\n        self.within_ents = within_ents\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n        if not Token.has_extension(\"hypothesis\"):\n            Token.set_extension(\"hypothesis\", default=False)\n\n        if not Token.has_extension(\"hypothesis_\"):\n            Token.set_extension(\n                \"hypothesis_\",\n                getter=lambda token: \"HYP\" if token._.hypothesis else \"CERT\",\n            )\n\n        if not Span.has_extension(\"hypothesis\"):\n            Span.set_extension(\"hypothesis\", default=False)\n\n        if not Span.has_extension(\"hypothesis_\"):\n            Span.set_extension(\n                \"hypothesis_\",\n                getter=lambda span: \"HYP\" if span._.hypothesis else \"CERT\",\n            )\n\n        if not Span.has_extension(\"hypothesis_cues\"):\n            Span.set_extension(\"hypothesis_cues\", default=[])\n\n        if not Doc.has_extension(\"hypothesis\"):\n            Doc.set_extension(\"hypothesis\", default=[])\n\n    def load_verbs(\n        self,\n        verbs_hyp: List[str],\n        verbs_eds: List[str],\n    ) -&gt; List[str]:\n\"\"\"\n        Conjugate \"classic\" verbs to conditional, and add hypothesis\n        verbs conjugated to all tenses.\n\n        Parameters\n        ----------\n        verbs_hyp: List of verbs that specifically imply an hypothesis.\n        verbs_eds: List of general verbs.\n\n        Returns\n        -------\n        list of hypothesis verbs conjugated at all tenses and classic\n        verbs conjugated to conditional.\n        \"\"\"\n\n        classic_verbs = get_verbs(verbs_eds)\n        classic_verbs = classic_verbs.loc[classic_verbs[\"mode\"] == \"Conditionnel\"]\n        list_classic_verbs = list(classic_verbs[\"term\"].unique())\n\n        hypo_verbs = get_verbs(verbs_hyp)\n        list_hypo_verbs_preceding = list(hypo_verbs[\"term\"].unique())\n\n        hypo_verbs_following = hypo_verbs.loc[hypo_verbs[\"tense\"] == \"Participe Pass\u00e9\"]\n        list_hypo_verbs_following = list(hypo_verbs_following[\"term\"].unique())\n\n        return (\n            list_hypo_verbs_preceding + list_classic_verbs,\n            list_hypo_verbs_following,\n        )\n\n    def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Finds entities related to hypothesis.\n\n        Parameters\n        ----------\n        doc: spaCy Doc object\n\n        Returns\n        -------\n        doc: spaCy Doc object, annotated for hypothesis\n        \"\"\"\n\n        matches = self.get_matches(doc)\n\n        terminations = get_spans(matches, \"termination\")\n        boundaries = self._boundaries(doc, terminations)\n\n        # Removes duplicate matches and pseudo-expressions in one statement\n        matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n        entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n        ents = None\n\n        for start, end in boundaries:\n\n            ents, entities = consume_spans(\n                entities,\n                filter=lambda s: check_inclusion(s, start, end),\n                second_chance=ents,\n            )\n\n            sub_matches, matches = consume_spans(\n                matches, lambda s: start &lt;= s.start &lt; end\n            )\n\n            if self.on_ents_only and not ents:\n                continue\n\n            sub_preceding = get_spans(sub_matches, \"preceding\")\n            sub_following = get_spans(sub_matches, \"following\")\n            sub_preceding += get_spans(sub_matches, \"verbs_preceding\")\n            sub_following += get_spans(sub_matches, \"verbs_following\")\n\n            if not sub_preceding + sub_following:\n                continue\n\n            if not self.on_ents_only:\n                for token in doc[start:end]:\n                    token._.hypothesis = any(\n                        m.end &lt;= token.i for m in sub_preceding\n                    ) or any(m.start &gt; token.i for m in sub_following)\n\n            for ent in ents:\n\n                if self.within_ents:\n                    cues = [m for m in sub_preceding if m.end &lt;= ent.end]\n                    cues += [m for m in sub_following if m.start &gt;= ent.start]\n                else:\n                    cues = [m for m in sub_preceding if m.end &lt;= ent.start]\n                    cues += [m for m in sub_following if m.start &gt;= ent.end]\n\n                hypothesis = ent._.hypothesis or bool(cues)\n\n                ent._.hypothesis = hypothesis\n\n                if self.explain and hypothesis:\n                    ent._.hypothesis_cues += cues\n\n                if not self.on_ents_only and hypothesis:\n                    for token in ent:\n                        token._.hypothesis = True\n\n        return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.load_verbs","title":"<code>load_verbs(verbs_hyp, verbs_eds)</code>","text":"<p>Conjugate \"classic\" verbs to conditional, and add hypothesis verbs conjugated to all tenses.</p> PARAMETER DESCRIPTION <code>verbs_hyp</code> <p> TYPE: <code>List[str]</code> </p> <code>verbs_eds</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list of hypothesis verbs conjugated at all tenses and classic</code> <code>verbs conjugated to conditional.</code> Source code in <code>edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py</code> <pre><code>def load_verbs(\n    self,\n    verbs_hyp: List[str],\n    verbs_eds: List[str],\n) -&gt; List[str]:\n\"\"\"\n    Conjugate \"classic\" verbs to conditional, and add hypothesis\n    verbs conjugated to all tenses.\n\n    Parameters\n    ----------\n    verbs_hyp: List of verbs that specifically imply an hypothesis.\n    verbs_eds: List of general verbs.\n\n    Returns\n    -------\n    list of hypothesis verbs conjugated at all tenses and classic\n    verbs conjugated to conditional.\n    \"\"\"\n\n    classic_verbs = get_verbs(verbs_eds)\n    classic_verbs = classic_verbs.loc[classic_verbs[\"mode\"] == \"Conditionnel\"]\n    list_classic_verbs = list(classic_verbs[\"term\"].unique())\n\n    hypo_verbs = get_verbs(verbs_hyp)\n    list_hypo_verbs_preceding = list(hypo_verbs[\"term\"].unique())\n\n    hypo_verbs_following = hypo_verbs.loc[hypo_verbs[\"tense\"] == \"Participe Pass\u00e9\"]\n    list_hypo_verbs_following = list(hypo_verbs_following[\"term\"].unique())\n\n    return (\n        list_hypo_verbs_preceding + list_classic_verbs,\n        list_hypo_verbs_following,\n    )\n</code></pre>"},{"location":"reference/pipelines/qualifiers/hypothesis/hypothesis/#edsnlp.pipelines.qualifiers.hypothesis.hypothesis.Hypothesis.process","title":"<code>process(doc)</code>","text":"<p>Finds entities related to hypothesis.</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy Doc object, annotated for hypothesis</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/hypothesis/hypothesis.py</code> <pre><code>def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Finds entities related to hypothesis.\n\n    Parameters\n    ----------\n    doc: spaCy Doc object\n\n    Returns\n    -------\n    doc: spaCy Doc object, annotated for hypothesis\n    \"\"\"\n\n    matches = self.get_matches(doc)\n\n    terminations = get_spans(matches, \"termination\")\n    boundaries = self._boundaries(doc, terminations)\n\n    # Removes duplicate matches and pseudo-expressions in one statement\n    matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n    entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n    ents = None\n\n    for start, end in boundaries:\n\n        ents, entities = consume_spans(\n            entities,\n            filter=lambda s: check_inclusion(s, start, end),\n            second_chance=ents,\n        )\n\n        sub_matches, matches = consume_spans(\n            matches, lambda s: start &lt;= s.start &lt; end\n        )\n\n        if self.on_ents_only and not ents:\n            continue\n\n        sub_preceding = get_spans(sub_matches, \"preceding\")\n        sub_following = get_spans(sub_matches, \"following\")\n        sub_preceding += get_spans(sub_matches, \"verbs_preceding\")\n        sub_following += get_spans(sub_matches, \"verbs_following\")\n\n        if not sub_preceding + sub_following:\n            continue\n\n        if not self.on_ents_only:\n            for token in doc[start:end]:\n                token._.hypothesis = any(\n                    m.end &lt;= token.i for m in sub_preceding\n                ) or any(m.start &gt; token.i for m in sub_following)\n\n        for ent in ents:\n\n            if self.within_ents:\n                cues = [m for m in sub_preceding if m.end &lt;= ent.end]\n                cues += [m for m in sub_following if m.start &gt;= ent.start]\n            else:\n                cues = [m for m in sub_preceding if m.end &lt;= ent.start]\n                cues += [m for m in sub_following if m.start &gt;= ent.end]\n\n            hypothesis = ent._.hypothesis or bool(cues)\n\n            ent._.hypothesis = hypothesis\n\n            if self.explain and hypothesis:\n                ent._.hypothesis_cues += cues\n\n            if not self.on_ents_only and hypothesis:\n                for token in ent:\n                    token._.hypothesis = True\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/hypothesis/patterns/","title":"<code>edsnlp.pipelines.qualifiers.hypothesis.patterns</code>","text":""},{"location":"reference/pipelines/qualifiers/negation/","title":"<code>edsnlp.pipelines.qualifiers.negation</code>","text":""},{"location":"reference/pipelines/qualifiers/negation/factory/","title":"<code>edsnlp.pipelines.qualifiers.negation.factory</code>","text":""},{"location":"reference/pipelines/qualifiers/negation/negation/","title":"<code>edsnlp.pipelines.qualifiers.negation.negation</code>","text":""},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation","title":"<code>Negation</code>","text":"<p>         Bases: <code>Qualifier</code></p> <p>Implements the NegEx algorithm.</p> <p>The component looks for five kinds of expressions in the text :</p> <ul> <li> <p>preceding negations, ie cues that precede a negated expression</p> </li> <li> <p>following negations, ie cues that follow a negated expression</p> </li> <li> <p>pseudo negations : contain a negation cue, but are not negations   (eg \"pas de doute\"/\"no doubt\")</p> </li> <li> <p>negation verbs, ie verbs that indicate a negation</p> </li> <li> <p>terminations, ie words that delimit propositions.   The negation spans from the preceding cue to the termination.</p> </li> </ul> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>attr</code> <p>spaCy's attribute to use</p> <p> TYPE: <code>str</code> </p> <code>pseudo</code> <p>List of pseudo negation terms.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>preceding</code> <p>List of preceding negation terms</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>following</code> <p>List of following negation terms.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>termination</code> <p>List of termination terms.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>verbs</code> <p>List of negation verbs.</p> <p> TYPE: <code>Optional[List[str]]</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <p> TYPE: <code>bool</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/negation/negation.py</code> <pre><code>class Negation(Qualifier):\n\"\"\"\n    Implements the NegEx algorithm.\n\n    The component looks for five kinds of expressions in the text :\n\n    - preceding negations, ie cues that precede a negated expression\n\n    - following negations, ie cues that follow a negated expression\n\n    - pseudo negations : contain a negation cue, but are not negations\n      (eg \"pas de doute\"/\"no doubt\")\n\n    - negation verbs, ie verbs that indicate a negation\n\n    - terminations, ie words that delimit propositions.\n      The negation spans from the preceding cue to the termination.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    attr : str\n        spaCy's attribute to use\n    pseudo : Optional[List[str]]\n        List of pseudo negation terms.\n    preceding : Optional[List[str]]\n        List of preceding negation terms\n    following : Optional[List[str]]\n        List of following negation terms.\n    termination : Optional[List[str]]\n        List of termination terms.\n    verbs : Optional[List[str]]\n        List of negation verbs.\n    on_ents_only : bool\n        Whether to look for matches around detected entities only.\n        Useful for faster inference in downstream tasks.\n    within_ents : bool\n        Whether to consider cues within entities.\n    explain : bool\n        Whether to keep track of cues for each entity.\n    \"\"\"\n\n    defaults = dict(\n        following=following,\n        preceding=preceding,\n        pseudo=pseudo,\n        verbs=verbs,\n        termination=termination,\n    )\n\n    def __init__(\n        self,\n        nlp: Language,\n        attr: str,\n        pseudo: Optional[List[str]],\n        preceding: Optional[List[str]],\n        following: Optional[List[str]],\n        termination: Optional[List[str]],\n        verbs: Optional[List[str]],\n        on_ents_only: bool,\n        within_ents: bool,\n        explain: bool,\n    ):\n\n        terms = self.get_defaults(\n            pseudo=pseudo,\n            preceding=preceding,\n            following=following,\n            termination=termination,\n            verbs=verbs,\n        )\n        terms[\"verbs_preceding\"], terms[\"verbs_following\"] = self.load_verbs(\n            terms[\"verbs\"]\n        )\n\n        super().__init__(\n            nlp=nlp,\n            attr=attr,\n            on_ents_only=on_ents_only,\n            explain=explain,\n            **terms,\n        )\n\n        self.within_ents = within_ents\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cl) -&gt; None:\n\n        if not Token.has_extension(\"negation\"):\n            Token.set_extension(\"negation\", default=False)\n\n        if not Token.has_extension(\"negated\"):\n            Token.set_extension(\n                \"negated\", getter=deprecated_getter_factory(\"negated\", \"negation\")\n            )\n\n        if not Token.has_extension(\"negation_\"):\n            Token.set_extension(\n                \"negation_\",\n                getter=lambda token: \"NEG\" if token._.negation else \"AFF\",\n            )\n\n        if not Token.has_extension(\"polarity_\"):\n            Token.set_extension(\n                \"polarity_\",\n                getter=deprecated_getter_factory(\"polarity_\", \"negation_\"),\n            )\n\n        if not Span.has_extension(\"negation\"):\n            Span.set_extension(\"negation\", default=False)\n\n        if not Span.has_extension(\"negated\"):\n            Span.set_extension(\n                \"negated\", getter=deprecated_getter_factory(\"negated\", \"negation\")\n            )\n\n        if not Span.has_extension(\"negation_cues\"):\n            Span.set_extension(\"negation_cues\", default=[])\n\n        if not Span.has_extension(\"negation_\"):\n            Span.set_extension(\n                \"negation_\",\n                getter=lambda span: \"NEG\" if span._.negation else \"AFF\",\n            )\n\n        if not Span.has_extension(\"polarity_\"):\n            Span.set_extension(\n                \"polarity_\",\n                getter=deprecated_getter_factory(\"polarity_\", \"negation_\"),\n            )\n\n        if not Doc.has_extension(\"negations\"):\n            Doc.set_extension(\"negations\", default=[])\n\n    def load_verbs(self, verbs: List[str]) -&gt; List[str]:\n\"\"\"\n        Conjugate negating verbs to specific tenses.\n\n        Parameters\n        ----------\n        verbs: list of negating verbs to conjugate\n\n        Returns\n        -------\n        list_neg_verbs_preceding: List of conjugated negating verbs preceding entities.\n        list_neg_verbs_following: List of conjugated negating verbs following entities.\n        \"\"\"\n\n        neg_verbs = get_verbs(verbs)\n\n        neg_verbs_preceding = neg_verbs.loc[\n            ((neg_verbs[\"mode\"] == \"Indicatif\") &amp; (neg_verbs[\"tense\"] == \"Pr\u00e9sent\"))\n            | (neg_verbs[\"tense\"] == \"Participe Pr\u00e9sent\")\n            | (neg_verbs[\"tense\"] == \"Participe Pass\u00e9\")\n            | (neg_verbs[\"tense\"] == \"Infinitif Pr\u00e9sent\")\n        ]\n        neg_verbs_following = neg_verbs.loc[neg_verbs[\"tense\"] == \"Participe Pass\u00e9\"]\n        list_neg_verbs_preceding = list(neg_verbs_preceding[\"term\"].unique())\n        list_neg_verbs_following = list(neg_verbs_following[\"term\"].unique())\n\n        return (list_neg_verbs_preceding, list_neg_verbs_following)\n\n    def annotate_entity(\n        self,\n        ent: Span,\n        sub_preceding: List[Span],\n        sub_following: List[Span],\n    ) -&gt; None:\n\"\"\"\n        Annotate entities using preceding and following negations.\n\n        Parameters\n        ----------\n        ent : Span\n            Entity to annotate\n        sub_preceding : List[Span]\n            List of preceding negations cues\n        sub_following : List[Span]\n            List of following negations cues\n        \"\"\"\n        if self.within_ents:\n            cues = [m for m in sub_preceding if m.end &lt;= ent.end]\n            cues += [m for m in sub_following if m.start &gt;= ent.start]\n        else:\n            cues = [m for m in sub_preceding if m.end &lt;= ent.start]\n            cues += [m for m in sub_following if m.start &gt;= ent.end]\n\n        negation = ent._.negation or bool(cues)\n\n        ent._.negation = negation\n\n        if self.explain and negation:\n            ent._.negation_cues += cues\n\n        if not self.on_ents_only and negation:\n            for token in ent:\n                token._.negation = True\n\n    def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Finds entities related to negation.\n\n        Parameters\n        ----------\n        doc: spaCy `Doc` object\n\n        Returns\n        -------\n        doc: spaCy `Doc` object, annotated for negation\n        \"\"\"\n\n        matches = self.get_matches(doc)\n\n        terminations = get_spans(matches, \"termination\")\n        boundaries = self._boundaries(doc, terminations)\n\n        entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n        ents = None\n\n        # Removes duplicate matches and pseudo-expressions in one statement\n        matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n        for start, end in boundaries:\n\n            ents, entities = consume_spans(\n                entities,\n                filter=lambda s: check_inclusion(s, start, end),\n                second_chance=ents,\n            )\n\n            sub_matches, matches = consume_spans(\n                matches, lambda s: start &lt;= s.start &lt; end\n            )\n\n            if self.on_ents_only and not ents:\n                continue\n\n            sub_preceding = get_spans(sub_matches, \"preceding\")\n            sub_following = get_spans(sub_matches, \"following\")\n            # Verbs preceding negated content\n            sub_preceding += get_spans(sub_matches, \"verbs_preceding\")\n            # Verbs following negated content\n            sub_following += get_spans(sub_matches, \"verbs_following\")\n\n            if not sub_preceding + sub_following:\n                continue\n\n            if not self.on_ents_only:\n                for token in doc[start:end]:\n                    token._.negation = any(\n                        m.end &lt;= token.i for m in sub_preceding\n                    ) or any(m.start &gt; token.i for m in sub_following)\n\n            for ent in ents:\n                self.annotate_entity(\n                    ent=ent,\n                    sub_preceding=sub_preceding,\n                    sub_following=sub_following,\n                )\n\n        return doc\n\n    def __call__(self, doc: Doc) -&gt; Doc:\n        return self.process(doc)\n</code></pre>"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.load_verbs","title":"<code>load_verbs(verbs)</code>","text":"<p>Conjugate negating verbs to specific tenses.</p> PARAMETER DESCRIPTION <code>verbs</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list_neg_verbs_preceding</code> <p> TYPE: <code>List of conjugated negating verbs preceding entities.</code> </p> <code>list_neg_verbs_following</code> <p> TYPE: <code>List of conjugated negating verbs following entities.</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/negation/negation.py</code> <pre><code>def load_verbs(self, verbs: List[str]) -&gt; List[str]:\n\"\"\"\n    Conjugate negating verbs to specific tenses.\n\n    Parameters\n    ----------\n    verbs: list of negating verbs to conjugate\n\n    Returns\n    -------\n    list_neg_verbs_preceding: List of conjugated negating verbs preceding entities.\n    list_neg_verbs_following: List of conjugated negating verbs following entities.\n    \"\"\"\n\n    neg_verbs = get_verbs(verbs)\n\n    neg_verbs_preceding = neg_verbs.loc[\n        ((neg_verbs[\"mode\"] == \"Indicatif\") &amp; (neg_verbs[\"tense\"] == \"Pr\u00e9sent\"))\n        | (neg_verbs[\"tense\"] == \"Participe Pr\u00e9sent\")\n        | (neg_verbs[\"tense\"] == \"Participe Pass\u00e9\")\n        | (neg_verbs[\"tense\"] == \"Infinitif Pr\u00e9sent\")\n    ]\n    neg_verbs_following = neg_verbs.loc[neg_verbs[\"tense\"] == \"Participe Pass\u00e9\"]\n    list_neg_verbs_preceding = list(neg_verbs_preceding[\"term\"].unique())\n    list_neg_verbs_following = list(neg_verbs_following[\"term\"].unique())\n\n    return (list_neg_verbs_preceding, list_neg_verbs_following)\n</code></pre>"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.annotate_entity","title":"<code>annotate_entity(ent, sub_preceding, sub_following)</code>","text":"<p>Annotate entities using preceding and following negations.</p> PARAMETER DESCRIPTION <code>ent</code> <p>Entity to annotate</p> <p> TYPE: <code>Span</code> </p> <code>sub_preceding</code> <p>List of preceding negations cues</p> <p> TYPE: <code>List[Span]</code> </p> <code>sub_following</code> <p>List of following negations cues</p> <p> TYPE: <code>List[Span]</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/negation/negation.py</code> <pre><code>def annotate_entity(\n    self,\n    ent: Span,\n    sub_preceding: List[Span],\n    sub_following: List[Span],\n) -&gt; None:\n\"\"\"\n    Annotate entities using preceding and following negations.\n\n    Parameters\n    ----------\n    ent : Span\n        Entity to annotate\n    sub_preceding : List[Span]\n        List of preceding negations cues\n    sub_following : List[Span]\n        List of following negations cues\n    \"\"\"\n    if self.within_ents:\n        cues = [m for m in sub_preceding if m.end &lt;= ent.end]\n        cues += [m for m in sub_following if m.start &gt;= ent.start]\n    else:\n        cues = [m for m in sub_preceding if m.end &lt;= ent.start]\n        cues += [m for m in sub_following if m.start &gt;= ent.end]\n\n    negation = ent._.negation or bool(cues)\n\n    ent._.negation = negation\n\n    if self.explain and negation:\n        ent._.negation_cues += cues\n\n    if not self.on_ents_only and negation:\n        for token in ent:\n            token._.negation = True\n</code></pre>"},{"location":"reference/pipelines/qualifiers/negation/negation/#edsnlp.pipelines.qualifiers.negation.negation.Negation.process","title":"<code>process(doc)</code>","text":"<p>Finds entities related to negation.</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/negation/negation.py</code> <pre><code>def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Finds entities related to negation.\n\n    Parameters\n    ----------\n    doc: spaCy `Doc` object\n\n    Returns\n    -------\n    doc: spaCy `Doc` object, annotated for negation\n    \"\"\"\n\n    matches = self.get_matches(doc)\n\n    terminations = get_spans(matches, \"termination\")\n    boundaries = self._boundaries(doc, terminations)\n\n    entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n    ents = None\n\n    # Removes duplicate matches and pseudo-expressions in one statement\n    matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n    for start, end in boundaries:\n\n        ents, entities = consume_spans(\n            entities,\n            filter=lambda s: check_inclusion(s, start, end),\n            second_chance=ents,\n        )\n\n        sub_matches, matches = consume_spans(\n            matches, lambda s: start &lt;= s.start &lt; end\n        )\n\n        if self.on_ents_only and not ents:\n            continue\n\n        sub_preceding = get_spans(sub_matches, \"preceding\")\n        sub_following = get_spans(sub_matches, \"following\")\n        # Verbs preceding negated content\n        sub_preceding += get_spans(sub_matches, \"verbs_preceding\")\n        # Verbs following negated content\n        sub_following += get_spans(sub_matches, \"verbs_following\")\n\n        if not sub_preceding + sub_following:\n            continue\n\n        if not self.on_ents_only:\n            for token in doc[start:end]:\n                token._.negation = any(\n                    m.end &lt;= token.i for m in sub_preceding\n                ) or any(m.start &gt; token.i for m in sub_following)\n\n        for ent in ents:\n            self.annotate_entity(\n                ent=ent,\n                sub_preceding=sub_preceding,\n                sub_following=sub_following,\n            )\n\n    return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/negation/patterns/","title":"<code>edsnlp.pipelines.qualifiers.negation.patterns</code>","text":""},{"location":"reference/pipelines/qualifiers/reported_speech/","title":"<code>edsnlp.pipelines.qualifiers.reported_speech</code>","text":""},{"location":"reference/pipelines/qualifiers/reported_speech/factory/","title":"<code>edsnlp.pipelines.qualifiers.reported_speech.factory</code>","text":""},{"location":"reference/pipelines/qualifiers/reported_speech/patterns/","title":"<code>edsnlp.pipelines.qualifiers.reported_speech.patterns</code>","text":""},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/","title":"<code>edsnlp.pipelines.qualifiers.reported_speech.reported_speech</code>","text":""},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech","title":"<code>ReportedSpeech</code>","text":"<p>         Bases: <code>Qualifier</code></p> <p>Implements a reported speech detection algorithm.</p> <p>The components looks for terms indicating patient statements, and quotations to detect patient speech.</p> PARAMETER DESCRIPTION <code>nlp</code> <p>spaCy nlp pipeline to use for matching.</p> <p> TYPE: <code>Language</code> </p> <code>quotation</code> <p>String gathering all quotation cues.</p> <p> TYPE: <code>str</code> </p> <code>verbs</code> <p>List of reported speech verbs.</p> <p> TYPE: <code>List[str]</code> </p> <code>following</code> <p>List of terms following a reported speech.</p> <p> TYPE: <code>List[str]</code> </p> <code>preceding</code> <p>List of terms preceding a reported speech.</p> <p> TYPE: <code>List[str]</code> </p> <code>filter_matches</code> <p>Whether to filter out overlapping matches.</p> <p> TYPE: <code>bool</code> </p> <code>attr</code> <p>spaCy's attribute to use: a string with the value \"TEXT\" or \"NORM\", or a dict with the key 'term_attr' we can also add a key for each regex.</p> <p> TYPE: <code>str</code> </p> <code>on_ents_only</code> <p>Whether to look for matches around detected entities only. Useful for faster inference in downstream tasks.</p> <p> TYPE: <code>bool</code> </p> <code>within_ents</code> <p>Whether to consider cues within entities.</p> <p> TYPE: <code>bool</code> </p> <code>explain</code> <p>Whether to keep track of cues for each entity.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py</code> <pre><code>class ReportedSpeech(Qualifier):\n\"\"\"\n    Implements a reported speech detection algorithm.\n\n    The components looks for terms indicating patient statements,\n    and quotations to detect patient speech.\n\n    Parameters\n    ----------\n    nlp : Language\n        spaCy nlp pipeline to use for matching.\n    quotation : str\n        String gathering all quotation cues.\n    verbs : List[str]\n        List of reported speech verbs.\n    following : List[str]\n        List of terms following a reported speech.\n    preceding : List[str]\n        List of terms preceding a reported speech.\n    filter_matches : bool\n        Whether to filter out overlapping matches.\n    attr : str\n        spaCy's attribute to use:\n        a string with the value \"TEXT\" or \"NORM\",\n        or a dict with the key 'term_attr'\n        we can also add a key for each regex.\n    on_ents_only : bool\n        Whether to look for matches around detected entities only.\n        Useful for faster inference in downstream tasks.\n    within_ents : bool\n        Whether to consider cues within entities.\n    explain : bool\n        Whether to keep track of cues for each entity.\n    \"\"\"\n\n    defaults = dict(\n        following=following,\n        preceding=preceding,\n        verbs=verbs,\n        quotation=quotation,\n    )\n\n    def __init__(\n        self,\n        nlp: Language,\n        attr: str,\n        pseudo: Optional[List[str]],\n        preceding: Optional[List[str]],\n        following: Optional[List[str]],\n        quotation: Optional[List[str]],\n        verbs: Optional[List[str]],\n        on_ents_only: bool,\n        within_ents: bool,\n        explain: bool,\n    ):\n\n        terms = self.get_defaults(\n            pseudo=pseudo,\n            preceding=preceding,\n            following=following,\n            quotation=quotation,\n            verbs=verbs,\n        )\n        terms[\"verbs\"] = self.load_verbs(terms[\"verbs\"])\n\n        quotation = terms.pop(\"quotation\")\n\n        super().__init__(\n            nlp=nlp,\n            attr=attr,\n            on_ents_only=on_ents_only,\n            explain=explain,\n            **terms,\n        )\n\n        self.regex_matcher = RegexMatcher(attr=attr)\n        self.regex_matcher.build_patterns(dict(quotation=quotation))\n\n        self.within_ents = within_ents\n\n        self.set_extensions()\n\n    @classmethod\n    def set_extensions(cls) -&gt; None:\n\n        if not Token.has_extension(\"reported_speech\"):\n            Token.set_extension(\"reported_speech\", default=False)\n\n        if not Token.has_extension(\"reported_speech_\"):\n            Token.set_extension(\n                \"reported_speech_\",\n                getter=lambda token: \"REPORTED\"\n                if token._.reported_speech\n                else \"DIRECT\",\n            )\n\n        if not Span.has_extension(\"reported_speech\"):\n            Span.set_extension(\"reported_speech\", default=False)\n\n        if not Span.has_extension(\"reported_speech_\"):\n            Span.set_extension(\n                \"reported_speech_\",\n                getter=lambda span: \"REPORTED\" if span._.reported_speech else \"DIRECT\",\n            )\n\n        if not Span.has_extension(\"reported_speech_cues\"):\n            Span.set_extension(\"reported_speech_cues\", default=[])\n\n        if not Doc.has_extension(\"rspeechs\"):\n            Doc.set_extension(\"rspeechs\", default=[])\n\n    def load_verbs(self, verbs: List[str]) -&gt; List[str]:\n\"\"\"\n        Conjugate reporting verbs to specific tenses (trhid person)\n\n        Parameters\n        ----------\n        verbs: list of reporting verbs to conjugate\n\n        Returns\n        -------\n        list_rep_verbs: List of reporting verbs conjugated to specific tenses.\n        \"\"\"\n\n        rep_verbs = get_verbs(verbs)\n\n        rep_verbs = rep_verbs.loc[\n            (\n                (rep_verbs[\"mode\"] == \"Indicatif\")\n                &amp; (rep_verbs[\"tense\"] == \"Pr\u00e9sent\")\n                &amp; (rep_verbs[\"person\"].isin([\"3s\", \"3p\"]))\n            )\n            | (rep_verbs[\"tense\"] == \"Participe Pr\u00e9sent\")\n            | (rep_verbs[\"tense\"] == \"Participe Pass\u00e9\")\n        ]\n\n        list_rep_verbs = list(rep_verbs[\"term\"].unique())\n\n        return list_rep_verbs\n\n    def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n        Finds entities related to reported speech.\n\n        Parameters\n        ----------\n        doc: spaCy Doc object\n\n        Returns\n        -------\n        doc: spaCy Doc object, annotated for negation\n        \"\"\"\n\n        matches = self.get_matches(doc)\n        matches += list(self.regex_matcher(doc, as_spans=True))\n\n        boundaries = self._boundaries(doc)\n\n        entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n        ents = None\n\n        # Removes duplicate matches and pseudo-expressions in one statement\n        matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n        for start, end in boundaries:\n\n            ents, entities = consume_spans(\n                entities,\n                filter=lambda s: check_inclusion(s, start, end),\n                second_chance=ents,\n            )\n\n            sub_matches, matches = consume_spans(\n                matches, lambda s: start &lt;= s.start &lt; end\n            )\n\n            if self.on_ents_only and not ents:\n                continue\n\n            sub_preceding = get_spans(sub_matches, \"preceding\")\n            sub_following = get_spans(sub_matches, \"following\")\n            sub_verbs = get_spans(sub_matches, \"verbs\")\n            sub_quotation = get_spans(sub_matches, \"quotation\")\n\n            if not sub_preceding + sub_following + sub_verbs + sub_quotation:\n                continue\n\n            if not self.on_ents_only:\n                for token in doc[start:end]:\n                    token._.reported_speech = (\n                        any(m.end &lt;= token.i for m in sub_preceding + sub_verbs)\n                        or any(m.start &gt; token.i for m in sub_following)\n                        or any(\n                            ((m.start &lt; token.i) &amp; (m.end &gt; token.i + 1))\n                            for m in sub_quotation\n                        )\n                    )\n            for ent in ents:\n\n                if self.within_ents:\n                    cues = [m for m in sub_preceding + sub_verbs if m.end &lt;= ent.end]\n                    cues += [m for m in sub_following if m.start &gt;= ent.start]\n                else:\n                    cues = [m for m in sub_preceding + sub_verbs if m.end &lt;= ent.start]\n                    cues += [m for m in sub_following if m.start &gt;= ent.end]\n\n                cues += [\n                    m\n                    for m in sub_quotation\n                    if (m.start &lt; ent.start) &amp; (m.end &gt; ent.end)\n                ]\n\n                reported_speech = ent._.reported_speech or bool(cues)\n                ent._.reported_speech = reported_speech\n\n                if self.explain:\n                    ent._.reported_speech_cues += cues\n\n                if not self.on_ents_only and reported_speech:\n                    for token in ent:\n                        token._.reported_speech = True\n        return doc\n</code></pre>"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.load_verbs","title":"<code>load_verbs(verbs)</code>","text":"<p>Conjugate reporting verbs to specific tenses (trhid person)</p> PARAMETER DESCRIPTION <code>verbs</code> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>list_rep_verbs</code> <p> TYPE: <code>List of reporting verbs conjugated to specific tenses.</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py</code> <pre><code>def load_verbs(self, verbs: List[str]) -&gt; List[str]:\n\"\"\"\n    Conjugate reporting verbs to specific tenses (trhid person)\n\n    Parameters\n    ----------\n    verbs: list of reporting verbs to conjugate\n\n    Returns\n    -------\n    list_rep_verbs: List of reporting verbs conjugated to specific tenses.\n    \"\"\"\n\n    rep_verbs = get_verbs(verbs)\n\n    rep_verbs = rep_verbs.loc[\n        (\n            (rep_verbs[\"mode\"] == \"Indicatif\")\n            &amp; (rep_verbs[\"tense\"] == \"Pr\u00e9sent\")\n            &amp; (rep_verbs[\"person\"].isin([\"3s\", \"3p\"]))\n        )\n        | (rep_verbs[\"tense\"] == \"Participe Pr\u00e9sent\")\n        | (rep_verbs[\"tense\"] == \"Participe Pass\u00e9\")\n    ]\n\n    list_rep_verbs = list(rep_verbs[\"term\"].unique())\n\n    return list_rep_verbs\n</code></pre>"},{"location":"reference/pipelines/qualifiers/reported_speech/reported_speech/#edsnlp.pipelines.qualifiers.reported_speech.reported_speech.ReportedSpeech.process","title":"<code>process(doc)</code>","text":"<p>Finds entities related to reported speech.</p> PARAMETER DESCRIPTION <code>doc</code> <p> TYPE: <code>Doc</code> </p> RETURNS DESCRIPTION <code>doc</code> <p> TYPE: <code>spaCy Doc object, annotated for negation</code> </p> Source code in <code>edsnlp/pipelines/qualifiers/reported_speech/reported_speech.py</code> <pre><code>def process(self, doc: Doc) -&gt; Doc:\n\"\"\"\n    Finds entities related to reported speech.\n\n    Parameters\n    ----------\n    doc: spaCy Doc object\n\n    Returns\n    -------\n    doc: spaCy Doc object, annotated for negation\n    \"\"\"\n\n    matches = self.get_matches(doc)\n    matches += list(self.regex_matcher(doc, as_spans=True))\n\n    boundaries = self._boundaries(doc)\n\n    entities = list(doc.ents) + list(doc.spans.get(\"discarded\", []))\n    ents = None\n\n    # Removes duplicate matches and pseudo-expressions in one statement\n    matches = filter_spans(matches, label_to_remove=\"pseudo\")\n\n    for start, end in boundaries:\n\n        ents, entities = consume_spans(\n            entities,\n            filter=lambda s: check_inclusion(s, start, end),\n            second_chance=ents,\n        )\n\n        sub_matches, matches = consume_spans(\n            matches, lambda s: start &lt;= s.start &lt; end\n        )\n\n        if self.on_ents_only and not ents:\n            continue\n\n        sub_preceding = get_spans(sub_matches, \"preceding\")\n        sub_following = get_spans(sub_matches, \"following\")\n        sub_verbs = get_spans(sub_matches, \"verbs\")\n        sub_quotation = get_spans(sub_matches, \"quotation\")\n\n        if not sub_preceding + sub_following + sub_verbs + sub_quotation:\n            continue\n\n        if not self.on_ents_only:\n            for token in doc[start:end]:\n                token._.reported_speech = (\n                    any(m.end &lt;= token.i for m in sub_preceding + sub_verbs)\n                    or any(m.start &gt; token.i for m in sub_following)\n                    or any(\n                        ((m.start &lt; token.i) &amp; (m.end &gt; token.i + 1))\n                        for m in sub_quotation\n                    )\n                )\n        for ent in ents:\n\n            if self.within_ents:\n                cues = [m for m in sub_preceding + sub_verbs if m.end &lt;= ent.end]\n                cues += [m for m in sub_following if m.start &gt;= ent.start]\n            else:\n                cues = [m for m in sub_preceding + sub_verbs if m.end &lt;= ent.start]\n                cues += [m for m in sub_following if m.start &gt;= ent.end]\n\n            cues += [\n                m\n                for m in sub_quotation\n                if (m.start &lt; ent.start) &amp; (m.end &gt; ent.end)\n            ]\n\n            reported_speech = ent._.reported_speech or bool(cues)\n            ent._.reported_speech = reported_speech\n\n            if self.explain:\n                ent._.reported_speech_cues += cues\n\n            if not self.on_ents_only and reported_speech:\n                for token in ent:\n                    token._.reported_speech = True\n    return doc\n</code></pre>"},{"location":"reference/pipelines/trainable/","title":"<code>edsnlp.pipelines.trainable</code>","text":""},{"location":"reference/pipelines/trainable/nested_ner/","title":"<code>edsnlp.pipelines.trainable.nested_ner</code>","text":""},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer","title":"<code>TrainableNer</code>","text":"<p>         Bases: <code>TrainablePipe</code></p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>class TrainableNer(TrainablePipe):\n    def __init__(\n        self,\n        vocab: Vocab,\n        model: Model,\n        name: str = \"nested_ner\",\n        ent_labels: Iterable[str] = (),\n        spans_labels: Mapping[str, Iterable[str]] = None,\n        scorer: Optional[Callable] = None,\n    ) -&gt; None:\n\"\"\"\n        Initialize a general named entity recognizer (with or without nested or\n        overlapping entities).\n\n        Parameters\n        ----------\n        vocab: Vocab\n            Spacy vocabulary\n        model: Model\n            The model to extract the spans\n        name: str\n            Name of the component\n        ent_labels: Iterable[str]\n            list of labels to filter entities for in `doc.ents`\n        spans_labels: Mapping[str, Iterable[str]]\n            Mapping from span group names to list of labels to look for entities\n            and assign the predicted entities\n        scorer: Optional[Callable]\n            Method to call to score predictions\n        \"\"\"\n\n        super().__init__(vocab, model, name)\n\n        self.cfg[\"ent_labels\"]: Optional[Tuple[str]] = (\n            tuple(ent_labels) if ent_labels is not None else None\n        )\n        self.cfg[\"spans_labels\"]: Optional[Dict[str, Tuple[str]]] = (\n            {k: tuple(labels) for k, labels in spans_labels.items()}\n            if spans_labels is not None\n            else None\n        )\n        self.cfg[\"labels\"] = tuple(\n            sorted(\n                set(\n                    (list(ent_labels) if ent_labels is not None else [])\n                    + [\n                        label\n                        for group in (spans_labels or {}).values()\n                        for label in group\n                    ]\n                )\n            )\n        )\n\n        self.scorer = scorer\n\n    @property\n    def labels(self) -&gt; Tuple[str]:\n\"\"\"Return the labels currently added to the component.\"\"\"\n        return self.cfg[\"labels\"]\n\n    @property\n    def spans_labels(self) -&gt; Dict[str, Tuple[str]]:\n\"\"\"Return the span group to labels filters mapping\"\"\"\n        return self.cfg[\"spans_labels\"]\n\n    @property\n    def ent_labels(self):\n\"\"\"Return the doc.ents labels filters\"\"\"\n        return self.cfg[\"ent_labels\"]\n\n    def add_label(self, label: str) -&gt; int:\n\"\"\"Add a new label to the pipe.\"\"\"\n        raise Exception(\"Cannot add a new label to the pipe\")\n\n    def predict(self, docs: List[Doc]) -&gt; Dict[str, Ints2d]:\n\"\"\"\n        Apply the pipeline's model to a batch of docs, without modifying them.\n\n        Parameters\n        ----------\n        docs: List[Doc]\n\n        Returns\n        -------\n        Int2d\n            The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor\n            that contain the spans' prediction for all the batch\n        \"\"\"\n        return self.model.predict((docs, None, True))[1]\n\n    def set_annotations(\n        self, docs: List[Doc], predictions: Dict[str, Ints2d], **kwargs\n    ) -&gt; None:\n\"\"\"\n        Modify a batch of `Doc` objects, using predicted spans.\n\n        Parameters\n        ----------\n        docs: List[Doc]\n            The documents to update\n        predictions:\n            Spans predictions, as returned by the model's predict method\n        \"\"\"\n        docs = list(docs)\n        new_doc_spans: List[List[Span]] = [[] for _ in docs]\n        for doc_idx, label_idx, begin, end in np_ops.asarray(predictions.get(\"spans\")):\n            label = self.labels[label_idx]\n            new_doc_spans[doc_idx].append(Span(docs[doc_idx], begin, end, label))\n\n        for doc, new_spans in zip(docs, new_doc_spans):\n            # Only add a span to `doc.ents` if its label is in `self.ents_labels`\n            doc.ents = filter_spans(\n                [s for s in new_spans if s.label_ in self.ent_labels]\n            )\n\n            # Only add a span to `doc.spans[name]` if its label is in the matching\n            # `self.spans_labels[name]` list\n            for name, group_labels in self.spans_labels.items():\n                doc.spans[name] = [s for s in new_spans if s.label_ in group_labels]\n\n    def update(\n        self,\n        examples: Iterable[Example],\n        *,\n        drop: float = 0.0,\n        set_annotations: bool = False,\n        sgd: Optional[Optimizer] = None,\n        losses: Optional[Dict[str, float]] = None,\n    ) -&gt; Dict[str, float]:\n\"\"\"\n        Learn from a batch of documents and gold-standard information,\n        updating the pipe's model. Delegates to begin_update and get_loss.\n\n        Unlike standard TrainablePipe components, the discrete ops (best selection\n        of tags) is performed by the model directly (`begin_update` returns the loss\n        and the predictions)\n\n        Parameters\n        ----------\n        examples: Iterable[Example]\n        drop: float = 0.0\n\n        set_annotations: bool\n            Whether to update the document with predicted spans\n        sgd: Optional[Optimizer]\n            Optimizer\n        losses: Optional[Dict[str, float]]\n            Dict of loss, updated in place\n\n        Returns\n        -------\n        Dict[str, float]\n            Updated losses dict\n        \"\"\"\n\n        if losses is None:\n            losses = {}\n        losses.setdefault(self.name, 0.0)\n        set_dropout_rate(self.model, drop)\n        examples = list(examples)\n\n        # run the model\n        docs = [eg.predicted for eg in examples]\n        gold = self.examples_to_truth(examples)\n        (loss, predictions), backprop = self.model.begin_update(\n            (docs, gold, set_annotations)\n        )\n        loss, gradient = self.get_loss(examples, loss)\n        backprop(gradient)\n        if sgd is not None:\n            self.model.finish_update(sgd)\n        if set_annotations:\n            self.set_annotations(docs, predictions)\n\n        losses[self.name] = loss\n\n        return loss\n\n    def get_loss(self, examples: Iterable[Example], loss) -&gt; Tuple[float, float]:\n\"\"\"Find the loss and gradient of loss for the batch of documents and\n        their predicted scores.\"\"\"\n        return float(loss.item()), self.model.ops.xp.array([1])\n\n    def initialize(\n        self,\n        get_examples: Callable[[], Iterable[Example]],\n        *,\n        nlp: Language = None,\n        labels: Optional[List[str]] = None,\n    ):\n\"\"\"\n        Initialize the pipe for training, using a representative set\n        of data examples.\n\n        1. If no ent_labels are provided, we scrap them from the ents\n           of the set of examples.\n        2. If no span labels are provided, we scrap them from the spans of the set\n           of examples, and filter these labels with the ents_labels.\n\n        Parameters\n        ----------\n        get_examples: Callable[[], Iterable[Example]]\n            Method to sample some examples\n        nlp: spacy.Language\n            Unused spacy model\n        labels\n            Unused list of labels\n        \"\"\"\n        sub_batch = list(islice(get_examples(), NUM_INITIALIZATION_EXAMPLES))\n        if self.ent_labels is None or self.spans_labels is None:\n            ent_labels_before = self.ent_labels\n            if self.ent_labels is None:\n                self.cfg[\"ent_labels\"] = tuple(\n                    sorted(\n                        {\n                            span.label_\n                            for doc in sub_batch\n                            for span in doc.reference.ents\n                        }\n                    )\n                )\n\n            if self.spans_labels is None:\n                spans_labels = defaultdict(lambda: set())\n                for doc in sub_batch:\n                    for name, group in doc.reference.spans.items():\n                        for span in group:\n                            if (\n                                ent_labels_before is None\n                                or span.label_ in ent_labels_before\n                            ):\n                                spans_labels[name].add(span.label_)\n\n                self.cfg[\"spans_labels\"] = {\n                    name: tuple(sorted(group)) for name, group in spans_labels.items()\n                }\n\n            self.cfg[\"labels\"] = tuple(\n                sorted(\n                    set(\n                        list(self.ent_labels)\n                        + [\n                            label\n                            for group in self.spans_labels.values()\n                            for label in group\n                        ]\n                    )\n                )\n            )\n\n        doc_sample = [eg.reference for eg in sub_batch]\n        spans_sample = self.examples_to_truth(sub_batch)\n        if spans_sample is None:\n            raise ValueError(\n                \"Call begin_training with relevant entities \"\n                \"and relations annotated in \"\n                \"at least a few reference examples!\"\n            )\n        self.model.attrs[\"set_n_labels\"](len(self.labels))\n        self.model.initialize(X=doc_sample, Y=spans_sample)\n\n    def examples_to_truth(self, examples: List[Example]) -&gt; Ints2d:\n\"\"\"\n        Converts the spans of the examples into a list\n        of (doc_idx, label_idx, begin, end) tuple as a tensor,\n        that will be fed to the model with the `begin_update` method.\n\n        Parameters\n        ----------\n        examples: List[Example]\n\n        Returns\n        -------\n        Ints2d\n        \"\"\"\n        label_vocab = {self.vocab.strings[l]: i for i, l in enumerate(self.labels)}\n        spans = set()\n        for eg_idx, eg in enumerate(examples):\n            for span in (\n                *eg.reference.ents,\n                *(\n                    span\n                    for name in (\n                        self.spans_labels\n                        if self.spans_labels is not None\n                        else eg.reference.spans\n                    )\n                    for span in eg.reference.spans.get(name, ())\n                ),\n            ):\n                label_idx = label_vocab.get(span.label)\n                if label_idx is None:\n                    continue\n                spans.add((eg_idx, label_idx, span.start, span.end))\n        truths = self.model.ops.asarray(list(spans))\n        return truths\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.labels","title":"<code>labels: Tuple[str]</code>  <code>property</code>","text":"<p>Return the labels currently added to the component.</p>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.spans_labels","title":"<code>spans_labels: Dict[str, Tuple[str]]</code>  <code>property</code>","text":"<p>Return the span group to labels filters mapping</p>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.ent_labels","title":"<code>ent_labels</code>  <code>property</code>","text":"<p>Return the doc.ents labels filters</p>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.__init__","title":"<code>__init__(vocab, model, name='nested_ner', ent_labels=(), spans_labels=None, scorer=None)</code>","text":"<p>Initialize a general named entity recognizer (with or without nested or overlapping entities).</p> PARAMETER DESCRIPTION <code>vocab</code> <p>Spacy vocabulary</p> <p> TYPE: <code>Vocab</code> </p> <code>model</code> <p>The model to extract the spans</p> <p> TYPE: <code>Model</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'nested_ner'</code> </p> <code>ent_labels</code> <p>list of labels to filter entities for in <code>doc.ents</code></p> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>()</code> </p> <code>spans_labels</code> <p>Mapping from span group names to list of labels to look for entities and assign the predicted entities</p> <p> TYPE: <code>Mapping[str, Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>scorer</code> <p>Method to call to score predictions</p> <p> TYPE: <code>Optional[Callable]</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def __init__(\n    self,\n    vocab: Vocab,\n    model: Model,\n    name: str = \"nested_ner\",\n    ent_labels: Iterable[str] = (),\n    spans_labels: Mapping[str, Iterable[str]] = None,\n    scorer: Optional[Callable] = None,\n) -&gt; None:\n\"\"\"\n    Initialize a general named entity recognizer (with or without nested or\n    overlapping entities).\n\n    Parameters\n    ----------\n    vocab: Vocab\n        Spacy vocabulary\n    model: Model\n        The model to extract the spans\n    name: str\n        Name of the component\n    ent_labels: Iterable[str]\n        list of labels to filter entities for in `doc.ents`\n    spans_labels: Mapping[str, Iterable[str]]\n        Mapping from span group names to list of labels to look for entities\n        and assign the predicted entities\n    scorer: Optional[Callable]\n        Method to call to score predictions\n    \"\"\"\n\n    super().__init__(vocab, model, name)\n\n    self.cfg[\"ent_labels\"]: Optional[Tuple[str]] = (\n        tuple(ent_labels) if ent_labels is not None else None\n    )\n    self.cfg[\"spans_labels\"]: Optional[Dict[str, Tuple[str]]] = (\n        {k: tuple(labels) for k, labels in spans_labels.items()}\n        if spans_labels is not None\n        else None\n    )\n    self.cfg[\"labels\"] = tuple(\n        sorted(\n            set(\n                (list(ent_labels) if ent_labels is not None else [])\n                + [\n                    label\n                    for group in (spans_labels or {}).values()\n                    for label in group\n                ]\n            )\n        )\n    )\n\n    self.scorer = scorer\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.add_label","title":"<code>add_label(label)</code>","text":"<p>Add a new label to the pipe.</p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def add_label(self, label: str) -&gt; int:\n\"\"\"Add a new label to the pipe.\"\"\"\n    raise Exception(\"Cannot add a new label to the pipe\")\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.predict","title":"<code>predict(docs)</code>","text":"<p>Apply the pipeline's model to a batch of docs, without modifying them.</p> PARAMETER DESCRIPTION <code>docs</code> <p> TYPE: <code>List[Doc]</code> </p> RETURNS DESCRIPTION <code>Int2d</code> <p>The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor that contain the spans' prediction for all the batch</p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def predict(self, docs: List[Doc]) -&gt; Dict[str, Ints2d]:\n\"\"\"\n    Apply the pipeline's model to a batch of docs, without modifying them.\n\n    Parameters\n    ----------\n    docs: List[Doc]\n\n    Returns\n    -------\n    Int2d\n        The predicted list of (doc_idx, label_idx, begin, end) tuples as a tensor\n        that contain the spans' prediction for all the batch\n    \"\"\"\n    return self.model.predict((docs, None, True))[1]\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.set_annotations","title":"<code>set_annotations(docs, predictions, **kwargs)</code>","text":"<p>Modify a batch of <code>Doc</code> objects, using predicted spans.</p> PARAMETER DESCRIPTION <code>docs</code> <p>The documents to update</p> <p> TYPE: <code>List[Doc]</code> </p> <code>predictions</code> <p>Spans predictions, as returned by the model's predict method</p> <p> TYPE: <code>Dict[str, Ints2d]</code> </p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def set_annotations(\n    self, docs: List[Doc], predictions: Dict[str, Ints2d], **kwargs\n) -&gt; None:\n\"\"\"\n    Modify a batch of `Doc` objects, using predicted spans.\n\n    Parameters\n    ----------\n    docs: List[Doc]\n        The documents to update\n    predictions:\n        Spans predictions, as returned by the model's predict method\n    \"\"\"\n    docs = list(docs)\n    new_doc_spans: List[List[Span]] = [[] for _ in docs]\n    for doc_idx, label_idx, begin, end in np_ops.asarray(predictions.get(\"spans\")):\n        label = self.labels[label_idx]\n        new_doc_spans[doc_idx].append(Span(docs[doc_idx], begin, end, label))\n\n    for doc, new_spans in zip(docs, new_doc_spans):\n        # Only add a span to `doc.ents` if its label is in `self.ents_labels`\n        doc.ents = filter_spans(\n            [s for s in new_spans if s.label_ in self.ent_labels]\n        )\n\n        # Only add a span to `doc.spans[name]` if its label is in the matching\n        # `self.spans_labels[name]` list\n        for name, group_labels in self.spans_labels.items():\n            doc.spans[name] = [s for s in new_spans if s.label_ in group_labels]\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.update","title":"<code>update(examples, *, drop=0.0, set_annotations=False, sgd=None, losses=None)</code>","text":"<p>Learn from a batch of documents and gold-standard information, updating the pipe's model. Delegates to begin_update and get_loss.</p> <p>Unlike standard TrainablePipe components, the discrete ops (best selection of tags) is performed by the model directly (<code>begin_update</code> returns the loss and the predictions)</p> PARAMETER DESCRIPTION <code>examples</code> <p> TYPE: <code>Iterable[Example]</code> </p> <code>drop</code> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <p>set_annotations: bool     Whether to update the document with predicted spans sgd: Optional[Optimizer]     Optimizer losses: Optional[Dict[str, float]]     Dict of loss, updated in place</p> RETURNS DESCRIPTION <code>Dict[str, float]</code> <p>Updated losses dict</p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def update(\n    self,\n    examples: Iterable[Example],\n    *,\n    drop: float = 0.0,\n    set_annotations: bool = False,\n    sgd: Optional[Optimizer] = None,\n    losses: Optional[Dict[str, float]] = None,\n) -&gt; Dict[str, float]:\n\"\"\"\n    Learn from a batch of documents and gold-standard information,\n    updating the pipe's model. Delegates to begin_update and get_loss.\n\n    Unlike standard TrainablePipe components, the discrete ops (best selection\n    of tags) is performed by the model directly (`begin_update` returns the loss\n    and the predictions)\n\n    Parameters\n    ----------\n    examples: Iterable[Example]\n    drop: float = 0.0\n\n    set_annotations: bool\n        Whether to update the document with predicted spans\n    sgd: Optional[Optimizer]\n        Optimizer\n    losses: Optional[Dict[str, float]]\n        Dict of loss, updated in place\n\n    Returns\n    -------\n    Dict[str, float]\n        Updated losses dict\n    \"\"\"\n\n    if losses is None:\n        losses = {}\n    losses.setdefault(self.name, 0.0)\n    set_dropout_rate(self.model, drop)\n    examples = list(examples)\n\n    # run the model\n    docs = [eg.predicted for eg in examples]\n    gold = self.examples_to_truth(examples)\n    (loss, predictions), backprop = self.model.begin_update(\n        (docs, gold, set_annotations)\n    )\n    loss, gradient = self.get_loss(examples, loss)\n    backprop(gradient)\n    if sgd is not None:\n        self.model.finish_update(sgd)\n    if set_annotations:\n        self.set_annotations(docs, predictions)\n\n    losses[self.name] = loss\n\n    return loss\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.get_loss","title":"<code>get_loss(examples, loss)</code>","text":"<p>Find the loss and gradient of loss for the batch of documents and their predicted scores.</p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def get_loss(self, examples: Iterable[Example], loss) -&gt; Tuple[float, float]:\n\"\"\"Find the loss and gradient of loss for the batch of documents and\n    their predicted scores.\"\"\"\n    return float(loss.item()), self.model.ops.xp.array([1])\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.initialize","title":"<code>initialize(get_examples, *, nlp=None, labels=None)</code>","text":"<p>Initialize the pipe for training, using a representative set of data examples.</p> <ol> <li>If no ent_labels are provided, we scrap them from the ents    of the set of examples.</li> <li>If no span labels are provided, we scrap them from the spans of the set    of examples, and filter these labels with the ents_labels.</li> </ol> PARAMETER DESCRIPTION <code>get_examples</code> <p>Method to sample some examples</p> <p> TYPE: <code>Callable[[], Iterable[Example]]</code> </p> <code>nlp</code> <p>Unused spacy model</p> <p> TYPE: <code>Language</code> DEFAULT: <code>None</code> </p> <code>labels</code> <p>Unused list of labels</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def initialize(\n    self,\n    get_examples: Callable[[], Iterable[Example]],\n    *,\n    nlp: Language = None,\n    labels: Optional[List[str]] = None,\n):\n\"\"\"\n    Initialize the pipe for training, using a representative set\n    of data examples.\n\n    1. If no ent_labels are provided, we scrap them from the ents\n       of the set of examples.\n    2. If no span labels are provided, we scrap them from the spans of the set\n       of examples, and filter these labels with the ents_labels.\n\n    Parameters\n    ----------\n    get_examples: Callable[[], Iterable[Example]]\n        Method to sample some examples\n    nlp: spacy.Language\n        Unused spacy model\n    labels\n        Unused list of labels\n    \"\"\"\n    sub_batch = list(islice(get_examples(), NUM_INITIALIZATION_EXAMPLES))\n    if self.ent_labels is None or self.spans_labels is None:\n        ent_labels_before = self.ent_labels\n        if self.ent_labels is None:\n            self.cfg[\"ent_labels\"] = tuple(\n                sorted(\n                    {\n                        span.label_\n                        for doc in sub_batch\n                        for span in doc.reference.ents\n                    }\n                )\n            )\n\n        if self.spans_labels is None:\n            spans_labels = defaultdict(lambda: set())\n            for doc in sub_batch:\n                for name, group in doc.reference.spans.items():\n                    for span in group:\n                        if (\n                            ent_labels_before is None\n                            or span.label_ in ent_labels_before\n                        ):\n                            spans_labels[name].add(span.label_)\n\n            self.cfg[\"spans_labels\"] = {\n                name: tuple(sorted(group)) for name, group in spans_labels.items()\n            }\n\n        self.cfg[\"labels\"] = tuple(\n            sorted(\n                set(\n                    list(self.ent_labels)\n                    + [\n                        label\n                        for group in self.spans_labels.values()\n                        for label in group\n                    ]\n                )\n            )\n        )\n\n    doc_sample = [eg.reference for eg in sub_batch]\n    spans_sample = self.examples_to_truth(sub_batch)\n    if spans_sample is None:\n        raise ValueError(\n            \"Call begin_training with relevant entities \"\n            \"and relations annotated in \"\n            \"at least a few reference examples!\"\n        )\n    self.model.attrs[\"set_n_labels\"](len(self.labels))\n    self.model.initialize(X=doc_sample, Y=spans_sample)\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.TrainableNer.examples_to_truth","title":"<code>examples_to_truth(examples)</code>","text":"<p>Converts the spans of the examples into a list of (doc_idx, label_idx, begin, end) tuple as a tensor, that will be fed to the model with the <code>begin_update</code> method.</p> PARAMETER DESCRIPTION <code>examples</code> <p> TYPE: <code>List[Example]</code> </p> RETURNS DESCRIPTION <code>Ints2d</code> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def examples_to_truth(self, examples: List[Example]) -&gt; Ints2d:\n\"\"\"\n    Converts the spans of the examples into a list\n    of (doc_idx, label_idx, begin, end) tuple as a tensor,\n    that will be fed to the model with the `begin_update` method.\n\n    Parameters\n    ----------\n    examples: List[Example]\n\n    Returns\n    -------\n    Ints2d\n    \"\"\"\n    label_vocab = {self.vocab.strings[l]: i for i, l in enumerate(self.labels)}\n    spans = set()\n    for eg_idx, eg in enumerate(examples):\n        for span in (\n            *eg.reference.ents,\n            *(\n                span\n                for name in (\n                    self.spans_labels\n                    if self.spans_labels is not None\n                    else eg.reference.spans\n                )\n                for span in eg.reference.spans.get(name, ())\n            ),\n        ):\n            label_idx = label_vocab.get(span.label)\n            if label_idx is None:\n                continue\n            spans.add((eg_idx, label_idx, span.start, span.end))\n    truths = self.model.ops.asarray(list(spans))\n    return truths\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.create_component","title":"<code>create_component(nlp, name, model, ent_labels=None, spans_labels=None, scorer=None)</code>","text":"<p>Construct a TrainableQualifier component.</p> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>@Language.factory(\n    \"nested_ner\",\n    default_config=NESTED_NER_DEFAULTS,\n    requires=[\"doc.ents\", \"doc.spans\"],\n    assigns=[\"doc.ents\", \"doc.spans\"],\n    default_score_weights={\n        \"ents_f\": 1.0,\n        \"ents_p\": 0.0,\n        \"ents_r\": 0.0,\n    },\n)\ndef create_component(\n    nlp: Language,\n    name: str,\n    model: Model,\n    ent_labels=None,\n    spans_labels=None,\n    scorer=None,\n):\n\"\"\"Construct a TrainableQualifier component.\"\"\"\n    return TrainableNer(\n        vocab=nlp.vocab,\n        model=model,\n        name=name,\n        ent_labels=ent_labels,\n        spans_labels=spans_labels,\n        scorer=scorer,\n    )\n</code></pre>"},{"location":"reference/pipelines/trainable/nested_ner/#edsnlp.pipelines.trainable.nested_ner.nested_ner_scorer","title":"<code>nested_ner_scorer(examples, **cfg)</code>","text":"<p>Scores the extracted entities that may be overlapping or nested by looking in <code>doc.ents</code>, and <code>doc.spans</code>.</p> PARAMETER DESCRIPTION <code>examples</code> <p> TYPE: <code>Iterable[Example]</code> </p> <code>cfg</code> <ul> <li>labels: Iterable[str] labels to take into account</li> <li>spans_labels: Iterable[str] span group names to look into for entities</li> </ul> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Dict[str, float]</code> Source code in <code>edsnlp/pipelines/trainable/nested_ner.py</code> <pre><code>def nested_ner_scorer(examples: Iterable[Example], **cfg):\n\"\"\"\n    Scores the extracted entities that may be overlapping or nested\n    by looking in `doc.ents`, and `doc.spans`.\n\n    Parameters\n    ----------\n    examples: Iterable[Example]\n    cfg: Dict[str]\n        - labels: Iterable[str] labels to take into account\n        - spans_labels: Iterable[str] span group names to look into for entities\n\n    Returns\n    -------\n    Dict[str, float]\n    \"\"\"\n    labels = set(cfg[\"labels\"]) if \"labels\" in cfg is not None else None\n    spans_labels = cfg[\"spans_labels\"]\n\n    pred_spans = set()\n    gold_spans = set()\n    for eg_idx, eg in enumerate(examples):\n        for span in (\n            *eg.predicted.ents,\n            *(\n                span\n                for name in (\n                    spans_labels if spans_labels is not None else eg.reference.spans\n                )\n                for span in eg.predicted.spans.get(name, ())\n            ),\n        ):\n            if labels is None or span.label_ in labels:\n                pred_spans.add((eg_idx, span.start, span.end, span.label_))\n\n        for span in (\n            *eg.reference.ents,\n            *(\n                span\n                for name in (\n                    spans_labels if spans_labels is not None else eg.reference.spans\n                )\n                for span in eg.reference.spans.get(name, ())\n            ),\n        ):\n            if labels is None or span.label_ in labels:\n                gold_spans.add((eg_idx, span.start, span.end, span.label_))\n\n    tp = len(pred_spans &amp; gold_spans)\n\n    return {\n        \"ents_p\": tp / len(pred_spans) if pred_spans else float(tp == len(pred_spans)),\n        \"ents_r\": tp / len(gold_spans) if gold_spans else float(tp == len(gold_spans)),\n        \"ents_f\": 2 * tp / (len(pred_spans) + len(gold_spans))\n        if pred_spans or gold_spans\n        else float(len(pred_spans) == len(gold_spans)),\n    }\n</code></pre>"},{"location":"reference/processing/","title":"<code>edsnlp.processing</code>","text":""},{"location":"reference/processing/distributed/","title":"<code>edsnlp.processing.distributed</code>","text":""},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.pyspark_type_finder","title":"<code>pyspark_type_finder(obj)</code>","text":"<p>Returns (when possible) the PySpark type of any python object</p> Source code in <code>edsnlp/processing/distributed.py</code> <pre><code>def pyspark_type_finder(obj):\n\"\"\"\n    Returns (when possible) the PySpark type of any python object\n    \"\"\"\n    try:\n        inferred_type = T._infer_type(obj)\n        logger.info(f\"Inferred type is {repr(inferred_type)}\")\n        return inferred_type\n    except TypeError:\n        raise TypeError(\"Cannot infer type for this object.\")\n</code></pre>"},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.pipe","title":"<code>pipe(note, nlp, context=[], additional_spans='discarded', extensions={})</code>","text":"<p>Function to apply a spaCy pipe to a pyspark or koalas DataFrame note</p> PARAMETER DESCRIPTION <code>note</code> <p>A Pyspark or Koalas DataFrame with a <code>note_id</code> and <code>note_text</code> column</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>A spaCy pipe</p> <p> TYPE: <code>Language</code> </p> <code>context</code> <p>A list of column to add to the generated SpaCy document as an extension. For instance, if <code>context=[\"note_datetime\"], the corresponding value found in the</code>note_datetime<code>column will be stored in</code>doc._.note_datetime<code>, which can be useful e.g. for the</code>dates` pipeline.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> <code>additional_spans</code> <p>A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as <code>doc.spans[spangroup_name]</code> and can be generated by some pipes. For instance, the <code>eds.dates</code> pipeline component populates <code>doc.spans['dates']</code></p> <p> TYPE: <code>Union[List[str], str], by default \"discarded\"</code> DEFAULT: <code>'discarded'</code> </p> <code>extensions</code> <p>Spans extensions to add to the extracted results: For instance, if <code>extensions=[\"score_name\"]</code>, the extracted result will include, for each entity, <code>ent._.score_name</code>.</p> <p> TYPE: <code>List[Tuple[str, T.DataType]], by default []</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pyspark DataFrame with one line per extraction</p> Source code in <code>edsnlp/processing/distributed.py</code> <pre><code>@module_checker\ndef pipe(\n    note: DataFrames,\n    nlp: Language,\n    context: List[str] = [],\n    additional_spans: Union[List[str], str] = \"discarded\",\n    extensions: Dict[str, T.DataType] = {},\n) -&gt; DataFrame:\n\"\"\"\n    Function to apply a spaCy pipe to a pyspark or koalas DataFrame note\n\n    Parameters\n    ----------\n    note : DataFrame\n        A Pyspark or Koalas DataFrame with a `note_id` and `note_text` column\n    nlp : Language\n        A spaCy pipe\n    context : List[str]\n        A list of column to add to the generated SpaCy document as an extension.\n        For instance, if `context=[\"note_datetime\"], the corresponding value found\n        in the `note_datetime` column will be stored in `doc._.note_datetime`,\n        which can be useful e.g. for the `dates` pipeline.\n    additional_spans : Union[List[str], str], by default \"discarded\"\n        A name (or list of names) of SpanGroup on which to apply the pipe too:\n        SpanGroup are available as `doc.spans[spangroup_name]` and can be generated\n        by some pipes. For instance, the `eds.dates` pipeline\n        component populates `doc.spans['dates']`\n    extensions : List[Tuple[str, T.DataType]], by default []\n        Spans extensions to add to the extracted results:\n        For instance, if `extensions=[\"score_name\"]`, the extracted result\n        will include, for each entity, `ent._.score_name`.\n\n    Returns\n    -------\n    DataFrame\n        A pyspark DataFrame with one line per extraction\n    \"\"\"\n\n    if context:\n        check_spacy_version_for_context()\n\n    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n    sc = spark.sparkContext\n\n    if not nlp.has_pipe(\"eds.context\"):\n        nlp.add_pipe(\"eds.context\", first=True, config=dict(context=context))\n\n    nlp_bc = sc.broadcast(nlp)\n\n    def _udf_factory(\n        additional_spans: Union[List[str], str] = \"discarded\",\n        extensions: Dict[str, T.DataType] = dict(),\n    ):\n\n        schema = T.ArrayType(\n            T.StructType(\n                [\n                    T.StructField(\"lexical_variant\", T.StringType(), False),\n                    T.StructField(\"label\", T.StringType(), False),\n                    T.StructField(\"span_type\", T.StringType(), True),\n                    T.StructField(\"start\", T.IntegerType(), False),\n                    T.StructField(\"end\", T.IntegerType(), False),\n                    *[\n                        T.StructField(slugify(extension_name), extension_type, True)\n                        for extension_name, extension_type in extensions.items()\n                    ],\n                ]\n            )\n        )\n\n        def f(\n            text,\n            *context_values,\n            additional_spans=additional_spans,\n            extensions=extensions,\n        ):\n\n            if text is None:\n                return []\n\n            nlp = nlp_bc.value\n\n            for _, pipe in nlp.pipeline:\n                if isinstance(pipe, BaseComponent):\n                    pipe.set_extensions()\n\n            doc = nlp.make_doc(text)\n            for context_name, context_value in zip(context, context_values):\n                doc._.set(context_name, context_value)\n            doc = nlp(doc)\n\n            ents = []\n\n            for ent in doc.ents:\n                parsed_extensions = [\n                    rgetattr(ent._, extension) for extension in extensions.keys()\n                ]\n\n                ents.append(\n                    (\n                        ent.text,\n                        ent.label_,\n                        \"ents\",\n                        ent.start_char,\n                        ent.end_char,\n                        *parsed_extensions,\n                    )\n                )\n\n            if additional_spans is None:\n                return ents\n\n            if type(additional_spans) == str:\n                additional_spans = [additional_spans]\n\n            for spans_name in additional_spans:\n\n                for ent in doc.spans.get(spans_name, []):\n\n                    parsed_extensions = [\n                        rgetattr(ent._, extension) for extension in extensions.keys()\n                    ]\n\n                    ents.append(\n                        (\n                            ent.text,\n                            ent.label_,\n                            spans_name,\n                            ent.start_char,\n                            ent.end_char,\n                            *parsed_extensions,\n                        )\n                    )\n\n            return ents\n\n        f_udf = F.udf(\n            partial(\n                f,\n                additional_spans=additional_spans,\n                extensions=extensions,\n            ),\n            schema,\n        )\n\n        return f_udf\n\n    matcher = _udf_factory(\n        additional_spans=additional_spans,\n        extensions=extensions,\n    )\n\n    n_needed_partitions = max(note.count() // 2000, 1)  # Batch sizes of 2000\n\n    note_nlp = note.repartition(n_needed_partitions).withColumn(\n        \"matches\", matcher(F.col(\"note_text\"), *[F.col(c) for c in context])\n    )\n\n    note_nlp = note_nlp.withColumn(\"matches\", F.explode(note_nlp.matches))\n\n    note_nlp = note_nlp.select(\"note_id\", \"matches.*\")\n\n    return note_nlp\n</code></pre>"},{"location":"reference/processing/distributed/#edsnlp.processing.distributed.custom_pipe","title":"<code>custom_pipe(note, nlp, results_extractor, dtypes, context=[])</code>","text":"<p>Function to apply a spaCy pipe to a pyspark or koalas DataFrame note, a generic callback function that converts a spaCy <code>Doc</code> object into a list of dictionaries.</p> PARAMETER DESCRIPTION <code>note</code> <p>A Pyspark or Koalas DataFrame with a <code>note_text</code> column</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>A spaCy pipe</p> <p> TYPE: <code>Language</code> </p> <code>results_extractor</code> <p>Arbitrary function that takes extract serialisable results from the computed spaCy <code>Doc</code> object. The output of the function must be a list of dictionaries containing the extracted spans or entities.</p> <p>There is no requirement for all entities to provide every dictionary key.</p> <p> TYPE: <code>Callable[[Doc], List[Dict[str, Any]]]</code> </p> <code>dtypes</code> <p>Dictionary containing all expected keys from the <code>results_extractor</code> function, along with their types.</p> <p> TYPE: <code>Dict[str, T.DataType]</code> </p> <code>context</code> <p>A list of column to add to the generated SpaCy document as an extension. For instance, if <code>context=[\"note_datetime\"], the corresponding value found in the</code>note_datetime<code>column will be stored in</code>doc._.note_datetime<code>, which can be useful e.g. for the</code>dates` pipeline.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pyspark DataFrame with one line per extraction</p> Source code in <code>edsnlp/processing/distributed.py</code> <pre><code>@module_checker\ndef custom_pipe(\n    note: DataFrames,\n    nlp: Language,\n    results_extractor: Callable[[Doc], List[Dict[str, Any]]],\n    dtypes: Dict[str, T.DataType],\n    context: List[str] = [],\n) -&gt; DataFrame:\n\"\"\"\n    Function to apply a spaCy pipe to a pyspark or koalas DataFrame note,\n    a generic callback function that converts a spaCy `Doc` object into a\n    list of dictionaries.\n\n    Parameters\n    ----------\n    note : DataFrame\n        A Pyspark or Koalas DataFrame with a `note_text` column\n    nlp : Language\n        A spaCy pipe\n    results_extractor : Callable[[Doc], List[Dict[str, Any]]]\n        Arbitrary function that takes extract serialisable results from the computed\n        spaCy `Doc` object. The output of the function must be a list of dictionaries\n        containing the extracted spans or entities.\n\n        There is no requirement for all entities to provide every dictionary key.\n    dtypes : Dict[str, T.DataType]\n        Dictionary containing all expected keys from the `results_extractor` function,\n        along with their types.\n    context : List[str]\n        A list of column to add to the generated SpaCy document as an extension.\n        For instance, if `context=[\"note_datetime\"], the corresponding value found\n        in the `note_datetime` column will be stored in `doc._.note_datetime`,\n        which can be useful e.g. for the `dates` pipeline.\n\n    Returns\n    -------\n    DataFrame\n        A pyspark DataFrame with one line per extraction\n    \"\"\"\n\n    if context:\n        check_spacy_version_for_context()\n\n    if (\"note_id\" not in context) and (\"note_id\" in dtypes.keys()):\n        context.append(\"note_id\")\n\n    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n    sc = spark.sparkContext\n\n    if not nlp.has_pipe(\"eds.context\"):\n        nlp.add_pipe(\"eds.context\", first=True, config=dict(context=context))\n\n    nlp_bc = sc.broadcast(nlp)\n\n    schema = T.ArrayType(\n        T.StructType([T.StructField(key, dtype) for key, dtype in dtypes.items()])\n    )\n\n    @F.udf(schema)\n    def udf(\n        text,\n        *context_values,\n    ):\n\n        if text is None:\n            return []\n\n        nlp_ = nlp_bc.value\n\n        for _, pipe in nlp.pipeline:\n            if isinstance(pipe, BaseComponent):\n                pipe.set_extensions()\n\n        doc = nlp_.make_doc(text)\n        for context_name, context_value in zip(context, context_values):\n            doc._.set(context_name, context_value)\n\n        doc = nlp_(doc)\n\n        results = []\n\n        for res in results_extractor(doc):\n            results.append([res.get(key) for key in dtypes])\n\n        return results\n\n    note_nlp = note.withColumn(\n        \"matches\", udf(F.col(\"note_text\"), *[F.col(c) for c in context])\n    )\n\n    note_nlp = note_nlp.withColumn(\"matches\", F.explode(note_nlp.matches))\n\n    if (\"note_id\" not in dtypes.keys()) and (\"note_id\" in note_nlp.columns):\n        note_nlp = note_nlp.select(\"note_id\", \"matches.*\")\n    else:\n        note_nlp = note_nlp.select(\"matches.*\")\n\n    return note_nlp\n</code></pre>"},{"location":"reference/processing/helpers/","title":"<code>edsnlp.processing.helpers</code>","text":""},{"location":"reference/processing/helpers/#edsnlp.processing.helpers.slugify","title":"<code>slugify(chained_attr)</code>","text":"<p>Slugify a chained attribute name</p> PARAMETER DESCRIPTION <code>chained_attr</code> <p>The string to slugify (replace dots by _)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The slugified string</p> Source code in <code>edsnlp/processing/helpers.py</code> <pre><code>def slugify(chained_attr: str) -&gt; str:\n\"\"\"\n    Slugify a chained attribute name\n\n    Parameters\n    ----------\n    chained_attr : str\n        The string to slugify (replace dots by _)\n\n    Returns\n    -------\n    str\n        The slugified string\n    \"\"\"\n    return chained_attr.replace(\".\", \"_\")\n</code></pre>"},{"location":"reference/processing/parallel/","title":"<code>edsnlp.processing.parallel</code>","text":""},{"location":"reference/processing/parallel/#edsnlp.processing.parallel.pipe","title":"<code>pipe(note, nlp, context=[], additional_spans=[], extensions=[], results_extractor=None, chunksize=100, n_jobs=-2, progress_bar=True, **pipe_kwargs)</code>","text":"<p>Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing</p> PARAMETER DESCRIPTION <code>note</code> <p>A pandas DataFrame with a <code>note_id</code> and <code>note_text</code> column</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>A spaCy pipe</p> <p> TYPE: <code>Language</code> </p> <code>context</code> <p>A list of column to add to the generated SpaCy document as an extension. For instance, if <code>context=[\"note_datetime\"], the corresponding value found in the</code>note_datetime<code>column will be stored in</code>doc._.note_datetime<code>, which can be useful e.g. for the</code>dates` pipeline.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> <code>results_extractor</code> <p>Arbitrary function that takes extract serialisable results from the computed spaCy <code>Doc</code> object. The output of the function must be a list of dictionaries containing the extracted spans or entities.</p> <p> TYPE: <code>Optional[Callable[[Doc], List[Dict[str, Any]]]]</code> DEFAULT: <code>None</code> </p> <code>additional_spans</code> <p>A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as <code>doc.spans[spangroup_name]</code> and can be generated by some pipes. For instance, the <code>date</code> pipe populates doc.spans['dates']</p> <p> TYPE: <code>Union[List[str], str], by default [] (empty list)</code> DEFAULT: <code>[]</code> </p> <code>extensions</code> <p>Spans extensions to add to the extracted results: For instance, if <code>extensions=[\"score_name\"]</code>, the extracted result will include, for each entity, <code>ent._.score_name</code>.</p> <p> TYPE: <code>List[Tuple[str, T.DataType]], by default []</code> DEFAULT: <code>[]</code> </p> <code>chunksize</code> <p>Batch size used to split tasks</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_jobs</code> <p>Max number of parallel jobs. The default value uses the maximum number of available cores.</p> <p> TYPE: <code>int</code> DEFAULT: <code>-2</code> </p> <code>progress_bar</code> <p>Whether to display a progress bar or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>**pipe_kwargs</code> <p>Arguments exposed in <code>processing.pipe_generator</code> are also available here</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas DataFrame with one line per extraction</p> Source code in <code>edsnlp/processing/parallel.py</code> <pre><code>def pipe(\n    note: pd.DataFrame,\n    nlp: Language,\n    context: List[str] = [],\n    additional_spans: Union[List[str], str] = [],\n    extensions: ExtensionSchema = [],\n    results_extractor: Optional[Callable[[Doc], List[Dict[str, Any]]]] = None,\n    chunksize: int = 100,\n    n_jobs: int = -2,\n    progress_bar: bool = True,\n    **pipe_kwargs,\n):\n\"\"\"\n    Function to apply a spaCy pipe to a pandas DataFrame note by using multiprocessing\n\n    Parameters\n    ----------\n    note : DataFrame\n        A pandas DataFrame with a `note_id` and `note_text` column\n    nlp : Language\n        A spaCy pipe\n    context : List[str]\n        A list of column to add to the generated SpaCy document as an extension.\n        For instance, if `context=[\"note_datetime\"], the corresponding value found\n        in the `note_datetime` column will be stored in `doc._.note_datetime`,\n        which can be useful e.g. for the `dates` pipeline.\n    results_extractor : Optional[Callable[[Doc], List[Dict[str, Any]]]]\n        Arbitrary function that takes extract serialisable results from the computed\n        spaCy `Doc` object. The output of the function must be a list of dictionaries\n        containing the extracted spans or entities.\n    additional_spans : Union[List[str], str], by default [] (empty list)\n        A name (or list of names) of SpanGroup on which to apply the pipe too:\n        SpanGroup are available as `doc.spans[spangroup_name]` and can be generated\n        by some pipes. For instance, the `date` pipe populates doc.spans['dates']\n    extensions : List[Tuple[str, T.DataType]], by default []\n        Spans extensions to add to the extracted results:\n        For instance, if `extensions=[\"score_name\"]`, the extracted result\n        will include, for each entity, `ent._.score_name`.\n    chunksize: int, by default 100\n        Batch size used to split tasks\n    n_jobs: int, by default -2\n        Max number of parallel jobs.\n        The default value uses the maximum number of available cores.\n    progress_bar: bool, by default True\n        Whether to display a progress bar or not\n    **pipe_kwargs:\n        Arguments exposed in `processing.pipe_generator` are also available here\n\n    Returns\n    -------\n    DataFrame\n        A pandas DataFrame with one line per extraction\n    \"\"\"\n\n    if context:\n        check_spacy_version_for_context()\n\n    # Setting the nlp variable\n    _define_nlp(nlp)\n\n    verbose = 10 if progress_bar else 0\n\n    executor = Parallel(\n        n_jobs, backend=\"multiprocessing\", prefer=\"processes\", verbose=verbose\n    )\n    executor.warn(f\"Used nlp components: {nlp.component_names}\")\n\n    pipe_kwargs[\"additional_spans\"] = additional_spans\n    pipe_kwargs[\"extensions\"] = extensions\n    pipe_kwargs[\"results_extractor\"] = results_extractor\n    pipe_kwargs[\"context\"] = context\n\n    if verbose:\n        executor.warn(f\"{int(len(note)/chunksize)} tasks to complete\")\n\n    do = delayed(_process_chunk)\n\n    tasks = (\n        do(chunk, **pipe_kwargs)\n        for chunk in _chunker(note, len(note), chunksize=chunksize)\n    )\n    result = executor(tasks)\n\n    out = _flatten(result)\n\n    return pd.DataFrame(out)\n</code></pre>"},{"location":"reference/processing/simple/","title":"<code>edsnlp.processing.simple</code>","text":""},{"location":"reference/processing/simple/#edsnlp.processing.simple.pipe","title":"<code>pipe(note, nlp, context=[], results_extractor=None, additional_spans=[], extensions=[], batch_size=1000, progress_bar=True)</code>","text":"<p>Function to apply a spaCy pipe to a pandas DataFrame note For a large DataFrame, prefer the parallel version.</p> PARAMETER DESCRIPTION <code>note</code> <p>A pandas DataFrame with a <code>note_id</code> and <code>note_text</code> column</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>A spaCy pipe</p> <p> TYPE: <code>Language</code> </p> <code>context</code> <p>A list of column to add to the generated SpaCy document as an extension. For instance, if <code>context=[\"note_datetime\"], the corresponding value found in the</code>note_datetime<code>column will be stored in</code>doc._.note_datetime<code>, which can be useful e.g. for the</code>dates` pipeline.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> <code>additional_spans</code> <p>A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as <code>doc.spans[spangroup_name]</code> and can be generated by some pipes. For instance, the <code>date</code> pipe populates doc.spans['dates']</p> <p> TYPE: <code>Union[List[str], str], by default \"discarded\"</code> DEFAULT: <code>[]</code> </p> <code>extensions</code> <p>Spans extensions to add to the extracted results: For instance, if <code>extensions=[\"score_name\"]</code>, the extracted result will include, for each entity, <code>ent._.score_name</code>.</p> <p> TYPE: <code>List[Tuple[str, T.DataType]], by default []</code> DEFAULT: <code>[]</code> </p> <code>batch_size</code> <p>Batch size used by spaCy's pipe</p> <p> TYPE: <code>int, by default 1000</code> DEFAULT: <code>1000</code> </p> <code>progress_bar</code> <p>Whether to display a progress bar or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas DataFrame with one line per extraction</p> Source code in <code>edsnlp/processing/simple.py</code> <pre><code>def pipe(\n    note: pd.DataFrame,\n    nlp: Language,\n    context: List[str] = [],\n    results_extractor: Optional[Callable[[Doc], List[Dict[str, Any]]]] = None,\n    additional_spans: Union[List[str], str] = [],\n    extensions: Union[List[str], str] = [],\n    batch_size: int = 1000,\n    progress_bar: bool = True,\n):\n\"\"\"\n    Function to apply a spaCy pipe to a pandas DataFrame note\n    For a large DataFrame, prefer the parallel version.\n\n    Parameters\n    ----------\n    note : DataFrame\n        A pandas DataFrame with a `note_id` and `note_text` column\n    nlp : Language\n        A spaCy pipe\n    context : List[str]\n        A list of column to add to the generated SpaCy document as an extension.\n        For instance, if `context=[\"note_datetime\"], the corresponding value found\n        in the `note_datetime` column will be stored in `doc._.note_datetime`,\n        which can be useful e.g. for the `dates` pipeline.\n    additional_spans : Union[List[str], str], by default \"discarded\"\n        A name (or list of names) of SpanGroup on which to apply the pipe too:\n        SpanGroup are available as `doc.spans[spangroup_name]` and can be generated\n        by some pipes. For instance, the `date` pipe populates doc.spans['dates']\n    extensions : List[Tuple[str, T.DataType]], by default []\n        Spans extensions to add to the extracted results:\n        For instance, if `extensions=[\"score_name\"]`, the extracted result\n        will include, for each entity, `ent._.score_name`.\n    batch_size : int, by default 1000\n        Batch size used by spaCy's pipe\n    progress_bar: bool, by default True\n        Whether to display a progress bar or not\n\n    Returns\n    -------\n    DataFrame\n        A pandas DataFrame with one line per extraction\n    \"\"\"\n    return pd.DataFrame(\n        _flatten(\n            _pipe_generator(\n                note=note,\n                nlp=nlp,\n                context=context,\n                results_extractor=results_extractor,\n                additional_spans=additional_spans,\n                extensions=extensions,\n                batch_size=batch_size,\n                progress_bar=progress_bar,\n            )\n        )\n    )\n</code></pre>"},{"location":"reference/processing/utils/","title":"<code>edsnlp.processing.utils</code>","text":""},{"location":"reference/processing/wrapper/","title":"<code>edsnlp.processing.wrapper</code>","text":""},{"location":"reference/processing/wrapper/#edsnlp.processing.wrapper.pipe","title":"<code>pipe(note, nlp, n_jobs=-2, context=[], results_extractor=None, additional_spans=[], extensions=[], **kwargs)</code>","text":"<p>Function to apply a spaCy pipe to a pandas or pyspark DataFrame</p> PARAMETER DESCRIPTION <code>note</code> <p>A pandas/pyspark/koalas DataFrame with a <code>note_id</code> and <code>note_text</code> column</p> <p> TYPE: <code>DataFrame</code> </p> <code>nlp</code> <p>A spaCy pipe</p> <p> TYPE: <code>Language</code> </p> <code>context</code> <p>A list of column to add to the generated SpaCy document as an extension. For instance, if <code>context=[\"note_datetime\"], the corresponding value found in the</code>note_datetime<code>column will be stored in</code>doc._.note_datetime<code>, which can be useful e.g. for the</code>dates` pipeline.</p> <p> TYPE: <code>List[str]</code> DEFAULT: <code>[]</code> </p> <code>n_jobs</code> <p>Only used when providing a Pandas DataFrame</p> <ul> <li><code>n_jobs=1</code> corresponds to <code>simple_pipe</code></li> <li><code>n_jobs&gt;1</code> corresponds to <code>parallel_pipe</code> with <code>n_jobs</code> parallel workers</li> <li><code>n_jobs=-1</code> corresponds to <code>parallel_pipe</code> with maximum number of workers</li> <li><code>n_jobs=-2</code> corresponds to <code>parallel_pipe</code> with maximum number of workers -1</li> </ul> <p> TYPE: <code>int, by default -2</code> DEFAULT: <code>-2</code> </p> <code>additional_spans</code> <p>A name (or list of names) of SpanGroup on which to apply the pipe too: SpanGroup are available as <code>doc.spans[spangroup_name]</code> and can be generated by some pipes. For instance, the <code>date</code> pipe populates doc.spans['dates']</p> <p> TYPE: <code>Union[List[str], str], by default \"discarded\"</code> DEFAULT: <code>[]</code> </p> <code>extensions</code> <p>Spans extensions to add to the extracted results: For instance, if <code>extensions=[\"score_name\"]</code>, the extracted result will include, for each entity, <code>ent._.score_name</code>.</p> <p> TYPE: <code>List[Tuple[str, T.DataType]], by default []</code> DEFAULT: <code>[]</code> </p> <code>kwargs</code> <p>Additional parameters depending on the <code>how</code> argument.</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame with one line per extraction</p> Source code in <code>edsnlp/processing/wrapper.py</code> <pre><code>def pipe(\n    note: DataFrames,\n    nlp: Language,\n    n_jobs: int = -2,\n    context: List[str] = [],\n    results_extractor: Optional[Callable[[Doc], List[Dict[str, Any]]]] = None,\n    additional_spans: Union[List[str], str] = [],\n    extensions: ExtensionSchema = [],\n    **kwargs: Dict[str, Any],\n) -&gt; DataFrames:\n\"\"\"\n    Function to apply a spaCy pipe to a pandas or pyspark DataFrame\n\n\n    Parameters\n    ----------\n    note : DataFrame\n        A pandas/pyspark/koalas DataFrame with a `note_id` and `note_text` column\n    nlp : Language\n        A spaCy pipe\n    context : List[str]\n        A list of column to add to the generated SpaCy document as an extension.\n        For instance, if `context=[\"note_datetime\"], the corresponding value found\n        in the `note_datetime` column will be stored in `doc._.note_datetime`,\n        which can be useful e.g. for the `dates` pipeline.\n    n_jobs : int, by default -2\n        Only used when providing a Pandas DataFrame\n\n        - `n_jobs=1` corresponds to `simple_pipe`\n        - `n_jobs&gt;1` corresponds to `parallel_pipe` with `n_jobs` parallel workers\n        - `n_jobs=-1` corresponds to `parallel_pipe` with maximum number of workers\n        - `n_jobs=-2` corresponds to `parallel_pipe` with maximum number of workers -1\n    additional_spans : Union[List[str], str], by default \"discarded\"\n        A name (or list of names) of SpanGroup on which to apply the pipe too:\n        SpanGroup are available as `doc.spans[spangroup_name]` and can be generated\n        by some pipes. For instance, the `date` pipe populates doc.spans['dates']\n    extensions : List[Tuple[str, T.DataType]], by default []\n        Spans extensions to add to the extracted results:\n        For instance, if `extensions=[\"score_name\"]`, the extracted result\n        will include, for each entity, `ent._.score_name`.\n    kwargs : Dict[str, Any]\n        Additional parameters depending on the `how` argument.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame with one line per extraction\n    \"\"\"\n\n    module = get_module(note)\n\n    if module == DataFrameModules.PANDAS:\n\n        kwargs.pop(\"dtypes\", None)\n\n        if n_jobs == 1:\n\n            return simple_pipe(\n                note=note,\n                nlp=nlp,\n                context=context,\n                results_extractor=results_extractor,\n                additional_spans=additional_spans,\n                extensions=extensions,\n                **kwargs,\n            )\n\n        else:\n\n            return parallel_pipe(\n                note=note,\n                nlp=nlp,\n                context=context,\n                results_extractor=results_extractor,\n                additional_spans=additional_spans,\n                extensions=extensions,\n                n_jobs=n_jobs,\n                **kwargs,\n            )\n\n    if type(extensions) != dict:\n        if extensions:\n            raise ValueError(\n\"\"\"\n                When using Spark or Koalas, you should provide extension names\n                along with the extension type (as a dictionnary):\n                `d[extension_name] = extension_type`\n                \"\"\"  # noqa W291\n            )\n        else:\n            extensions = {}\n\n    from .distributed import custom_pipe\n    from .distributed import pipe as distributed_pipe\n\n    if results_extractor is None:\n\n        return distributed_pipe(\n            note=note,\n            nlp=nlp,\n            context=context,\n            additional_spans=additional_spans,\n            extensions=extensions,\n            **kwargs,\n        )\n    else:\n\n        dtypes = kwargs.pop(\"dtypes\")\n\n        return custom_pipe(\n            note=note,\n            nlp=nlp,\n            context=context,\n            results_extractor=results_extractor,\n            dtypes=dtypes,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/utils/","title":"<code>edsnlp.utils</code>","text":""},{"location":"reference/utils/blocs/","title":"<code>edsnlp.utils.blocs</code>","text":"<p>Utility that extracts code blocs and runs them.</p> <p>Largely inspired by https://github.com/koaning/mktestdocs</p>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_outputs","title":"<code>check_outputs(code)</code>","text":"<p>Looks for output patterns, and modifies the bloc:</p> <ol> <li>The preceding line becomes <code>v = expr</code></li> <li>The output line becomes an <code>assert</code> statement</li> </ol> PARAMETER DESCRIPTION <code>code</code> <p>Code block</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Modified code bloc with assert statements</p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def check_outputs(code: str) -&gt; str:\n\"\"\"\n    Looks for output patterns, and modifies the bloc:\n\n    1. The preceding line becomes `#!python v = expr`\n    2. The output line becomes an `#!python assert` statement\n\n    Parameters\n    ----------\n    code : str\n        Code block\n\n    Returns\n    -------\n    str\n        Modified code bloc with assert statements\n    \"\"\"\n\n    lines: List[str] = code.split(\"\\n\")\n    code = []\n\n    skip = False\n\n    if len(lines) &lt; 2:\n        return code\n\n    for expression, output in zip(lines[:-1], lines[1:]):\n        if skip:\n            skip = not skip\n            continue\n\n        if output.startswith(OUTPUT_PATTERN):\n            expression = f\"v = {expression}\"\n\n            output = output[len(OUTPUT_PATTERN) :].replace('\"', r\"\\\"\")\n            output = f'assert repr(v) == \"{output}\" or str(v) == \"{output}\"'\n\n            code.append(expression)\n            code.append(output)\n\n            skip = True\n\n        else:\n            code.append(expression)\n\n    if not skip:\n        code.append(output)\n\n    return \"\\n\".join(code)\n</code></pre>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.remove_indentation","title":"<code>remove_indentation(code, indent)</code>","text":"<p>Remove indentation from a code bloc.</p> PARAMETER DESCRIPTION <code>code</code> <p>Code bloc</p> <p> TYPE: <code>str</code> </p> <code>indent</code> <p>Level of indentation</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Modified code bloc</p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def remove_indentation(code: str, indent: int) -&gt; str:\n\"\"\"\n    Remove indentation from a code bloc.\n\n    Parameters\n    ----------\n    code : str\n        Code bloc\n    indent : int\n        Level of indentation\n\n    Returns\n    -------\n    str\n        Modified code bloc\n    \"\"\"\n\n    if not indent:\n        return code\n\n    lines = []\n\n    for line in code.split(\"\\n\"):\n        lines.append(line[indent:])\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.grab_code_blocks","title":"<code>grab_code_blocks(docstring, lang='python')</code>","text":"<p>Given a docstring, grab all the markdown codeblocks found in docstring.</p> PARAMETER DESCRIPTION <code>docstring</code> <p>Full text.</p> <p> TYPE: <code>str</code> </p> <code>lang</code> <p>Language to execute, by default \"python\"</p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>'python'</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>Extracted code blocks</p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def grab_code_blocks(docstring: str, lang=\"python\") -&gt; List[str]:\n\"\"\"\n    Given a docstring, grab all the markdown codeblocks found in docstring.\n\n    Parameters\n    ----------\n    docstring : str\n        Full text.\n    lang : str, optional\n        Language to execute, by default \"python\"\n\n    Returns\n    -------\n    List[str]\n        Extracted code blocks\n    \"\"\"\n    codeblocks = []\n\n    for match in BLOCK_PATTERN.finditer(docstring):\n        d = match.groupdict()\n\n        if d[\"skip\"]:\n            continue\n\n        if lang in d[\"title\"]:\n            code = remove_indentation(d[\"code\"], len(d[\"indent\"]))\n            code = check_outputs(code)\n            codeblocks.append(code)\n\n    return codeblocks\n</code></pre>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.printer","title":"<code>printer(code)</code>","text":"<p>Prints a code bloc with lines for easier debugging.</p> PARAMETER DESCRIPTION <code>code</code> <p>Code bloc.</p> <p> TYPE: <code>str</code> </p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def printer(code: str) -&gt; None:\n\"\"\"\n    Prints a code bloc with lines for easier debugging.\n\n    Parameters\n    ----------\n    code : str\n        Code bloc.\n    \"\"\"\n    lines = []\n    for i, line in enumerate(code.split(\"\\n\")):\n        lines.append(f\"{i + 1:03}  {line}\")\n\n    print(\"\\n\".join(lines))\n</code></pre>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_docstring","title":"<code>check_docstring(obj, lang='')</code>","text":"<p>Given a function, test the contents of the docstring.</p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def check_docstring(obj, lang=\"\"):\n\"\"\"\n    Given a function, test the contents of the docstring.\n    \"\"\"\n    for b in grab_code_blocks(obj.__doc__, lang=lang):\n        try:\n            exec(b, {\"__MODULE__\": \"__main__\"})\n        except Exception:\n            print(f\"Error Encountered in `{obj.__name__}`. Caused by:\\n\")\n            printer(b)\n            raise\n</code></pre>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_raw_string","title":"<code>check_raw_string(raw, lang='python')</code>","text":"<p>Given a raw string, test the contents.</p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def check_raw_string(raw, lang=\"python\"):\n\"\"\"\n    Given a raw string, test the contents.\n    \"\"\"\n    for b in grab_code_blocks(raw, lang=lang):\n        try:\n            exec(b, {\"__MODULE__\": \"__main__\"})\n        except Exception:\n            printer(b)\n            raise\n</code></pre>"},{"location":"reference/utils/blocs/#edsnlp.utils.blocs.check_md_file","title":"<code>check_md_file(path, memory=False)</code>","text":"<p>Given a markdown file, parse the contents for Python code blocs and check that each independant bloc does not cause an error.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to the markdown file to execute.</p> <p> TYPE: <code>Path</code> </p> <code>memory</code> <p>Whether to keep results from one bloc to the next, by default <code>False</code></p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>False</code> </p> Source code in <code>edsnlp/utils/blocs.py</code> <pre><code>def check_md_file(path: Path, memory: bool = False) -&gt; None:\n\"\"\"\n    Given a markdown file, parse the contents for Python code blocs\n    and check that each independant bloc does not cause an error.\n\n    Parameters\n    ----------\n    path : Path\n        Path to the markdown file to execute.\n    memory : bool, optional\n        Whether to keep results from one bloc to the next, by default `#!python False`\n    \"\"\"\n    text = Path(path).read_text()\n    if memory:\n        check_raw_file_full(text, lang=\"python\")\n    else:\n        check_raw_string(text, lang=\"python\")\n</code></pre>"},{"location":"reference/utils/collections/","title":"<code>edsnlp.utils.collections</code>","text":""},{"location":"reference/utils/collections/#edsnlp.utils.collections.dedup","title":"<code>dedup(sequence, key=None)</code>","text":"<p>Deduplicate a sequence, keeping the last occurrence of each item.</p> PARAMETER DESCRIPTION <code>sequence</code> <p>Sequence to deduplicate</p> <p> TYPE: <code>Sequence</code> </p> <code>key</code> <p>Key function to use for deduplication, by default None</p> <p> TYPE: <code>Callable, optional</code> DEFAULT: <code>None</code> </p> Source code in <code>edsnlp/utils/collections.py</code> <pre><code>def dedup(sequence, key=None):\n\"\"\"\n    Deduplicate a sequence, keeping the last occurrence of each item.\n\n    Parameters\n    ----------\n    sequence : Sequence\n        Sequence to deduplicate\n    key : Callable, optional\n        Key function to use for deduplication, by default None\n    \"\"\"\n    key = (lambda x: x) if key is None else key\n    return list({key(item): item for item in sequence}.values())\n</code></pre>"},{"location":"reference/utils/colors/","title":"<code>edsnlp.utils.colors</code>","text":""},{"location":"reference/utils/colors/#edsnlp.utils.colors.create_colors","title":"<code>create_colors(labels)</code>","text":"<p>Assign a colour for each label, using category20 palette. The method loops over the colour palette in case there are too many labels.</p> PARAMETER DESCRIPTION <code>labels</code> <p>List of labels to colorise in displacy.</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>Dict[str, str]</code> <p>A displacy-compatible colour assignment.</p> Source code in <code>edsnlp/utils/colors.py</code> <pre><code>def create_colors(labels: List[str]) -&gt; Dict[str, str]:\n\"\"\"\n    Assign a colour for each label, using category20 palette.\n    The method loops over the colour palette in case there are too many labels.\n\n    Parameters\n    ----------\n    labels : List[str]\n        List of labels to colorise in displacy.\n\n    Returns\n    -------\n    Dict[str, str]\n        A displacy-compatible colour assignment.\n    \"\"\"\n\n    colors = {label: cat for label, cat in zip(labels, cycle(CATEGORY20))}\n\n    return colors\n</code></pre>"},{"location":"reference/utils/deprecation/","title":"<code>edsnlp.utils.deprecation</code>","text":""},{"location":"reference/utils/deprecation/#edsnlp.utils.deprecation.deprecated_factory","title":"<code>deprecated_factory(name, new_name=None, default_config=None, func=None, **kwargs)</code>","text":"<p>Execute the Language.factory method on a modified factory function. The modification adds a deprecation warning.</p> PARAMETER DESCRIPTION <code>name</code> <p>The deprecated name for the pipeline</p> <p> TYPE: <code>str</code> </p> <code>new_name</code> <p>The new name for the pipeline, which should be used, by default None</p> <p> TYPE: <code>Optional[str], optional</code> DEFAULT: <code>None</code> </p> <code>default_config</code> <p>The configuration that should be passed to Language.factory, by default None</p> <p> TYPE: <code>Optional[Dict[str, Any]], optional</code> DEFAULT: <code>None</code> </p> <code>func</code> <p>The function to decorate, by default None</p> <p> TYPE: <code>Optional[Callable], optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Callable</code> Source code in <code>edsnlp/utils/deprecation.py</code> <pre><code>def deprecated_factory(\n    name: str,\n    new_name: Optional[str] = None,\n    default_config: Optional[Dict[str, Any]] = None,\n    func: Optional[Callable] = None,\n    **kwargs,\n) -&gt; Callable:\n\"\"\"\n    Execute the Language.factory method on a modified factory function.\n    The modification adds a deprecation warning.\n\n    Parameters\n    ----------\n    name : str\n        The deprecated name for the pipeline\n    new_name : Optional[str], optional\n        The new name for the pipeline, which should be used, by default None\n    default_config : Optional[Dict[str, Any]], optional\n        The configuration that should be passed to Language.factory, by default None\n    func : Optional[Callable], optional\n        The function to decorate, by default None\n\n    Returns\n    -------\n    Callable\n    \"\"\"\n\n    if default_config is None:\n        default_config = dict()\n\n    wrapper = Language.factory(name, default_config=default_config, **kwargs)\n\n    def wrap(factory):\n\n        # Define decorator\n        # We use micheles' decorator package to keep the same signature\n        # See https://github.com/micheles/decorator/\n        @decorator\n        def decorate(\n            f,\n            *args,\n            **kwargs,\n        ):\n            deprecation(name, new_name)\n            return f(\n                *args,\n                **kwargs,\n            )\n\n        decorated = decorate(factory)\n\n        wrapper(decorated)\n\n        return factory\n\n    if func is not None:\n        return wrap(func)\n\n    return wrap\n</code></pre>"},{"location":"reference/utils/examples/","title":"<code>edsnlp.utils.examples</code>","text":""},{"location":"reference/utils/examples/#edsnlp.utils.examples.find_matches","title":"<code>find_matches(example)</code>","text":"<p>Finds entities within the example.</p> PARAMETER DESCRIPTION <code>example</code> <p>Example to process.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[re.Match]</code> <p>List of matches for entities.</p> Source code in <code>edsnlp/utils/examples.py</code> <pre><code>def find_matches(example: str) -&gt; List[re.Match]:\n\"\"\"\n    Finds entities within the example.\n\n    Parameters\n    ----------\n    example : str\n        Example to process.\n\n    Returns\n    -------\n    List[re.Match]\n        List of matches for entities.\n    \"\"\"\n    return list(entity_pattern.finditer(example))\n</code></pre>"},{"location":"reference/utils/examples/#edsnlp.utils.examples.parse_match","title":"<code>parse_match(match)</code>","text":"<p>Parse a regex match representing an entity.</p> PARAMETER DESCRIPTION <code>match</code> <p>Match for an entity.</p> <p> TYPE: <code>re.Match</code> </p> RETURNS DESCRIPTION <code>Match</code> <p>Usable representation for the entity match.</p> Source code in <code>edsnlp/utils/examples.py</code> <pre><code>def parse_match(match: re.Match) -&gt; Match:\n\"\"\"\n    Parse a regex match representing an entity.\n\n    Parameters\n    ----------\n    match : re.Match\n        Match for an entity.\n\n    Returns\n    -------\n    Match\n        Usable representation for the entity match.\n    \"\"\"\n\n    lexical_variant = match.group()\n    start_char = match.start()\n    end_char = match.end()\n\n    text = text_pattern.findall(lexical_variant)[0]\n    modifiers = modifiers_pattern.findall(lexical_variant)[0]\n\n    m = Match(start_char=start_char, end_char=end_char, text=text, modifiers=modifiers)\n\n    return m\n</code></pre>"},{"location":"reference/utils/examples/#edsnlp.utils.examples.parse_example","title":"<code>parse_example(example)</code>","text":"<p>Parses an example : finds examples and removes the tags.</p> PARAMETER DESCRIPTION <code>example</code> <p>Example to process.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tuple[str, List[Entity]]</code> <p>Cleaned text and extracted entities.</p> Source code in <code>edsnlp/utils/examples.py</code> <pre><code>def parse_example(example: str) -&gt; Tuple[str, List[Entity]]:\n\"\"\"\n    Parses an example : finds examples and removes the tags.\n\n    Parameters\n    ----------\n    example : str\n        Example to process.\n\n    Returns\n    -------\n    Tuple[str, List[Entity]]\n        Cleaned text and extracted entities.\n    \"\"\"\n\n    matches = [parse_match(match) for match in find_matches(example=example)]\n    text = \"\"\n    entities = []\n\n    cursor = 0\n\n    for match in matches:\n\n        text += example[cursor : match.start_char]\n        start_char = len(text)\n        text += match.text\n        end_char = len(text)\n\n        cursor = match.end_char\n\n        entity = Entity(\n            start_char=start_char,\n            end_char=end_char,\n            modifiers=[\n                Modifier.parse_obj(m.groupdict())\n                for m in single_modifiers_pattern.finditer(match.modifiers)\n            ],\n        )\n\n        entities.append(entity)\n\n    text += example[cursor:]\n\n    return text, entities\n</code></pre>"},{"location":"reference/utils/extensions/","title":"<code>edsnlp.utils.extensions</code>","text":""},{"location":"reference/utils/extensions/#edsnlp.utils.extensions.rgetattr","title":"<code>rgetattr(obj, attr, *args)</code>","text":"<p>Get attribute recursively</p> PARAMETER DESCRIPTION <code>obj</code> <p>An object</p> <p> TYPE: <code>Any</code> </p> <code>attr</code> <p>The name of the attribute to get. Can contain dots.</p> <p> TYPE: <code>str</code> </p> Source code in <code>edsnlp/utils/extensions.py</code> <pre><code>def rgetattr(obj: Any, attr: str, *args: List[Any]) -&gt; Any:\n\"\"\"\n    Get attribute recursively\n\n    Parameters\n    ----------\n    obj : Any\n        An object\n    attr : str\n        The name of the attribute to get. Can contain dots.\n    \"\"\"\n\n    def _getattr(obj, attr):\n        return None if obj is None else getattr(obj, attr, *args)\n\n    return functools.reduce(_getattr, [obj] + attr.split(\".\"))\n</code></pre>"},{"location":"reference/utils/filter/","title":"<code>edsnlp.utils.filter</code>","text":""},{"location":"reference/utils/filter/#edsnlp.utils.filter.default_sort_key","title":"<code>default_sort_key(span)</code>","text":"<p>Returns the sort key for filtering spans.</p> PARAMETER DESCRIPTION <code>span</code> <p>Span to sort.</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>key</code> <p>Sort key.</p> <p> TYPE: <code>Tuple(int, int)</code> </p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def default_sort_key(span: Span) -&gt; Tuple[int, int]:\n\"\"\"\n    Returns the sort key for filtering spans.\n\n    Parameters\n    ----------\n    span : Span\n        Span to sort.\n\n    Returns\n    -------\n    key : Tuple(int, int)\n        Sort key.\n    \"\"\"\n    if isinstance(span, tuple):\n        span = span[0]\n    return span.end - span.start, -span.start\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.start_sort_key","title":"<code>start_sort_key(span)</code>","text":"<p>Returns the sort key for filtering spans by start order.</p> PARAMETER DESCRIPTION <code>span</code> <p>Span to sort.</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>key</code> <p>Sort key.</p> <p> TYPE: <code>Tuple(int, int)</code> </p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def start_sort_key(span: Union[Span, Tuple[Span, Any]]) -&gt; Tuple[int, int]:\n\"\"\"\n    Returns the sort key for filtering spans by start order.\n\n    Parameters\n    ----------\n    span : Span\n        Span to sort.\n\n    Returns\n    -------\n    key : Tuple(int, int)\n        Sort key.\n    \"\"\"\n    if isinstance(span, tuple):\n        span = span[0]\n    return span.start\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.filter_spans","title":"<code>filter_spans(spans, label_to_remove=None, return_discarded=False, sort_key=default_sort_key)</code>","text":"<p>Re-definition of spacy's filtering function, that returns discarded spans as well as filtered ones.</p> <p>Can also accept a <code>label_to_remove</code> argument, useful for filtering out pseudo cues. If set, <code>results</code> can contain overlapping spans: only spans overlapping with excluded labels are removed. The main expected use case is for pseudo-cues.</p> <p>It can handle an iterable of tuples instead of an iterable of <code>Span</code>s. The primary use-case is the use with the <code>RegexMatcher</code>'s capacity to return the span's <code>groupdict</code>.</p> <p>The spaCy documentation states:</p> <p>Filter a sequence of spans and remove duplicates or overlaps. Useful for creating named entities (where one token can only be part of one entity) or when merging spans with <code>Retokenizer.merge</code>. When spans overlap, the (first) longest span is preferred over shorter spans.</p> <p>Filtering out spans</p> <p>If the <code>label_to_remove</code> argument is supplied, it might be tempting to filter overlapping spans that are not part of a label to remove.</p> <p>The reason we keep all other possibly overlapping labels is that in qualifier pipelines, the same cue can precede and follow a marked entity. Hence we need to keep every example.</p> PARAMETER DESCRIPTION <code>spans</code> <p>Spans to filter.</p> <p> TYPE: <code>Iterable[Union[Span, Tuple[Span, Any]]]</code> </p> <code>return_discarded</code> <p>Whether to return discarded spans.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>label_to_remove</code> <p>Label to remove. If set, results can contain overlapping spans.</p> <p> TYPE: <code>str, optional</code> DEFAULT: <code>None</code> </p> <code>sort_key</code> <p>Key to sorting spans before applying overlap conflict resolution. A span with a higher key will have precedence over another span. By default, the largest, leftmost spans are selected first.</p> <p> TYPE: <code>Callable[Span, Any], optional</code> DEFAULT: <code>default_sort_key</code> </p> RETURNS DESCRIPTION <code>results</code> <p>Filtered spans</p> <p> TYPE: <code>List[Union[Span, Tuple[Span, Any]]]</code> </p> <code>discarded</code> <p>Discarded spans</p> <p> TYPE: <code>List[Union[Span, Tuple[Span, Any]]], optional</code> </p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def filter_spans(\n    spans: Iterable[Union[\"Span\", Tuple[\"Span\", Any]]],\n    label_to_remove: Optional[str] = None,\n    return_discarded: bool = False,\n    sort_key: Callable[[Span], Any] = default_sort_key,\n) -&gt; Union[\n    List[Union[Span, Tuple[Span, Any]]],\n    Tuple[List[Union[Span, Tuple[Span, Any]]], List[Union[Span, Tuple[Span, Any]]]],\n]:\n\"\"\"\n    Re-definition of spacy's filtering function, that returns discarded spans\n    as well as filtered ones.\n\n    Can also accept a `label_to_remove` argument, useful for filtering out\n    pseudo cues. If set, `results` can contain overlapping spans: only\n    spans overlapping with excluded labels are removed. The main expected\n    use case is for pseudo-cues.\n\n    It can handle an iterable of tuples instead of an iterable of `Span`s.\n    The primary use-case is the use with the `RegexMatcher`'s capacity to\n    return the span's `groupdict`.\n\n    !!! note \"\"\n\n        The **spaCy documentation states**:\n\n        &gt; Filter a sequence of spans and remove duplicates or overlaps.\n        &gt; Useful for creating named entities (where one token can only\n        &gt; be part of one entity) or when merging spans with\n        &gt; `Retokenizer.merge`. When spans overlap, the (first)\n        &gt; longest span is preferred over shorter spans.\n\n    !!! danger \"Filtering out spans\"\n\n        If the `label_to_remove` argument is supplied, it might be tempting to\n        filter overlapping spans that are not part of a label to remove.\n\n        The reason we keep all other possibly overlapping labels is that in qualifier\n        pipelines, the same cue can precede **and** follow a marked entity.\n        Hence we need to keep every example.\n\n    Parameters\n    ----------\n    spans : Iterable[Union[\"Span\", Tuple[\"Span\", Any]]]\n        Spans to filter.\n    return_discarded : bool\n        Whether to return discarded spans.\n    label_to_remove : str, optional\n        Label to remove. If set, results can contain overlapping spans.\n    sort_key : Callable[Span, Any], optional\n        Key to sorting spans before applying overlap conflict resolution.\n        A span with a higher key will have precedence over another span.\n        By default, the largest, leftmost spans are selected first.\n\n    Returns\n    -------\n    results : List[Union[Span, Tuple[Span, Any]]]\n        Filtered spans\n    discarded : List[Union[Span, Tuple[Span, Any]]], optional\n        Discarded spans\n    \"\"\"\n    sorted_spans = sorted(spans, key=sort_key, reverse=True)\n    result = []\n    discarded = []\n    seen_tokens = set()\n    for span in sorted_spans:\n        s = span if isinstance(span, Span) else span[0]\n        # Check for end - 1 here because boundaries are inclusive\n        token_range = set(range(s.start, s.end))\n        if token_range.isdisjoint(seen_tokens):\n            if label_to_remove is None or s.label_ != label_to_remove:\n                result.append(span)\n            if label_to_remove is None or s.label_ == label_to_remove:\n                seen_tokens.update(token_range)\n        elif label_to_remove is None or s.label_ != label_to_remove:\n            discarded.append(span)\n\n    result = sorted(result, key=start_sort_key)\n    discarded = sorted(discarded, key=start_sort_key)\n\n    if return_discarded:\n        return result, discarded\n\n    return result\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.consume_spans","title":"<code>consume_spans(spans, filter, second_chance=None)</code>","text":"<p>Consume a list of span, according to a filter.</p> <p>Warning</p> <p>This method makes the hard hypothesis that:</p> <ol> <li>Spans are sorted.</li> <li>Spans are consumed in sequence and only once.</li> </ol> <p>The second item is problematic for the way we treat long entities, hence the <code>second_chance</code> parameter, which lets entities be seen more than once.</p> PARAMETER DESCRIPTION <code>spans</code> <p>List of spans to filter</p> <p> TYPE: <code>List of spans</code> </p> <code>filter</code> <p>Filtering function. Should return True when the item is to be included.</p> <p> TYPE: <code>Callable</code> </p> <code>second_chance</code> <p>Optional list of spans to include again (useful for long entities), by default None</p> <p> TYPE: <code>List of spans, optional</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>matches</code> <p>List of spans consumed by the filter.</p> <p> TYPE: <code>List of spans</code> </p> <code>remainder</code> <p>List of remaining spans in the original <code>spans</code> parameter.</p> <p> TYPE: <code>List of spans</code> </p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def consume_spans(\n    spans: List[Span],\n    filter: Callable,\n    second_chance: Optional[List[Span]] = None,\n) -&gt; Tuple[List[Span], List[Span]]:\n\"\"\"\n    Consume a list of span, according to a filter.\n\n    !!! warning\n        This method makes the hard hypothesis that:\n\n        1. Spans are sorted.\n        2. Spans are consumed in sequence and only once.\n\n        The second item is problematic for the way we treat long entities,\n        hence the `second_chance` parameter, which lets entities be seen\n        more than once.\n\n    Parameters\n    ----------\n    spans : List of spans\n        List of spans to filter\n    filter : Callable\n        Filtering function. Should return True when the item is to be included.\n    second_chance : List of spans, optional\n        Optional list of spans to include again (useful for long entities),\n        by default None\n\n    Returns\n    -------\n    matches : List of spans\n        List of spans consumed by the filter.\n    remainder : List of spans\n        List of remaining spans in the original `spans` parameter.\n    \"\"\"\n\n    if not second_chance:\n        second_chance = []\n    else:\n        second_chance = [m for m in second_chance if filter(m)]\n\n    if not spans:\n        return second_chance, []\n\n    for i, span in enumerate(spans):\n        if not filter(span):\n            break\n        else:\n            i += 1\n\n    matches = spans[:i]\n    remainder = spans[i:]\n\n    matches.extend(second_chance)\n\n    return matches, remainder\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.get_spans","title":"<code>get_spans(spans, label)</code>","text":"<p>Extracts spans with a given label. Prefer using hash label for performance reasons.</p> PARAMETER DESCRIPTION <code>spans</code> <p>List of spans to filter.</p> <p> TYPE: <code>List[Span]</code> </p> <code>label</code> <p>Label to filter on.</p> <p> TYPE: <code>Union[int, str]</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>Filtered spans.</p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def get_spans(spans: List[Span], label: Union[int, str]) -&gt; List[Span]:\n\"\"\"\n    Extracts spans with a given label.\n    Prefer using hash label for performance reasons.\n\n    Parameters\n    ----------\n    spans : List[Span]\n        List of spans to filter.\n    label : Union[int, str]\n        Label to filter on.\n\n    Returns\n    -------\n    List[Span]\n        Filtered spans.\n    \"\"\"\n    if isinstance(label, int):\n        return [span for span in spans if span.label == label]\n    else:\n        return [span for span in spans if span.label_ == label]\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.span_f1","title":"<code>span_f1(a, b)</code>","text":"<p>Computes the F1 overlap between two spans.</p> PARAMETER DESCRIPTION <code>a</code> <p>First span</p> <p> TYPE: <code>Span</code> </p> <code>b</code> <p>Second span</p> <p> TYPE: <code>Span</code> </p> RETURNS DESCRIPTION <code>float</code> <p>F1 overlap</p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def span_f1(a: Span, b: Span) -&gt; float:\n\"\"\"\n    Computes the F1 overlap between two spans.\n\n    Parameters\n    ----------\n    a: Span\n        First span\n    b: Span\n        Second span\n\n    Returns\n    -------\n    float\n        F1 overlap\n    \"\"\"\n    start_a, end_a = a.start, a.end\n    start_b, end_b = b.start, b.end\n    overlap = max(0, min(end_a, end_b) - max(start_a, start_b))\n    return 2 * overlap / (end_a - start_a + end_b - start_b)\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.align_spans","title":"<code>align_spans(source, target, sort_by_overlap=False)</code>","text":"<p>Aligns two lists of spans, by matching source spans that overlap target spans. This function is optimized to avoid quadratic complexity.</p> PARAMETER DESCRIPTION <code>source</code> <p>List of spans to align.</p> <p> TYPE: <code>List[Span]</code> </p> <code>target</code> <p>List of spans to align.</p> <p> TYPE: <code>List[Span]</code> </p> <code>sort_by_overlap</code> <p>Whether to sort the aligned spans by maximum dice/f1 overlap with the target span.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[List[Span]]</code> <p>Subset of <code>source</code> spans for each target span</p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def align_spans(\n    source: Sequence[Span],\n    target: Sequence[Span],\n    sort_by_overlap: bool = False,\n) -&gt; List[List[Span]]:\n\"\"\"\n    Aligns two lists of spans, by matching source spans that overlap target spans.\n    This function is optimized to avoid quadratic complexity.\n\n    Parameters\n    ----------\n    source : List[Span]\n        List of spans to align.\n    target : List[Span]\n        List of spans to align.\n    sort_by_overlap : bool\n        Whether to sort the aligned spans by maximum dice/f1 overlap\n        with the target span.\n\n    Returns\n    -------\n    List[List[Span]]\n        Subset of `source` spans for each target span\n    \"\"\"\n    source = sorted(source, key=lambda x: (x.start, x.end))\n    target = sorted(target, key=lambda x: (x.start, x.end))\n\n    aligned = [set() for _ in target]\n    source_idx = 0\n    for target_idx in range(len(target)):\n        while source[source_idx].end &lt;= target[target_idx].start:\n            source_idx += 1\n        i = source_idx\n        while i &lt; len(source) and source[i].start &lt;= target[target_idx].end:\n            aligned[target_idx].add(source[i])\n            i += 1\n\n    aligned = [list(span_set) for span_set in aligned]\n\n    # Sort the aligned spans by maximum dice/f1 overlap with the target span\n    if sort_by_overlap:\n        aligned = [\n            sorted(span_set, key=lambda x: span_f1(x, y), reverse=True)\n            for span_set, y in zip(aligned, target)\n        ]\n\n    return aligned\n</code></pre>"},{"location":"reference/utils/filter/#edsnlp.utils.filter.get_span_group","title":"<code>get_span_group(doclike, group)</code>","text":"<p>Get the spans of a span group that are contained inside a doclike object.</p> PARAMETER DESCRIPTION <code>doclike</code> <p>Doclike object to act as a mask.</p> <p> TYPE: <code>Union[Doc, Span]</code> </p> <code>group</code> <p>Group name from which to get the spans.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>List[Span]</code> <p>List of spans.</p> Source code in <code>edsnlp/utils/filter.py</code> <pre><code>def get_span_group(doclike: Union[Doc, Span], group: str) -&gt; List[Span]:\n\"\"\"\n    Get the spans of a span group that are contained inside a doclike object.\n\n    Parameters\n    ----------\n    doclike : Union[Doc, Span]\n        Doclike object to act as a mask.\n    group : str\n        Group name from which to get the spans.\n\n    Returns\n    -------\n    List[Span]\n        List of spans.\n    \"\"\"\n    if isinstance(doclike, Doc):\n        return [span for span in doclike.spans.get(group, ())]\n    else:\n        return [\n            span\n            for span in doclike.doc.spans.get(group, ())\n            if span.start &gt;= doclike.start and span.end &lt;= doclike.end\n        ]\n</code></pre>"},{"location":"reference/utils/inclusion/","title":"<code>edsnlp.utils.inclusion</code>","text":""},{"location":"reference/utils/inclusion/#edsnlp.utils.inclusion.check_inclusion","title":"<code>check_inclusion(span, start, end)</code>","text":"<p>Checks whether the span overlaps the boundaries.</p> PARAMETER DESCRIPTION <code>span</code> <p>Span to check.</p> <p> TYPE: <code>Span</code> </p> <code>start</code> <p>Start of the boundary</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>End of the boundary</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the span overlaps the boundaries.</p> Source code in <code>edsnlp/utils/inclusion.py</code> <pre><code>def check_inclusion(span: Span, start: int, end: int) -&gt; bool:\n\"\"\"\n    Checks whether the span overlaps the boundaries.\n\n    Parameters\n    ----------\n    span : Span\n        Span to check.\n    start : int\n        Start of the boundary\n    end : int\n        End of the boundary\n\n    Returns\n    -------\n    bool\n        Whether the span overlaps the boundaries.\n    \"\"\"\n\n    if span.start &gt;= end or span.end &lt;= start:\n        return False\n    return True\n</code></pre>"},{"location":"reference/utils/inclusion/#edsnlp.utils.inclusion.check_sent_inclusion","title":"<code>check_sent_inclusion(span, start, end)</code>","text":"<p>Checks whether the span overlaps the boundaries.</p> PARAMETER DESCRIPTION <code>span</code> <p>Span to check.</p> <p> TYPE: <code>Span</code> </p> <code>start</code> <p>Start of the boundary</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>End of the boundary</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the span overlaps the boundaries.</p> Source code in <code>edsnlp/utils/inclusion.py</code> <pre><code>def check_sent_inclusion(span: Span, start: int, end: int) -&gt; bool:\n\"\"\"\n    Checks whether the span overlaps the boundaries.\n\n    Parameters\n    ----------\n    span : Span\n        Span to check.\n    start : int\n        Start of the boundary\n    end : int\n        End of the boundary\n\n    Returns\n    -------\n    bool\n        Whether the span overlaps the boundaries.\n    \"\"\"\n    if span.sent.start &gt;= end or span.sent.end &lt;= start:\n        return False\n    return True\n</code></pre>"},{"location":"reference/utils/lists/","title":"<code>edsnlp.utils.lists</code>","text":""},{"location":"reference/utils/lists/#edsnlp.utils.lists.flatten","title":"<code>flatten(my_list)</code>","text":"<p>Flatten (if necessary) a list of sublists</p> PARAMETER DESCRIPTION <code>my_list</code> <p>A list of items, each items in turn can be a list</p> <p> TYPE: <code>List</code> </p> RETURNS DESCRIPTION <code>List</code> <p>A flatten list</p> Source code in <code>edsnlp/utils/lists.py</code> <pre><code>def flatten(my_list: List):\n\"\"\"\n    Flatten (if necessary) a list of sublists\n\n    Parameters\n    ----------\n    my_list : List\n        A list of items, each items in turn can be a list\n\n    Returns\n    -------\n    List\n        A flatten list\n    \"\"\"\n    if not my_list:\n        return my_list\n    my_list = [item if isinstance(item, list) else [item] for item in my_list]\n\n    return [item for sublist in my_list for item in sublist]\n</code></pre>"},{"location":"reference/utils/merge_configs/","title":"<code>edsnlp.utils.merge_configs</code>","text":""},{"location":"reference/utils/merge_configs/#edsnlp.utils.merge_configs.merge_configs","title":"<code>merge_configs(config, *updates, remove_extra=False)</code>","text":"<p>Deep merge two configs.</p> Source code in <code>edsnlp/utils/merge_configs.py</code> <pre><code>def merge_configs(\n    config: Union[Dict[str, Any], Config],\n    *updates: Union[Dict[str, Any], Config],\n    remove_extra: bool = False,\n) -&gt; Union[Dict[str, Any], Config]:\n\"\"\"Deep merge two configs.\"\"\"\n\n    def deep_set(current, path, val):\n        path = path.split(\".\")\n        for part in path[:-1]:\n            current = current[part]\n        current[path[-1]] = val\n\n    def rec(old, new):\n        if remove_extra:\n            # Filter out values in the original config that are not in defaults\n            keys = list(new.keys())\n            for key in keys:\n                if key not in old:\n                    del new[key]\n        for key, new_val in list(new.items()):\n            if \".\" in key:\n                deep_set(old, key, new_val)\n                continue\n\n            if key not in old:\n                old[key] = new_val\n                continue\n\n            old_val = old[key]\n            if isinstance(old_val, dict) and isinstance(new_val, dict):\n                old_promise = next((k for k in old_val if k.startswith(\"@\")), None)\n                new_promise = next((k for k in new_val if k.startswith(\"@\")), None)\n                if (\n                    new_promise is not None\n                    and old_promise != new_promise\n                    or old_val.get(old_promise) != new_val.get(new_promise)\n                ):\n                    old[key] = new_val\n                else:\n                    rec(old[key], new_val)\n            else:\n                old[key] = new_val\n        return old\n\n    config = deepcopy(config)\n    for u in updates:\n        u = deepcopy(u)\n        rec(config, u)\n    return Config(config)\n</code></pre>"},{"location":"reference/utils/regex/","title":"<code>edsnlp.utils.regex</code>","text":""},{"location":"reference/utils/regex/#edsnlp.utils.regex.make_pattern","title":"<code>make_pattern(patterns, with_breaks=False, name=None)</code>","text":"<p>Create OR pattern from a list of patterns.</p> PARAMETER DESCRIPTION <code>patterns</code> <p>List of patterns to merge.</p> <p> TYPE: <code>List[str]</code> </p> <code>with_breaks</code> <p>Whether to add breaks (<code>\\b</code>) on each side, by default False</p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>False</code> </p> <code>name</code> <p>Name of the group, using regex <code>?P&lt;&gt;</code> directive.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Merged pattern.</p> Source code in <code>edsnlp/utils/regex.py</code> <pre><code>def make_pattern(\n    patterns: List[str],\n    with_breaks: bool = False,\n    name: Optional[str] = None,\n) -&gt; str:\nr\"\"\"\n    Create OR pattern from a list of patterns.\n\n    Parameters\n    ----------\n    patterns : List[str]\n        List of patterns to merge.\n    with_breaks : bool, optional\n        Whether to add breaks (`\\b`) on each side, by default False\n    name: str, optional\n        Name of the group, using regex `?P&lt;&gt;` directive.\n\n    Returns\n    -------\n    str\n        Merged pattern.\n    \"\"\"\n\n    if name:\n        prefix = f\"(?P&lt;{name}&gt;\"\n    else:\n        prefix = \"(\"\n\n    # Sorting by length might be more efficient\n    patterns.sort(key=len, reverse=True)\n\n    pattern = prefix + \"|\".join(patterns) + \")\"\n\n    if with_breaks:\n        pattern = r\"\\b\" + pattern + r\"\\b\"\n\n    return pattern\n</code></pre>"},{"location":"reference/utils/regex/#edsnlp.utils.regex.compile_regex","title":"<code>compile_regex(reg, flags)</code>","text":"<p>This function tries to compile <code>reg</code>  using the <code>re</code> module, and fallbacks to the <code>regex</code> module that is more permissive.</p> PARAMETER DESCRIPTION <code>reg</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Union[re.Pattern, regex.Pattern]</code> Source code in <code>edsnlp/utils/regex.py</code> <pre><code>def compile_regex(reg: str, flags: re.RegexFlag):\n\"\"\"\n    This function tries to compile `reg`  using the `re` module, and\n    fallbacks to the `regex` module that is more permissive.\n\n    Parameters\n    ----------\n    reg: str\n\n    Returns\n    -------\n    Union[re.Pattern, regex.Pattern]\n    \"\"\"\n    try:\n        return re.compile(reg, flags=flags)\n    except re.error:\n        try:\n            return regex.compile(reg, flags=flags)\n        except regex.error:\n            raise Exception(\"Could not compile: {}\".format(repr(reg)))\n</code></pre>"},{"location":"reference/utils/resources/","title":"<code>edsnlp.utils.resources</code>","text":""},{"location":"reference/utils/resources/#edsnlp.utils.resources.get_verbs","title":"<code>get_verbs(verbs=None, check_contains=True)</code>","text":"<p>Extract verbs from the resources, as a pandas dataframe.</p> PARAMETER DESCRIPTION <code>verbs</code> <p>List of verbs to keep. Returns all verbs by default.</p> <p> TYPE: <code>List[str], optional</code> DEFAULT: <code>None</code> </p> <code>check_contains</code> <p>Whether to check that no verb is missing if a list of verbs was provided. By default True</p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>pd.DataFrame</code> <p>DataFrame containing conjugated verbs.</p> Source code in <code>edsnlp/utils/resources.py</code> <pre><code>def get_verbs(\n    verbs: Optional[List[str]] = None, check_contains: bool = True\n) -&gt; pd.DataFrame:\n\"\"\"\n    Extract verbs from the resources, as a pandas dataframe.\n\n    Parameters\n    ----------\n    verbs : List[str], optional\n        List of verbs to keep. Returns all verbs by default.\n    check_contains : bool, optional\n        Whether to check that no verb is missing if a list of verbs was provided.\n        By default True\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing conjugated verbs.\n    \"\"\"\n\n    conjugated_verbs = pd.read_csv(BASE_DIR / \"resources\" / \"verbs.csv.gz\")\n\n    if not verbs:\n        return conjugated_verbs\n\n    verbs = set(verbs)\n\n    selected_verbs = conjugated_verbs[conjugated_verbs.verb.isin(verbs)]\n\n    if check_contains:\n        assert len(verbs) == selected_verbs.verb.nunique(), \"Some verbs are missing !\"\n\n    return selected_verbs\n</code></pre>"},{"location":"reference/utils/resources/#edsnlp.utils.resources.get_adicap_dict","title":"<code>get_adicap_dict()</code>  <code>cached</code>","text":"RETURNS DESCRIPTION <code>Dict</code> Source code in <code>edsnlp/utils/resources.py</code> <pre><code>@lru_cache()\ndef get_adicap_dict():\n\"\"\"\n    Returns\n    -------\n    Dict\n    \"\"\"\n\n    with gzip.open(BASE_DIR / \"resources\" / \"adicap.json.gz\", \"r\") as fin:\n        decode_dict = json.loads(fin.read().decode(\"utf-8\"))\n\n    return decode_dict\n</code></pre>"},{"location":"reference/utils/training/","title":"<code>edsnlp.utils.training</code>","text":""},{"location":"reference/utils/training/#edsnlp.utils.training.make_spacy_corpus_config","title":"<code>make_spacy_corpus_config(train_data, dev_data, data_format=None, nlp=None, seed=0, reader='spacy.Corpus.v1')</code>","text":"<p>Helper to create a spacy's corpus config from training and dev data by loading the documents accordingly and exporting the documents using spacy's DocBin.</p> PARAMETER DESCRIPTION <code>train_data</code> <p>The training data. Can be: - a list of spacy.Doc - a path to a given dataset</p> <p> TYPE: <code>Union[str, List[Doc]]</code> </p> <code>dev_data</code> <p>The development data. Can be: - a list of spacy.Doc - a path to a given dataset - the number of documents to take from the training data - the fraction of documents to take from the training data</p> <p> TYPE: <code>Union[str, List[Doc], int, float]</code> </p> <code>data_format</code> <p>Optional data format to determine how we should load the documents from the disk</p> <p> TYPE: <code>Union[Optional[DataFormat], str]</code> DEFAULT: <code>None</code> </p> <code>nlp</code> <p>Optional spacy model to load documents from non-spacy formats (like brat)</p> <p> TYPE: <code>Optional[spacy.Language]</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>The seed if we need to shuffle the data when splitting the dataset</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>reader</code> <p>Which spacy reader to use when loading the data</p> <p> TYPE: <code>str</code> DEFAULT: <code>'spacy.Corpus.v1'</code> </p> RETURNS DESCRIPTION <code>Config</code> Source code in <code>edsnlp/utils/training.py</code> <pre><code>def make_spacy_corpus_config(\n    train_data: Union[str, List[Doc]],\n    dev_data: Union[str, List[Doc], int, float],\n    data_format: Union[Optional[DataFormat], str] = None,\n    nlp: Optional[spacy.Language] = None,\n    seed: int = 0,\n    reader: str = \"spacy.Corpus.v1\",\n):\n\"\"\"\n    Helper to create a spacy's corpus config from training and dev data by\n    loading the documents accordingly and exporting the documents using spacy's DocBin.\n\n    Parameters\n    ----------\n    train_data: Union[str, List[Doc]]\n        The training data. Can be:\n            - a list of spacy.Doc\n            - a path to a given dataset\n    dev_data: Union[str, List[Doc], int, float]\n        The development data. Can be:\n            - a list of spacy.Doc\n            - a path to a given dataset\n            - the number of documents to take from the training data\n            - the fraction of documents to take from the training data\n    data_format: Optional[DataFormat]\n        Optional data format to determine how we should load the documents from the disk\n    nlp: Optional[spacy.Language]\n        Optional spacy model to load documents from non-spacy formats (like brat)\n    seed: int\n        The seed if we need to shuffle the data when splitting the dataset\n    reader: str\n        Which spacy reader to use when loading the data\n\n    Returns\n    -------\n    Config\n    \"\"\"\n    fix_random_seed(seed)\n    train_docs = dev_docs = None\n\n    if data_format is None:\n        if isinstance(train_data, list):\n            assert all(isinstance(doc, Doc) for doc in train_data)\n            train_docs = train_data\n        elif isinstance(train_data, (str, Path)) and train_data.endswith(\".spacy\"):\n            data_format = DataFormat.spacy\n        else:\n            raise Exception()\n    if data_format == DataFormat.brat:\n        train_docs = list(BratConnector(train_data).brat2docs(nlp))\n    elif data_format == DataFormat.spacy:\n        if isinstance(dev_data, (float, int)):\n            train_docs = DocBin().from_disk(train_data)\n    elif train_docs is None:\n        raise Exception()\n\n    if isinstance(dev_data, (float, int)):\n        if isinstance(dev_data, float):\n            n_dev = int(len(train_docs)) * dev_data\n        else:\n            n_dev = dev_data\n        shuffle(train_docs)\n        dev_docs = train_docs[:n_dev]\n        train_docs = train_docs[n_dev:]\n    elif data_format == DataFormat.brat:\n        dev_docs = list(BratConnector(dev_data).brat2docs(nlp))\n    elif data_format == DataFormat.spacy:\n        pass\n    elif data_format is None:\n        if isinstance(dev_data, list):\n            assert all(isinstance(doc, Doc) for doc in dev_data)\n            dev_docs = dev_data\n        else:\n            raise Exception()\n    else:\n        raise Exception()\n\n    if data_format != \"spacy\" or isinstance(dev_data, (float, int)):\n        tmp_path = Path(tempfile.mkdtemp())\n        train_path = tmp_path / \"train.spacy\"\n        dev_path = tmp_path / \"dev.spacy\"\n\n        DocBin(docs=train_docs).to_disk(train_path)\n        DocBin(docs=dev_docs).to_disk(dev_path)\n    else:\n        train_path = train_data\n        dev_path = dev_data\n\n    return Config().from_str(\n        f\"\"\"\n        [corpora]\n\n        [corpora.train]\n            @readers = {reader}\n            path = {train_path}\n            max_length = 0\n            gold_preproc = false\n            limit = 0\n            augmenter = null\n\n        [corpora.dev]\n            @readers = {reader}\n            path = {dev_path}\n            max_length = 0\n            gold_preproc = false\n            limit = 0\n            augmenter = null\n    \"\"\"\n    )\n</code></pre>"},{"location":"reference/utils/training/#edsnlp.utils.training.train","title":"<code>train(nlp, output_path, config, use_gpu=-1)</code>","text":"<p>Training help to learn weight of trainable components in a pipeline. This function has been adapted from https://github.com/explosion/spaCy/blob/397197e/spacy/cli/train.py#L18</p> PARAMETER DESCRIPTION <code>nlp</code> <p>Spacy model to train</p> <p> TYPE: <code>spacy.Language</code> </p> <code>output_path</code> <p>Path to save the model</p> <p> TYPE: <code>Union[Path, str]</code> </p> <code>config</code> <p>Optional config overrides</p> <p> TYPE: <code>Union[Config, dict]</code> </p> <code>use_gpu</code> <p>Which gpu to use for training (-1 means CPU)</p> <p> TYPE: <code>int</code> DEFAULT: <code>-1</code> </p> Source code in <code>edsnlp/utils/training.py</code> <pre><code>def train(\n    nlp: spacy.Language,\n    output_path: Union[Path, str],\n    config: Union[Config, dict],\n    use_gpu: int = -1,\n):\n\"\"\"\n    Training help to learn weight of trainable components in a pipeline.\n    This function has been adapted from\n    https://github.com/explosion/spaCy/blob/397197e/spacy/cli/train.py#L18\n\n    Parameters\n    ----------\n    nlp: spacy.Language\n        Spacy model to train\n    output_path: Union[Path,str]\n        Path to save the model\n    config: Union[Config,dict]\n        Optional config overrides\n    use_gpu: bool\n        Which gpu to use for training (-1 means CPU)\n    \"\"\"\n    if \"components\" in config:\n        raise ValueError(\n            \"Cannot update components config after the model has been \" \"instantiated.\"\n        )\n\n    output_path = Path(output_path)\n    nlp.config = merge_configs(\n        nlp.config, DEFAULT_TRAIN_CONFIG, config, remove_extra=False\n    )\n\n    config = nlp.config.interpolate()\n\n    nlp.config = config\n    if \"seed\" not in config[\"training\"]:\n        raise ValueError(Errors.E1015.format(value=\"[training] seed\"))\n    if \"gpu_allocator\" not in config[\"training\"]:\n        raise ValueError(Errors.E1015.format(value=\"[training] gpu_allocator\"))\n    if config[\"training\"][\"seed\"] is not None:\n        fix_random_seed(config[\"training\"][\"seed\"])\n    allocator = config[\"training\"][\"gpu_allocator\"]\n    if use_gpu &gt;= 0 and allocator:\n        set_gpu_allocator(allocator)\n\n    # Use nlp config here before it's resolved to functions\n    sourced = get_sourced_components(config)\n\n    # ----------------------------- #\n    # Resolve functions and classes #\n    # ----------------------------- #\n    # Resolve all training-relevant sections using the filled nlp config\n    T = registry.resolve(config[\"training\"], schema=ConfigSchemaTraining)\n    dot_names = [T[\"train_corpus\"], T[\"dev_corpus\"]]\n    if not isinstance(T[\"train_corpus\"], str):\n        raise ConfigValidationError(\n            desc=Errors.E897.format(\n                field=\"training.train_corpus\", type=type(T[\"train_corpus\"])\n            )\n        )\n    if not isinstance(T[\"dev_corpus\"], str):\n        raise ConfigValidationError(\n            desc=Errors.E897.format(\n                field=\"training.dev_corpus\", type=type(T[\"dev_corpus\"])\n            )\n        )\n    train_corpus, dev_corpus = resolve_dot_names(config, dot_names)\n    optimizer = T[\"optimizer\"]\n    # Components that shouldn't be updated during training\n    frozen_components = T[\"frozen_components\"]\n    # Sourced components that require resume_training\n    resume_components = [p for p in sourced if p not in frozen_components]\n    logger.info(f\"Pipeline: {nlp.pipe_names}\")\n    if resume_components:\n        with nlp.select_pipes(enable=resume_components):\n            logger.info(f\"Resuming training for: {resume_components}\")\n            nlp.resume_training(sgd=optimizer)\n    # Make sure that listeners are defined before initializing further\n    nlp._link_components()\n    with nlp.select_pipes(disable=[*frozen_components, *resume_components]):\n        if T[\"max_epochs\"] == -1:\n            sample_size = 100\n            logger.debug(\n                f\"Due to streamed train corpus, using only first {sample_size} \"\n                f\"examples for initialization. If necessary, provide all labels \"\n                f\"in [initialize]. More info: https://spacy.io/api/cli#init_labels\"\n            )\n            nlp.initialize(\n                lambda: islice(train_corpus(nlp), sample_size), sgd=optimizer\n            )\n        else:\n            nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)\n        logger.info(f\"Initialized pipeline components: {nlp.pipe_names}\")\n    # Detect components with listeners that are not frozen consistently\n    for name, proc in nlp.pipeline:\n        for listener in getattr(\n            proc, \"listening_components\", []\n        ):  # e.g. tok2vec/transformer\n            # Don't warn about components not in the pipeline\n            if listener not in nlp.pipe_names:\n                continue\n            if listener in frozen_components and name not in frozen_components:\n                logger.warning(Warnings.W087.format(name=name, listener=listener))\n            # We always check this regardless, in case user freezes tok2vec\n            if listener not in frozen_components and name in frozen_components:\n                if name not in T[\"annotating_components\"]:\n                    logger.warning(Warnings.W086.format(name=name, listener=listener))\n\n    os.makedirs(output_path, exist_ok=True)\n    train_loop(nlp, output_path)\n</code></pre>"},{"location":"reference/viz/","title":"<code>edsnlp.viz</code>","text":""},{"location":"reference/viz/quick_examples/","title":"<code>edsnlp.viz.quick_examples</code>","text":""},{"location":"reference/viz/quick_examples/#edsnlp.viz.quick_examples.QuickExample","title":"<code>QuickExample</code>","text":"Source code in <code>edsnlp/viz/quick_examples.py</code> <pre><code>class QuickExample:\n    def __init__(self, nlp: Language, extensions: List[str] = []):\n        self.nlp = nlp\n        self.qualifiers = get_qualifier_extensions(nlp)\n        self.extensions = extensions\n\n    def __call__(\n        self, object: Union[str, Doc], as_dataframe: bool = False\n    ) -&gt; Optional[pd.DataFrame]:\n\"\"\"\n        Displays the text and a table of entities\n\n        Parameters\n        ----------\n        as_dataframe : bool, optional\n            If true, returns the table as a DataFrame instead of displaying it,\n            by default False\n\n        Returns\n        -------\n        Optional[pd.DataFrame]\n            The DataFrame describing the document\n        \"\"\"\n        if isinstance(object, str):\n            self.txt = object\n            self.doc = self.nlp(object)\n        elif isinstance(object, Doc):\n            self.txt = object.text\n            self.doc = object\n        self.get_ents()\n        self.get_ents_interval()\n        self.get_text()\n        return self.display(as_dataframe=as_dataframe)\n\n    def get_ents(self):\n\n        all_spans = {k: list(s) for k, s in self.doc.spans.items() if s}\n        all_spans[\"ents\"] = list(self.doc.ents).copy()\n\n        ents = []\n\n        for key, spans in all_spans.items():\n            for span in spans:\n                if span in all_spans[\"ents\"]:\n                    all_spans[\"ents\"].remove(span)\n                start, end = span.start, span.end\n                text = get_text(span, attr=\"TEXT\", ignore_excluded=False)\n                ent = dict(\n                    key=key,\n                    start=start,\n                    end=end,\n                    text=text,\n                )\n                for name, extension in self.qualifiers.items():\n                    ent[name] = rgetattr(span, extension)\n                for extension in self.extensions:\n                    ent[extension] = rgetattr(span, extension)\n                ents.append(ent)\n\n        self.ents = ents\n\n    def get_ents_interval(self):\n\"\"\"\n        From the list of all entities, removes overlapping spans\n        \"\"\"\n\n        intervals = []\n        for ent in self.ents:\n            interval = (ent[\"start\"], ent[\"end\"])\n            istart, iend = interval\n\n            i = bisect.bisect_right(intervals, (iend, len(self.doc) + 1))\n\n            for idx, (start, end) in enumerate(intervals[:i]):\n                if end &gt; istart:\n                    interval = (start, iend)\n                    del intervals[idx]\n                    break\n\n            bisect.insort(intervals, interval)\n\n        self.intervals = intervals\n\n    def is_ent(self, tok: Token) -&gt; bool:\n\"\"\"\n        Check if the provided Token is part of an entity\n\n        Parameters\n        ----------\n        tok : Token\n            A spaCy Token\n\n        Returns\n        -------\n        bool\n            True if `tok` is part of an entity\n        \"\"\"\n        for interval in self.intervals:\n            if (tok.i &gt;= interval[0]) and (tok.i &lt; interval[1]):\n                return True\n        return False\n\n    def get_text(self) -&gt; None:\n\"\"\"\n        Adds bold tags to `self.text`\n        \"\"\"\n        text = []\n        for tok in self.doc:\n            raw_tok_text = tok.text + tok.whitespace_\n            tok_text = (\n                f\"[bold]{raw_tok_text}[not bold]\" if self.is_ent(tok) else raw_tok_text\n            )\n            text.append(tok_text)\n        self.text = \"\".join(text)\n\n    def display(self, as_dataframe: bool = False) -&gt; Optional[pd.DataFrame]:\n\"\"\"\n        Displays the text and a table of entities\n\n        Parameters\n        ----------\n        as_dataframe : bool, optional\n            If true, returns the table as a DataFrame instead of displaying it,\n            by default False\n\n        Returns\n        -------\n        Optional[pd.DataFrame]\n            The DataFrame describing the document\n        \"\"\"\n\n        console = Console()\n\n        table = Table(title=self.text + \"\\n\")\n\n        headers = [\"Entity\", \"Source\"] + list(self.qualifiers.keys()) + self.extensions\n\n        if not as_dataframe:\n            [table.add_column(h) for h in headers]\n\n            for ent in self.ents:\n                table.add_row(\n                    ent[\"text\"],\n                    ent[\"key\"],\n                    *(\n                        \"[green]\" + str(ent[q]) if ent[q] else \"[red]\" + str(ent[q])\n                        for q in self.qualifiers\n                    ),\n                    *(str(ent[extension]) for extension in self.extensions),\n                )\n\n            console.print(table)\n\n        else:\n            df = pd.DataFrame(\n                [\n                    [\n                        ent[\"text\"],\n                        ent[\"key\"],\n                        *(ent[q] for q in list(self.qualifiers.keys())),\n                        *(ent[e] for e in self.extensions),\n                    ]\n                    for ent in self.ents\n                ],\n                columns=headers,\n            )\n\n            console.print(self.text)\n            return df\n</code></pre>"},{"location":"reference/viz/quick_examples/#edsnlp.viz.quick_examples.QuickExample.__call__","title":"<code>__call__(object, as_dataframe=False)</code>","text":"<p>Displays the text and a table of entities</p> PARAMETER DESCRIPTION <code>as_dataframe</code> <p>If true, returns the table as a DataFrame instead of displaying it, by default False</p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Optional[pd.DataFrame]</code> <p>The DataFrame describing the document</p> Source code in <code>edsnlp/viz/quick_examples.py</code> <pre><code>def __call__(\n    self, object: Union[str, Doc], as_dataframe: bool = False\n) -&gt; Optional[pd.DataFrame]:\n\"\"\"\n    Displays the text and a table of entities\n\n    Parameters\n    ----------\n    as_dataframe : bool, optional\n        If true, returns the table as a DataFrame instead of displaying it,\n        by default False\n\n    Returns\n    -------\n    Optional[pd.DataFrame]\n        The DataFrame describing the document\n    \"\"\"\n    if isinstance(object, str):\n        self.txt = object\n        self.doc = self.nlp(object)\n    elif isinstance(object, Doc):\n        self.txt = object.text\n        self.doc = object\n    self.get_ents()\n    self.get_ents_interval()\n    self.get_text()\n    return self.display(as_dataframe=as_dataframe)\n</code></pre>"},{"location":"reference/viz/quick_examples/#edsnlp.viz.quick_examples.QuickExample.get_ents_interval","title":"<code>get_ents_interval()</code>","text":"<p>From the list of all entities, removes overlapping spans</p> Source code in <code>edsnlp/viz/quick_examples.py</code> <pre><code>def get_ents_interval(self):\n\"\"\"\n    From the list of all entities, removes overlapping spans\n    \"\"\"\n\n    intervals = []\n    for ent in self.ents:\n        interval = (ent[\"start\"], ent[\"end\"])\n        istart, iend = interval\n\n        i = bisect.bisect_right(intervals, (iend, len(self.doc) + 1))\n\n        for idx, (start, end) in enumerate(intervals[:i]):\n            if end &gt; istart:\n                interval = (start, iend)\n                del intervals[idx]\n                break\n\n        bisect.insort(intervals, interval)\n\n    self.intervals = intervals\n</code></pre>"},{"location":"reference/viz/quick_examples/#edsnlp.viz.quick_examples.QuickExample.is_ent","title":"<code>is_ent(tok)</code>","text":"<p>Check if the provided Token is part of an entity</p> PARAMETER DESCRIPTION <code>tok</code> <p>A spaCy Token</p> <p> TYPE: <code>Token</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if <code>tok</code> is part of an entity</p> Source code in <code>edsnlp/viz/quick_examples.py</code> <pre><code>def is_ent(self, tok: Token) -&gt; bool:\n\"\"\"\n    Check if the provided Token is part of an entity\n\n    Parameters\n    ----------\n    tok : Token\n        A spaCy Token\n\n    Returns\n    -------\n    bool\n        True if `tok` is part of an entity\n    \"\"\"\n    for interval in self.intervals:\n        if (tok.i &gt;= interval[0]) and (tok.i &lt; interval[1]):\n            return True\n    return False\n</code></pre>"},{"location":"reference/viz/quick_examples/#edsnlp.viz.quick_examples.QuickExample.get_text","title":"<code>get_text()</code>","text":"<p>Adds bold tags to <code>self.text</code></p> Source code in <code>edsnlp/viz/quick_examples.py</code> <pre><code>def get_text(self) -&gt; None:\n\"\"\"\n    Adds bold tags to `self.text`\n    \"\"\"\n    text = []\n    for tok in self.doc:\n        raw_tok_text = tok.text + tok.whitespace_\n        tok_text = (\n            f\"[bold]{raw_tok_text}[not bold]\" if self.is_ent(tok) else raw_tok_text\n        )\n        text.append(tok_text)\n    self.text = \"\".join(text)\n</code></pre>"},{"location":"reference/viz/quick_examples/#edsnlp.viz.quick_examples.QuickExample.display","title":"<code>display(as_dataframe=False)</code>","text":"<p>Displays the text and a table of entities</p> PARAMETER DESCRIPTION <code>as_dataframe</code> <p>If true, returns the table as a DataFrame instead of displaying it, by default False</p> <p> TYPE: <code>bool, optional</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Optional[pd.DataFrame]</code> <p>The DataFrame describing the document</p> Source code in <code>edsnlp/viz/quick_examples.py</code> <pre><code>def display(self, as_dataframe: bool = False) -&gt; Optional[pd.DataFrame]:\n\"\"\"\n    Displays the text and a table of entities\n\n    Parameters\n    ----------\n    as_dataframe : bool, optional\n        If true, returns the table as a DataFrame instead of displaying it,\n        by default False\n\n    Returns\n    -------\n    Optional[pd.DataFrame]\n        The DataFrame describing the document\n    \"\"\"\n\n    console = Console()\n\n    table = Table(title=self.text + \"\\n\")\n\n    headers = [\"Entity\", \"Source\"] + list(self.qualifiers.keys()) + self.extensions\n\n    if not as_dataframe:\n        [table.add_column(h) for h in headers]\n\n        for ent in self.ents:\n            table.add_row(\n                ent[\"text\"],\n                ent[\"key\"],\n                *(\n                    \"[green]\" + str(ent[q]) if ent[q] else \"[red]\" + str(ent[q])\n                    for q in self.qualifiers\n                ),\n                *(str(ent[extension]) for extension in self.extensions),\n            )\n\n        console.print(table)\n\n    else:\n        df = pd.DataFrame(\n            [\n                [\n                    ent[\"text\"],\n                    ent[\"key\"],\n                    *(ent[q] for q in list(self.qualifiers.keys())),\n                    *(ent[e] for e in self.extensions),\n                ]\n                for ent in self.ents\n            ],\n            columns=headers,\n        )\n\n        console.print(self.text)\n        return df\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>We provide step-by-step guides to get you started. We cover the following use-cases:</p> <ul> <li>Matching a terminology: you're looking for a concept within a corpus of texts.</li> <li>Qualifying entities: you want to make sure that the concept you've extracted are not invalidated by linguistic modulation.</li> <li>Detecting dates, which could serve as the basis for an event ordering algorithm.</li> <li>Processing multiple texts: to improve the inference speed of your pipeline !</li> <li>Detecting Hospitalisation Reason: you want to look spans that mention the reason of hospitalisation or tag entities as the reason.</li> <li>Detecting false endlines: classify each endline and add the attribute <code>excluded</code> to the these tokens.</li> </ul>"},{"location":"tutorials/#rationale","title":"Rationale","text":"<p>In a typical medical NLP pipeline, a group of clinicians would define a list of synonyms for a given concept of interest (say, for example, diabetes), and look for that terminology in a corpus of documents.</p> <p>Now, consider the following example:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>There is an obvious problem: none of these examples should lead us to include this particular patient into the cohort.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p> <p>To curb this issue, EDS-NLP proposes rule-based pipelines that qualify entities to help the user make an informed decision about which patient should be included in a real-world data cohort.</p> <p>To sum up, a typical medical NLP project consists in:</p> <ol> <li>Editing a terminology</li> <li>\"Matching\" this terminology on a corpus, ie extract phrases that belong to that terminology</li> <li>\"Qualifying\" entities to avoid false positives</li> </ol> <p>Once the pipeline is ready, we need to deploy it efficiently.</p>"},{"location":"tutorials/detecting-dates/","title":"Detecting dates","text":"<p>We now know how to match a terminology and qualify detected entities, which covers most use cases for a typical medical NLP project. In this tutorial, we'll see how to use EDS-NLP to detect and normalise date mentions using <code>eds.dates</code>.</p> <p>This can have many applications, for dating medical events in particular. The <code>eds.consultation_dates</code> component, for instance, combines the date detection capabilities with a few simple patterns to detect the date of the consultation, when mentioned in clinical reports.</p>"},{"location":"tutorials/detecting-dates/#dates-in-clinical-notes","title":"Dates in clinical notes","text":"<p>Consider the following example:</p> FrenchEnglish <pre><code>Le patient est admis le 21 janvier pour une douleur dans le cou.\nIl se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\n</code></pre> <pre><code>The patient is admitted on January 21st for a neck pain.\nHe complains about chronique pain that started three years ago.\n</code></pre> <p>Clinical notes contain many different types of dates. To name a few examples:</p> Type Description Examples Absolute Explicit date <code>2022-03-03</code> Partial Date missing the day, month or year <code>le 3 janvier/on January 3rd</code>, <code>en 2021/in 2021</code> Relative Relative dates <code>hier/yesterday</code>, <code>le mois dernier/last month</code> Duration Durations <code>pendant trois mois/for three months</code> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p>"},{"location":"tutorials/detecting-dates/#extracting-dates","title":"Extracting dates","text":"<p>The followings snippet adds the <code>eds.date</code> component to the pipeline:</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.dates\")  # (1)\n\ntext = (\n    \"Le patient est admis le 21 janvier pour une douleur dans le cou.\\n\"\n    \"Il se plaint d'une douleur chronique qui a d\u00e9but\u00e9 il y a trois ans.\"\n)\n\n# Detecting dates becomes trivial\ndoc = nlp(text)\n\n# Likewise, accessing detected dates is hassle-free\ndates = doc.spans[\"dates\"]  # (2)\n</code></pre> <ol> <li>The date detection component is declared with <code>eds.dates</code></li> <li>Dates are saved in the <code>doc.spans[\"dates\"]</code> key</li> </ol> <p>After this, accessing dates and there normalisation becomes trivial:</p> <pre><code># \u2191 Omitted code above \u2191\n\ndates  # (1)\n# Out: [21 janvier, il y a trois ans]\n</code></pre> <ol> <li><code>dates</code> is a list of spaCy <code>Span</code> objects.</li> </ol>"},{"location":"tutorials/detecting-dates/#normalisation","title":"Normalisation","text":"<p>We can review each date and get its normalisation:</p> <code>date.text</code> <code>date._.date</code> <code>21 janvier</code> <code>{\"day\": 21, \"month\": 1}</code> <code>il y a trois ans</code> <code>{\"direction\": \"past\", \"year\": 3}</code> <p>Dates detected by the pipeline component are parsed into a dictionary-like object. It includes every information that is actually contained in the text.</p> <p>To get a more usable representation, you may call the <code>to_datetime()</code> method. If there's enough information, the date will be represented in a <code>datetime.datetime</code> or <code>datetime.timedelta</code> object. If some information is missing, It will return <code>None</code>. Alternatively for this case, you can optionally set to <code>True</code> the parameter <code>infer_from_context</code> and you may also give a value for <code>note_datetime</code>.</p> <p>Date normalisation</p> <p>Since dates can be missing some information (eg <code>en ao\u00fbt</code>), we refrain from outputting a <code>datetime</code> object in that case. Doing so would amount to guessing, and we made the choice of letting you decide how you want to handle missing dates.</p>"},{"location":"tutorials/detecting-dates/#what-next","title":"What next?","text":"<p>The <code>eds.dates</code> pipeline component's role is merely to detect and normalise dates. It is the user's responsibility to use this information in a downstream application.</p> <p>For instance, you could use this pipeline to date medical entities. Let's do that.</p>"},{"location":"tutorials/detecting-dates/#a-medical-event-tagger","title":"A medical event tagger","text":"<p>Our pipeline will detect entities and events separately, and we will post-process the output <code>Doc</code> object to determine whether a given entity can be linked to a date.</p> <pre><code>import spacy\nfrom datetime import datetime\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.dates\")\n\nconfig = dict(\n    regex=dict(admission=[\"admissions?\", \"admise?\", \"prise? en charge\"]),\n    attr=\"LOWER\",\n)\nnlp.add_pipe(\"eds.matcher\", config=config)\n\ntext = (\n    \"Le patient est admis le 12 avril pour une douleur \"\n    \"survenue il y a trois jours. \"\n    \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re. \"\n    \"Il a \u00e9t\u00e9 diagnostiqu\u00e9 en mai 1995.\"\n)\n\ndoc = nlp(text)\n</code></pre> <p>At this point, the document is ready to be post-processed: its <code>ents</code> and <code>spans[\"dates\"]</code> are populated:</p> <pre><code># \u2191 Omitted code above \u2191\n\ndoc.ents\n# Out: (admis, pris en charge)\n\ndoc.spans[\"dates\"]\n# Out: [12 avril, il y a trois jours, l'ann\u00e9e derni\u00e8re, mai 1995]\n\nnote_datetime = datetime(year=1999, month=8, day=27)\n\nfor i, date in enumerate(doc.spans[\"dates\"]):\n    print(\n        i,\n        \" - \",\n        date,\n        \" - \",\n        date._.date.to_datetime(\n            note_datetime=note_datetime, infer_from_context=False, tz=None\n        ),\n    )\n    # Out: 0  -  12 avril  -  None\n    # Out: 1  -  il y a trois jours  -  1999-08-24 00:00:00\n    # Out: 2  -  l'ann\u00e9e derni\u00e8re  -  1998-08-27 00:00:00\n    # Out: 3  -  mai 1995  -  None\n\n\nfor i, date in enumerate(doc.spans[\"dates\"]):\n    print(\n        i,\n        \" - \",\n        date,\n        \" - \",\n        date._.date.to_datetime(\n            note_datetime=note_datetime,\n            infer_from_context=True,\n            tz=None,\n            default_day=15,\n        ),\n    )\n    # Out: 0  -  12 avril  -  1999-04-12T00:00:00\n    # Out: 1  -  il y a trois jours  -  1999-08-24 00:00:00\n    # Out: 2  -  l'ann\u00e9e derni\u00e8re  -  1998-08-27 00:00:00\n    # Out: 3  -  mai 1995  -  1995-05-15T00:00:00\n</code></pre> <p>As a first heuristic, let's consider that an entity can be linked to a date if the two are in the same sentence. In the case where multiple dates are present, we'll select the closest one.</p> utils.py<pre><code>from spacy.tokens import Span\nfrom typing import List, Optional\n\n\ndef candidate_dates(ent: Span) -&gt; List[Span]:\n\"\"\"Return every dates in the same sentence as the entity\"\"\"\n    return [date for date in ent.doc.spans[\"dates\"] if date.sent == ent.sent]\n\n\ndef get_event_date(ent: Span) -&gt; Optional[Span]:\n\"\"\"Link an entity to the closest date in the sentence, if any\"\"\"\n\n    dates = candidate_dates(ent)  # (1)\n\n    if not dates:\n        return\n\n    dates = sorted(\n        dates,\n        key=lambda d: min(abs(d.start - ent.end), abs(ent.start - d.end)),\n    )\n\n    return dates[0]  # (2)\n</code></pre> <ol> <li>Get all dates present in the same sentence.</li> <li>Sort the dates, and keep the first item.</li> </ol> <p>We can apply this simple function:</p> <pre><code>import spacy\nfrom utils import get_event_date\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.dates\")\n\nconfig = dict(\n    regex=dict(admission=[\"admissions?\", \"admise?\", \"prise? en charge\"]),\n    attr=\"LOWER\",\n)\nnlp.add_pipe(\"eds.matcher\", config=config)\n\ntext = (\n    \"Le patient est admis le 12 avril pour une douleur \"\n    \"survenue il y a trois jours. \"\n    \"Il avait \u00e9t\u00e9 pris en charge l'ann\u00e9e derni\u00e8re.\"\n)\n\ndoc = nlp(text)\n\nfor ent in doc.ents:\n    date = get_event_date(ent)\n    print(f\"{ent.text:&lt;20}{date.text:&lt;20}{date._.date.to_datetime()}\")\n# Out: admis               12 avril 2020       2020-04-12T00:00:00+02:00\n# Out: pris en charge      l'ann\u00e9e derni\u00e8re    -1 year\n</code></pre> <p>Which will output:</p> <code>ent</code> <code>get_event_date(ent)</code> <code>get_event_date(ent)._.date.to_datetime()</code> admis 12 avril <code>2020-04-12T00:00:00+02:00</code> pris en charge l'ann\u00e9e derni\u00e8re <code>-1 year</code>"},{"location":"tutorials/endlines/","title":"Detecting end-of-lines","text":"<p>A common problem in medical corpus is that the character <code>\\n</code> does not necessarily correspond to a real new line as in other domains.</p> <p>For example, it is common to find texts like:</p> <pre><code>Il doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\n</code></pre> <p>Inserted new line characters</p> <p>This issue is especially impactful for clinical notes that have been extracted from PDF documents. In that case, the new line character could be deliberately inserted by the doctor, or more likely added to respect the layout during the edition of the PDF.</p> <p>The aim of this tutorial is to train a unsupervised model to detect this false endlines and to use it for inference. The implemented model is based on the work of Zweigenbaum et al1.</p>"},{"location":"tutorials/endlines/#training-the-model","title":"Training the model","text":"<p>Let's train the model using an example corpus of three documents:</p> <pre><code>import spacy\nfrom edsnlp.pipelines.core.endlines import EndLinesModel\n\nnlp = spacy.blank(\"fr\")\n\ntext1 = \"\"\"Le patient est arriv\u00e9 hier soir.\nIl est accompagn\u00e9 par son fils\n\nANTECEDENTS\nIl a fait une TS en 2010;\nFumeur, il est arr\u00eat\u00e9 il a 5 mois\nChirurgie de coeur en 2011\nCONCLUSION\nIl doit prendre\nle medicament indiqu\u00e9 3 fois par jour. Revoir m\u00e9decin\ndans 1 mois.\nDIAGNOSTIC :\n\nAntecedents Familiaux:\n- 1. P\u00e8re avec diab\u00e8te\n\"\"\"\n\ntext2 = \"\"\"J'aime le \\nfromage...\\n\"\"\"\ntext3 = (\n    \"/n\"\n    \"Intervention(s) - acte(s) r\u00e9alis\u00e9(s) :/n\"\n    \"Parathyro\u00efdectomie \u00e9lective le [DATE]\"\n)\n\ntexts = [\n    text1,\n    text2,\n    text3,\n]\n\ncorpus = nlp.pipe(texts)\n\n# Fit the model\nendlines = EndLinesModel(nlp=nlp)  # (1)\ndf = endlines.fit_and_predict(corpus)  # (2)\n\n# Save model\nPATH = \"/tmp/path_to_model\"\nendlines.save(PATH)\n</code></pre> <ol> <li>Initialize the <code>EndLinesModel</code>    object and then fit (and predict) in the training corpus.</li> <li>The corpus should be an iterable of spacy documents.</li> </ol>"},{"location":"tutorials/endlines/#use-a-trained-model-for-inference","title":"Use a trained model for inference","text":"<pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n\nPATH = \"/path_to_model\"\nnlp.add_pipe(\"eds.endlines\", config=dict(model_path=PATH))  # (1)\nnlp.add_pipe(\"eds.sentences\")  # (1)\n\ndocs = list(nlp.pipe([text1, text2, text3]))\n\ndoc = docs[1]\ndoc\n# Out: J'aime le\n# Out: fromage...\n\nlist(doc.sents)[0]\n# Out: J'aime le\n# Out: fromage...\n</code></pre> <ol> <li>You should specify the path to the trained model here.</li> <li>All fake new line are excluded by setting their <code>tag</code> to 'EXCLUDED' and all true new lines' <code>tag</code> are set to 'ENDLINE'.</li> </ol>"},{"location":"tutorials/endlines/#declared-extensions","title":"Declared extensions","text":"<p>It lets downstream matchers skip excluded tokens (see normalisation) for more detail.</p> <ol> <li> <p>Pierre Zweigenbaum, Cyril Grouin, and Thomas Lavergne. Une cat\u00e9gorisation de fins de lignes non-supervis\u00e9e (end-of-line classification with no supervision). In Actes de la conf\u00e9rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters), 364\u2013371. Paris, France, 7 2016. AFCP - ATALA. URL: https://aclanthology.org/2016.jeptalnrecital-poster.7.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/matching-a-terminology/","title":"Matching a terminology","text":"<p>Matching a terminology is perhaps the most basic application of a medical NLP pipeline.</p> <p>In this tutorial, we will cover :</p> <ul> <li>Matching a terminology using spaCy's matchers, as well as RegExps</li> <li>Matching on a specific attribute</li> </ul> <p>You should consider reading the matcher's specific documentation for a description.</p> <p>Comparison to spaCy's matcher</p> <p>spaCy's <code>Matcher</code> and <code>PhraseMatcher</code> use a very efficient algorithm that compare a hashed representation token by token. They are not components by themselves, but can underpin rule-based pipelines.</p> <p>EDS-NLP's <code>RegexMatcher</code> lets the user match entire expressions using regular expressions. To achieve this, the matcher has to get to the text representation, match on it, and get back to spaCy's abstraction.</p> <p>The <code>EDSPhraseMatcher</code> lets EDS-NLP reuse spaCy's efficient algorithm, while adding the ability to skip pollution tokens (see the normalisation documentation for detail)</p>"},{"location":"tutorials/matching-a-terminology/#a-simple-use-case-finding-covid19","title":"A simple use case : finding COVID19","text":"<p>Let's try to find mentions of COVID19 and references to patients within a clinical note.</p> <pre><code>import spacy\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],\n    respiratoire=[\"asthmatique\", \"respiratoire\"],\n)\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.matcher\", config=dict(terms=terms))\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (asthmatique,)\n</code></pre> <p>Let's unpack what happened:</p> <ol> <li>We defined a dictionary of terms to look for, in the form <code>{'label': list of terms}</code>.</li> <li>We declared a spaCy pipeline, and add the <code>eds.matcher</code> component.</li> <li>We applied the pipeline to the texts...</li> <li>... and explored the extracted entities.</li> </ol> <p>This example showcases a limitation of our term dictionary : the phrases <code>COVID19</code> and <code>difficult\u00e9s respiratoires</code> were not detected by the pipeline.</p> <p>To increase recall, we could just add every possible variation :</p> <pre><code>terms = dict(\n-    covid=[\"coronavirus\", \"covid19\"],\n+    covid=[\"coronavirus\", \"covid19\", \"COVID19\"],\n-    respiratoire=[\"asthmatique\", \"respiratoire\"],\n+    respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n</code></pre> <p>But what if we come across <code>Coronavirus</code>? Surely we can do better!</p>"},{"location":"tutorials/matching-a-terminology/#matching-on-normalised-text","title":"Matching on normalised text","text":"<p>We can modify the matcher's configuration to match on other attributes instead of the verbatim input. You can refer to spaCy's list of available token attributes.</p> <p>Let's focus on two:</p> <ol> <li>The <code>LOWER</code> attribute, which lets you match on a lowercased version of the text.</li> <li>The <code>NORM</code> attribute, which adds some basic normalisation (eg <code>\u0153</code> to <code>oe</code>). EDS-NLP provides a <code>eds.normalizer</code> component that extends the level of cleaning on the <code>NORM</code> attribute.</li> </ol>"},{"location":"tutorials/matching-a-terminology/#the-lower-attribute","title":"The <code>LOWER</code> attribute","text":"<p>Matching on the lowercased version is extremely easy:</p> <pre><code>import spacy\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\n    covid=[\"coronavirus\", \"covid19\"],\n    respiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        terms=terms,\n        attr=\"LOWER\",  # (1)\n    ),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>The matcher's <code>attr</code> parameter defines the attribute that the matcher will use. It is set to <code>\"TEXT\"</code> by default (ie verbatim text).</li> </ol> <p>This code is complete, and should run as is.</p>"},{"location":"tutorials/matching-a-terminology/#using-the-normalisation-component","title":"Using the normalisation component","text":"<p>EDS-NLP provides its own normalisation component, which modifies the <code>NORM</code> attribute in place. It handles:</p> <ul> <li>removal of accentuated characters;</li> <li>normalisation of quotes and apostrophes;</li> <li>lowercasing, which enabled by default in spaCy \u2013 EDS-NLP lets you disable it;</li> <li>removal of pollution.</li> </ul> <p>Pollution in clinical texts</p> <p>EDS-NLP is meant to be deployed on clinical reports extracted from hospitals information systems. As such, it is often riddled with extraction issues or administrative artifacts that \"pollute\" the report.</p> <p>As a core principle, EDS-NLP never modifies the input text, and <code>nlp(text).text == text</code> is always true. However, we can tag some tokens as pollution elements, and avoid using them for matching the terminology.</p> <p>You can activate it like any other component.</p> <pre><code>import spacy\n\ntext = (\n\"Motif de prise en charge : probable pneumopathie a ===== COVID19, \"  # (1)\n\"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nterms = dict(\ncovid=[\"coronavirus\", \"covid19\", \"pneumopathie \u00e0 covid19\"],  # (2)\nrespiratoire=[\"asthmatique\", \"respiratoire\", \"respiratoires\"],\n)\n\nnlp = spacy.blank(\"fr\")\n\n# Add the normalisation component\nnlp.add_pipe(\"eds.normalizer\")  # (3)\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        terms=terms,\nattr=\"NORM\",  # (4)\nignore_excluded=True,  # (5)\n),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (pneumopathie a ===== COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>We've modified the example to include a simple pollution.</li> <li>We've added <code>pneumopathie \u00e0 covid19</code> to the list of synonyms detected by the pipeline.    Note that in the synonym we provide, we kept the accentuated <code>\u00e0</code>, whereas the example    displays an unaccentuated <code>a</code>.</li> <li>The component can be configured. See the specific documentation for detail.</li> <li>The normalisation lives in the <code>NORM</code> attribute</li> <li>We can tell the matcher to ignore excluded tokens (tokens tagged as pollution by the normalisation component).    This is not an obligation.</li> </ol> <p>Using the normalisation component, you can match on a normalised version of the text, as well as skip pollution tokens during the matching process.</p> <p>Using term matching with the normalisation</p> <p>If you use the term matcher with the normalisation, bear in mind that the examples go through the pipeline. That's how the matcher was able to recover <code>pneumopathie a ===== COVID19</code> despite the fact that we used an accentuated <code>\u00e0</code> in the terminology.</p> <p>The term matcher matches the input text to the provided terminology, using the selected attribute in both cases. The <code>NORM</code> attribute that corresponds to <code>\u00e0</code> and <code>a</code> is the same: <code>a</code>.</p>"},{"location":"tutorials/matching-a-terminology/#preliminary-conclusion","title":"Preliminary conclusion","text":"<p>We have matched all mentions! However, we had to spell out the singular and plural form of <code>respiratoire</code>... And what if we wanted to detect <code>covid 19</code>, or <code>covid-19</code> ? Of course, we could write out every imaginable possibility, but this will quickly become tedious.</p>"},{"location":"tutorials/matching-a-terminology/#using-regular-expressions","title":"Using regular expressions","text":"<p>Let us redefine the pipeline once again, this time using regular expressions:</p> <pre><code>import spacy\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie a COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        regex=regex,  # (1)\n        terms=terms,  # (2)\n        attr=\"LOWER\",  # (3)\n    ),\n)\n\ndoc = nlp(text)\n\ndoc.ents\n# Out: (COVID19, respiratoires, asthmatique)\n</code></pre> <ol> <li>We can now match using regular expressions.</li> <li>We can mix and match patterns! Here we keep looking for patients using spaCy's term matching.</li> <li>RegExp matching is not limited to the verbatim text! You can choose to use one of spaCy's native attribute, ignore excluded tokens, etc.</li> </ol> <p>This code is complete, and should run as is.</p> <p>Using regular expressions can help define richer patterns using more compact queries.</p>"},{"location":"tutorials/matching-a-terminology/#visualising-matched-entities","title":"Visualising matched entities","text":"<p>EDS-NLP is part of the spaCy ecosystem, which means we can benefit from spaCy helper functions. For instance, spaCy's visualiser displacy can let us visualise the matched entities:</p> <pre><code># \u2191 Omitted code above \u2191\n\nfrom spacy import displacy\n\ncolors = {\n    \"covid\": \"orange\",\n    \"respiratoire\": \"steelblue\",\n}\noptions = {\n    \"colors\": colors,\n}\n\ndisplacy.render(doc, style=\"ent\", options=options)\n</code></pre> <p>If you run this within a notebook, you should get:</p> Motif de prise en charge : probable pneumopathie a              COVID19         covid      , sans difficult\u00e9s              respiratoires         respiratoire Le p\u00e8re du patient est              asthmatique         respiratoire      ."},{"location":"tutorials/multiple-texts/","title":"Processing multiple texts","text":"<p>In the previous tutorials, we've seen how to apply a spaCy NLP pipeline to a single text. Once the pipeline is tested and ready to be applied on an entire corpus, we'll want to deploy it efficiently.</p> <p>In this tutorial, we'll cover a few best practices and some caveats to avoid. Then, we'll explore methods that EDS-NLP provides to use a spaCy pipeline directly on a pandas or Spark DataFrame. These can drastically increase throughput.</p> <p>Consider this simple pipeline:</p> Pipeline definition: pipeline.py<pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.normalizer\")\n\nconfig = dict(\n    terms=dict(patient=[\"patient\", \"malade\"]),\n    attr=\"NORM\",\n)\nnlp.add_pipe(\"eds.matcher\", config=config)\n\n# Add qualifiers\nnlp.add_pipe(\"eds.negation\")\nnlp.add_pipe(\"eds.hypothesis\")\nnlp.add_pipe(\"eds.family\")\n\n# Add date detection\nnlp.add_pipe(\"eds.dates\")\n</code></pre> <p>Let's deploy it on a large number of documents.</p>"},{"location":"tutorials/multiple-texts/#what-about-a-for-loop","title":"What about a <code>for</code> loop?","text":"<p>Suppose we have a corpus of text:</p> <pre><code>text = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ncorpus = [text] * 10000  # (1)\n</code></pre> <ol> <li>This is admittedly ugly. But you get the idea, we have a corpus of 10 000 documents we want to process...</li> </ol> <p>You could just apply the pipeline document by document.</p> A naive approach<pre><code># \u2191 Omitted code above \u2191\n\ndocs = [nlp(text) for text in corpus]\n</code></pre> <p>It turns out spaCy has a powerful parallelisation engine for an efficient processing of multiple texts. So the first step for writing more efficient spaCy code is to use <code>nlp.pipe</code> when processing multiple texts:</p> <pre><code>- docs = [nlp(text) for text in corpus]\n+ docs = list(nlp.pipe(corpus))\n</code></pre> <p>The <code>nlp.pipe</code> method takes an iterable as input, and outputs a generator of <code>Doc</code> object. Under the hood, texts are processed in batches, which is often much more efficient.</p> <p>Batch processing and EDS-NLP</p> <p>For now, EDS-NLP does not natively parallelise its components, so the gain from using <code>nlp.pipe</code> will not be that significant.</p> <p>Nevertheless, it's good practice to avoid using <code>for</code> loops when possible. Moreover, you will benefit from the batched tokenisation step.</p> <p>The way EDS-NLP is used may depend on how many documents you are working with. Once working with tens of thousands of them, parallelising the processing can be really efficient (up to 20x faster), but will require a (tiny) bit more work. Here are shown 4 ways to analyse texts depending on your needs</p> <p>A wrapper is available to simply switch between those use cases.</p>"},{"location":"tutorials/multiple-texts/#processing-a-pandas-dataframe","title":"Processing a pandas DataFrame","text":"<p>Processing text within a pandas DataFrame is a very common use case. In many applications, you'll select a corpus of documents over a distributed cluster, load it in memory and process all texts.</p> <p>The OMOP CDM</p> <p>In every tutorial that mentions distributing EDS-NLP over a corpus of documents, we will expect the data to be organised using a flavour of the OMOP Common Data Model.</p> <p>The OMOP CDM defines two tables of interest to us:</p> <ul> <li>the <code>note</code> table contains the clinical notes</li> <li>the <code>note_nlp</code> table holds the results of   a NLP pipeline applied to the <code>note</code> table.</li> </ul> <p>To make sure we can follow along, we propose three recipes for getting the DataFrame: using a dummy dataset like before, loading a CSV or by loading a Spark DataFrame into memory.</p> Dummy exampleLoading data from a CSVLoading data from a Spark DataFrame <pre><code>import pandas as pd\n\ntext = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ncorpus = [text] * 1000\n\ndata = pd.DataFrame(dict(note_text=corpus))\ndata[\"note_id\"] = range(len(data))\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"note.csv\")\n</code></pre> <pre><code>from pyspark.sql.session import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"SELECT * FROM note\")\ndf = df.select(\"note_id\", \"note_text\")\n\ndata = df.limit(1000).toPandas()  # (1)\n</code></pre> <ol> <li>We limit the size of the DataFrame to make sure we do not overwhelm our machine.</li> </ol> <p>We'll see in what follows how we can efficiently deploy our pipeline on the <code>data</code> object.</p>"},{"location":"tutorials/multiple-texts/#by-hand","title":"\"By hand\"","text":"<p>We can deploy the pipeline using <code>nlp.pipe</code> directly, but we'll need some work to format the results in a usable way. Let's see how this might go, before using EDS-NLP's helper function to avoid the boilerplate code.</p> processing.py<pre><code>from spacy.tokens import Doc\nfrom typing import Any, Dict, List\n\n\ndef get_entities(doc: Doc) -&gt; List[Dict[str, Any]]:\n\"\"\"Return a list of dict representation for the entities\"\"\"\n\n    entities = []\n\n    for ent in doc.ents:\n        d = dict(\n            start=ent.start_char,\n            end=ent.end_char,\n            label=ent.label_,\n            lexical_variant=ent.text,\n            negation=ent._.negation,\n            hypothesis=ent._.hypothesis,\n            family=ent._.family,\n            key=\"ents\",\n        )\n        entities.append(d)\n\n    for date in doc.spans.get(\"dates\", []):\n        d = dict(\n            start=date.start_char,\n            end=date.end_char,\n            label=date._.date,\n            lexical_variant=date.text,\n            key=\"dates\",\n        )\n        entities.append(d)\n\n    return entities\n</code></pre> <pre><code># \u2191 Omitted code above \u2191\nfrom processing import get_entities\nimport pandas as pd\n\ndata[\"doc\"] = list(nlp.pipe(data.note_text))  # (1)\ndata[\"entities\"] = data.doc.apply(get_entities)  # (2)\n\n# \"Explode\" the dataframe\ndata = data[[\"note_id\", \"entities\"]].explode(\"entities\")\ndata = data.dropna()\n\ndata = data.reset_index(drop=True)\n\ndata = data[[\"note_id\"]].join(pd.json_normalize(data.entities))\n</code></pre> <ol> <li>We use spaCy's efficient <code>nlp.pipe</code> method</li> <li>This part is far from optimal, since it uses apply... But the computationally heavy part is in the previous line,    since <code>get_entities</code> merely reads pre-computed values from the document.</li> </ol> <p>The result on the first note:</p> note_id start end label lexical_variant negation hypothesis family key 0 0 7 patient Patient 0 0 0 ents 0 114 121 patient patient 0 0 1 ents 0 17 34 2021-09-25 25 septembre 2021 nan nan nan dates"},{"location":"tutorials/multiple-texts/#using-eds-nlps-helper-functions","title":"Using EDS-NLP's helper functions","text":"<p>Let's see how we can efficiently deploy our pipeline using EDS-NLP's utility methods.</p> <p>They share the same arguments:</p> Argument Description Default <code>note</code> A DataFrame, with two required columns, <code>note_id</code> and <code>note_text</code> Required <code>nlp</code> The pipeline object Required <code>context</code> A list of column names to add context to the generate <code>Doc</code> <code>[]</code> <code>additional_spans</code> Keys in <code>doc.spans</code> to include besides <code>doc.ents</code> <code>[]</code> <code>extensions</code> Custom extensions to use <code>[]</code> <code>results_extractor</code> An arbitrary callback function that turns a <code>Doc</code> into a list of dictionaries <code>None</code> (use extensions) <p>Adding context</p> <p>You might want to store some context information contained in the <code>note</code> DataFrame as an extension in the generated <code>Doc</code> object.</p> <p>For instance, you may use the <code>eds.dates</code> pipeline in coordination with the <code>note_datetime</code> field to normalise a relative date (eg <code>Le patient est venu il y a trois jours/The patient came three days ago</code>).</p> <p>In this case, you can use the <code>context</code> parameter and provide a list of column names you want to add:</p> <pre><code>note_nlp = single_pipe(\n    data,\n    nlp,\n    context=[\"note_datetime\"],\n    additional_spans=[\"dates\"],\n    extensions=[\"date.day\", \"date.month\", \"date.year\"],\n)\n</code></pre> <p>In this example, the <code>note_datetime</code> field becomes available as <code>doc._.note_datetime</code>.</p> <p>Depending on your pipeline, you may want to extract other extensions. To do so, simply provide those extension names (without the leading underscore) to the <code>extensions</code> argument. This should cover most use-cases.</p> <p>In case you need more fine-grained control over how you want to process the results of your pipeline, you can provide an arbitrary <code>results_extractor</code> function. Said function is expected to take a spaCy <code>Doc</code> object as input, and return a list of dictionaries that will be used to construct the <code>note_nlp</code> table. For instance, the <code>get_entities</code> function defined earlier could be distributed directly:</p> <pre><code># \u2191 Omitted code above \u2191\nfrom edsnlp.processing.simple import pipe as single_pipe\nfrom processing import get_entities\n\nnote_nlp = single_pipe(\n    data,\n    nlp,\n    results_extractor=get_entities,\n)\n</code></pre> <p>A few caveats on using an arbitrary function</p> <p>Should you use multiprocessing, your arbitrary function needs to be serialisable as a pickle object in order to be distributed. That implies a few limitations on the way your function can be defined.</p> <p>Namely, your function needs to be discoverable (see the pickle documentation on the subject). When deploying it should be defined such a way that can be accessed by the worker processes.</p> <p>For that reason, arbitrary functions can only be distributed via Spark/Koalas if their source code is advertised to the Spark workers. To that end, you should define your custom function in a pip-installed Python package.</p>"},{"location":"tutorials/multiple-texts/#single-process","title":"Single process","text":"<p>EDS-NLP provides a <code>single_pipe</code> helper function that avoids the hassle we just went through in the previous section. Using it is trivial:</p> <pre><code># \u2191 Omitted code above \u2191\nfrom edsnlp.processing.simple import pipe as single_pipe\n\nnote_nlp = single_pipe(\n    data,\n    nlp,\n    additional_spans=[\"dates\"],\n    extensions=[\"date.day\", \"date.month\", \"date.year\"],\n)\n</code></pre> <p>In just two Python statements, we get the exact same result as before!</p>"},{"location":"tutorials/multiple-texts/#multiple-processes","title":"Multiple processes","text":"<p>Depending on the size of your corpus, and if you have CPU cores to spare, you may want to distribute the computation. Again, EDS-NLP makes it extremely easy for you, through the <code>parallel_pipe</code> helper:</p> <pre><code># \u2191 Omitted code above \u2191\nfrom edsnlp.processing.parallel import pipe as parallel_pipe\n\nnote_nlp = parallel_pipe(\n    data,\n    nlp,\n    additional_spans=[\"dates\"],\n    extensions=[\"date.day\", \"date.month\", \"date.year\"],\n    n_jobs=-2,  # (1)\n)\n</code></pre> <ol> <li>The <code>n_jobs</code> parameter controls the number of workers that you deploy in parallel. Negative inputs means \"all cores minus <code>abs(n_jobs + 1)</code>\"</li> </ol> <p>Using a large number of workers and memory use</p> <p>In spaCy, even a rule-based pipeline is a memory intensive object. Be wary of using too many workers, lest you get a memory error.</p> <p>Depending on your machine, you should get a significant speed boost (we got 20x acceleration on a shared cluster using 62 cores).</p>"},{"location":"tutorials/multiple-texts/#deploying-eds-nlp-on-sparkkoalas","title":"Deploying EDS-NLP on Spark/Koalas","text":"<p>Should you need to deploy spaCy on a distributed DataFrame such as a Spark or a Koalas DataFrame, EDS-NLP has you covered. The procedure for those two types of DataFrame is virtually the same. Under the hood, EDS-NLP automatically deals with the necessary conversions.</p> <p>Suppose you have a Spark DataFrame:</p> Using a dummy exampleLoading a pre-existing tableUsing a Koalas DataFrame <pre><code>from pyspark.sql.session import SparkSession\nfrom pyspark.sql import types as T\n\nspark = SparkSession.builder.getOrCreate()\n\nschema = T.StructType(\n    [\n        T.StructField(\"note_id\", T.IntegerType()),\n        T.StructField(\"note_text\", T.StringType()),\n    ]\n)\n\ntext = (\n    \"Patient admis le 25 septembre 2021 pour suspicion de Covid.\\n\"\n    \"Pas de cas de coronavirus dans ce service.\\n\"\n    \"Le p\u00e8re du patient est atteint du covid.\"\n)\n\ndata = [(i, text) for i in range(1000)]\n\ndf = spark.createDataFrame(data=data, schema=schema)\n</code></pre> <pre><code>from pyspark.sql.session import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"SELECT * FROM note\")\ndf = df.select(\"note_id\", \"note_text\")\n</code></pre> <pre><code>from pyspark.sql.session import SparkSession\nimport databricks.koalas\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"SELECT note_id, note_text FROM note\").to_koalas()\n</code></pre>"},{"location":"tutorials/multiple-texts/#declaring-types","title":"Declaring types","text":"<p>There is a minor twist, though: Spark (or Koalas) needs to know in advance the type of each extension you want to save. Thus, if you need additional extensions to be saved, you'll have to provide a dictionary to the <code>extensions</code> argument instead of a list of strings. This dictionary will have the name of the extension as keys and its PySpark type as value.</p> <p>Accepted types are the ones present in <code>pyspark.sql.types</code>.</p> <p>EDS-NLP provides a helper function, <code>pyspark_type_finder</code>, is available to get the correct type for most Python objects. You just need to provide an example of the type you wish to collect:</p> <pre><code>int_type = pyspark_type_finder(1)\n\n# Out: IntegerType()\n</code></pre> <p>Be careful when providing the example</p> <p>Do not blindly provide the first entity matched by your pipeline: it might be ill-suited. For instance, the <code>Span._.date</code> makes sense for a date span, but will be <code>None</code> if you use an entity...</p>"},{"location":"tutorials/multiple-texts/#deploying-the-pipeline","title":"Deploying the pipeline","text":"<p>Once again, using the helper is trivial:</p> SparkKoalas <pre><code># \u2191 Omitted code above \u2191\nfrom edsnlp.processing.distributed import pipe as distributed_pipe\n\nnote_nlp = distributed_pipe(\n    df,\n    nlp,\n    additional_spans=[\"dates\"],\n    extensions={\"date.year\": int_type, \"date.month\": int_type, \"date.day\": int_type},\n)\n\n# Check that the pipeline was correctly distributed:\nnote_nlp.show(5)\n</code></pre> <pre><code># \u2191 Omitted code above \u2191\nfrom edsnlp.processing.distributed import pipe as distributed_pipe\n\nnote_nlp = distributed_pipe(\n    df,\n    nlp,\n    additional_spans=[\"dates\"],\n    extensions={\"date.year\": int_type, \"date.month\": int_type, \"date.day\": int_type},\n)\n\n# Check that the pipeline was correctly distributed:\nnote_nlp.head()\n</code></pre> <p>Using Spark or Koalas, you can deploy EDS-NLP pipelines on tens of millions of documents with ease!</p>"},{"location":"tutorials/multiple-texts/#one-function-to-rule-them-all","title":"One function to rule them all","text":"<p>EDS-NLP provides a wrapper to simplify deployment even further:</p> <pre><code># \u2191 Omitted code above \u2191\nfrom edsnlp.processing import pipe\n\n### Small pandas DataFrame\nnote_nlp = pipe(\n    note=df.limit(1000).toPandas(),\n    nlp=nlp,\n    n_jobs=1,\n    additional_spans=[\"dates\"],\n    extensions=[\"date.day\", \"date.month\", \"date.year\"],\n)\n\n### Larger pandas DataFrame\nnote_nlp = pipe(\n    note=df.limit(10000).toPandas(),\n    nlp=nlp,\n    n_jobs=-2,\n    additional_spans=[\"dates\"],\n    extensions=[\"date.day\", \"date.month\", \"date.year\"],\n)\n\n### Huge Spark or Koalas DataFrame\nnote_nlp = pipe(\n    note=df,\n    nlp=nlp,\n    how=\"spark\",\n    additional_spans=[\"dates\"],\n    extensions={\"date.year\": int_type, \"date.month\": int_type, \"date.day\": int_type},\n)\n</code></pre>"},{"location":"tutorials/qualifying-entities/","title":"Qualifying entities","text":"<p>In the previous tutorial, we saw how to match a terminology on a text. Using the <code>doc.ents</code> attribute, we can check whether a document mentions a concept of interest to build a cohort or describe patients.</p>"},{"location":"tutorials/qualifying-entities/#the-issue","title":"The issue","text":"<p>However, consider the classical example where we look for the <code>diabetes</code> concept:</p> FrenchEnglish <pre><code>Le patient n'est pas diab\u00e9tique.\nLe patient est peut-\u00eatre diab\u00e9tique.\nLe p\u00e8re du patient est diab\u00e9tique.\n</code></pre> <pre><code>The patient is not diabetic.\nThe patient could be diabetic.\nThe patient's father is diabetic.\n</code></pre> <p>None of these expressions should be used to build a cohort: the detected entity is either negated, speculative, or does not concern the patient themself. That's why we need to qualify the matched entities.</p> <p>Warning</p> <p>We show an English example just to explain the issue. EDS-NLP remains a French-language medical NLP library.</p>"},{"location":"tutorials/qualifying-entities/#the-solution","title":"The solution","text":"<p>We can use EDS-NLP's qualifier pipelines to achieve that. Let's add specific components to our pipeline to detect these three modalities.</p>"},{"location":"tutorials/qualifying-entities/#adding-qualifiers","title":"Adding qualifiers","text":"<p>Adding qualifier pipelines is straightforward:</p> <pre><code>import spacy\n\ntext = (\n    \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        regex=regex,\n        terms=terms,\n        attr=\"LOWER\",\n    ),\n)\n\nnlp.add_pipe(\"eds.sentences\")  # (1)\nnlp.add_pipe(\"eds.negation\")  # Negation component\nnlp.add_pipe(\"eds.hypothesis\")  # Speculation pipeline\nnlp.add_pipe(\"eds.family\")  # Family context detection\n</code></pre> <ol> <li>Qualifiers pipelines need sentence boundaries to be set (see the specific documentation for detail).</li> </ol> <p>This code is complete, and should run as is.</p>"},{"location":"tutorials/qualifying-entities/#reading-the-results","title":"Reading the results","text":"<p>Let's output the results as a pandas DataFrame for better readability:</p> <pre><code>import spacy\nimport pandas as pd\ntext = (\n    \"Motif de prise en charge : probable pneumopathie \u00e0 COVID19, \"\n    \"sans difficult\u00e9s respiratoires\\n\"\n    \"Le p\u00e8re du patient est asthmatique.\"\n)\n\nregex = dict(\n    covid=r\"(coronavirus|covid[-\\s]?19)\",\n    respiratoire=r\"respiratoires?\",\n)\nterms = dict(respiratoire=\"asthmatique\")\n\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        regex=regex,\n        terms=terms,\n        attr=\"LOWER\",\n    ),\n)\n\nnlp.add_pipe(\"eds.sentences\")\n\nnlp.add_pipe(\"eds.negation\")  # Negation component\nnlp.add_pipe(\"eds.hypothesis\")  # Speculation pipeline\nnlp.add_pipe(\"eds.family\")  # Family context detection\n\ndoc = nlp(text)\n\n# Extraction as a pandas DataFrame\nentities = []\nfor ent in doc.ents:\nd = dict(\nlexical_variant=ent.text,\nlabel=ent.label_,\nnegation=ent._.negation,\nhypothesis=ent._.hypothesis,\nfamily=ent._.family,\n)\nentities.append(d)\ndf = pd.DataFrame.from_records(entities)\n</code></pre> <p>This code is complete, and should run as is.</p> <p>We get the following result:</p> lexical_variant label negation hypothesis family COVID19 covid False True False respiratoires respiratoire True False False asthmatique respiratoire False False True"},{"location":"tutorials/qualifying-entities/#conclusion","title":"Conclusion","text":"<p>The qualifier pipelines limits the number of false positives by detecting linguistic modulations such as negations or speculations. Go to the full documentation for a complete presentation of the different pipelines, their configuration options and validation performance.</p> <p>Recall the qualifier pipeline proposed by EDS-NLP:</p> Pipeline Description <code>eds.negation</code> Rule-based negation detection <code>eds.family</code> Rule-based family context detection <code>eds.hypothesis</code> Rule-based speculation detection <code>eds.reported_speech</code> Rule-based reported speech detection <code>eds.history</code> Rule-based history detection"},{"location":"tutorials/quick-examples/","title":"Display single text outputs","text":"<p>If you are</p> <ul> <li>Developping a new pipeline</li> <li>Testing various inputs on an existing pipeline</li> <li>...</li> </ul> <p>you might want to quickly apply a pipeline and display the output <code>doc</code> in a comprehensible way.</p> <pre><code>from edsnlp.viz import QuickExample\n\nE = QuickExample(nlp)  # (1)\n</code></pre> <ol> <li>This is the <code>Language</code> instance that should be defined beforehands</li> </ol> <p>Next, simply call <code>E</code> with any string:</p> <pre><code>txt = \"Le patient pr\u00e9sente une anomalie.\"\nE(txt)\n</code></pre> <pre>                              Le patient pr\u00e9sente une anomalie                               \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Entity   \u2503 Source   \u2503 eds.hypoth\u2026 \u2503 eds.negation \u2503 eds.family \u2503 eds.history \u2503 eds.report\u2026 \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 patient  \u2502 patient  \u2502 False       \u2502 False        \u2502 False      \u2502 False       \u2502 False       \u2502\n\u2502 anomalie \u2502 anomalie \u2502 False       \u2502 False        \u2502 False      \u2502 False       \u2502 False       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <p>By default, each <code>Qualifiers</code> in <code>nlp</code> adds a corresponding column to the output. Additionnal informations can be displayed by using the <code>extensions</code> parameter. For instance, if entities have a custom <code>ent._.custom_ext</code> extensions, it can be displayed by providing the extension when instantiating <code>QuickExample</code>:</p> <pre><code>E = QuickExample(nlp, extensions=[\"_.custom_ext\"])\n</code></pre> <p>Finally, if you prefer to output a DataFrame instead of displaying a table, set the <code>as_dataframe</code> parameter to True:</p> <pre><code>E = QuickExample(nlp)\nE(txt, as_dataframe=True)\n</code></pre>"},{"location":"tutorials/reason/","title":"Detecting Reason of Hospitalisation","text":"<p>In this tutorial we will use the pipeline <code>eds.reason</code> to :</p> <ul> <li>Identify spans that corresponds to the reason of hospitalisation</li> <li>Check if there are named entities overlapping with my span of 'reason of hospitalisation'</li> <li>Check for all named entities if they are tagged <code>is_reason</code></li> </ul> <pre><code>import spacy\n\ntext = \"\"\"COMPTE RENDU D'HOSPITALISATION du 11/07/2018 au 12/07/2018\nMOTIF D'HOSPITALISATION\nMonsieur Dupont Jean Michel, de sexe masculin, \u00e2g\u00e9e de 39 ans, n\u00e9e le 23/11/1978,\na \u00e9t\u00e9 hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n\nANT\u00c9C\u00c9DENTS\nAnt\u00e9c\u00e9dents m\u00e9dicaux :\nPremier \u00e9pisode d'asthme en mai 2018.\"\"\"\n\nnlp = spacy.blank(\"fr\")\n\n# Extraction d'entit\u00e9s nomm\u00e9es\nnlp.add_pipe(\n    \"eds.matcher\",\n    config=dict(\n        terms=dict(\n            respiratoire=[\n                \"asthmatique\",\n                \"asthme\",\n                \"toux\",\n            ]\n        )\n    ),\n)\n\n\nnlp.add_pipe(\"eds.normalizer\")\nnlp.add_pipe(\"eds.sections\")\nnlp.add_pipe(\"eds.reason\", config=dict(use_sections=True))\n\ndoc = nlp(text)\n</code></pre> <p>The pipeline <code>reason</code> will add a key of spans called <code>reasons</code>. We check the first item in this list.</p> <pre><code># \u2191 Omitted code above \u2191\n\nreason = doc.spans[\"reasons\"][0]\nreason\n# Out: hospitalis\u00e9 du 11/08/2019 au 17/08/2019 pour attaque d'asthme.\n</code></pre> <p>Naturally, all spans included the <code>reasons</code> key have the attribute <code>reason._.is_reason == True</code>.</p> <pre><code># \u2191 Omitted code above \u2191\n\nreason._.is_reason\n# Out: True\n</code></pre> <pre><code># \u2191 Omitted code above \u2191\n\nentities = reason._.ents_reason  # (1)\nfor e in entities:\n    print(\n        \"Entity:\",\n        e.text,\n        \"-- Label:\",\n        e.label_,\n        \"-- is_reason:\",\n        e._.is_reason,\n    )\n# Out: Entity: asthme -- Label: respiratoire -- is_reason: True\n</code></pre> <ol> <li>We check if the span include named entities, their labels and the attribute is_reason</li> </ol> <p>We can verify that named entities that do not overlap with the spans of reason, have their attribute <code>reason._.is_reason == False</code>:</p> <pre><code>for e in doc.ents:\n    print(e.start, e, e._.is_reason)\n# Out: 42 asthme True\n# Out: 54 asthme False\n</code></pre>"},{"location":"tutorials/spacy101/","title":"spaCy 101","text":"<p>EDS-NLP is a spaCy library. To use it, you will need to familiarise yourself with some key spaCy concepts.</p> <p>Skip if you're familiar with spaCy</p> <p>This page is intended as a crash course for the very basic spaCy concepts that are needed to use EDS-NLP. If you've already used spaCy, you should probably skip to the next page.</p> <p>In a nutshell, spaCy offers three things:</p> <ul> <li>a convenient abstraction with a language-dependent, rule-based, deterministic and non-destructive tokenizer</li> <li>a rich set of rule-based and trainable components</li> <li>a configuration and training system</li> </ul> <p>We will focus on the first item.</p> <p>Be sure to check out spaCy's crash course page for more information on the possibilities offered by the library.</p>"},{"location":"tutorials/spacy101/#resources","title":"Resources","text":"<p>The spaCy documentation is one of the great strengths of the library. In particular, you should check out the \"Advanced NLP with spaCy\" course, which provides a more in-depth presentation.</p>"},{"location":"tutorials/spacy101/#spacy-in-action","title":"spaCy in action","text":"<p>Consider the following minimal example:</p> <pre><code>import spacy  # (1)\n\n# Initialise a spaCy pipeline\nnlp = spacy.blank(\"fr\")  # (2)\n\ntext = \"Michel est un penseur lat\u00e9ral.\"  # (3)\n\n# Apply the pipeline\ndoc = nlp(text)  # (4)\n\ndoc.text\n# Out: 'Michel est un penseur lat\u00e9ral.'\n</code></pre> <ol> <li>Import spaCy...</li> <li>Load a pipeline. In spaCy, the <code>nlp</code> object handles the entire processing.</li> <li>Define a text you want to process.</li> <li>Apply the pipeline and get a spaCy <code>Doc</code> object.</li> </ol> <p>We just created a spaCy pipeline and applied it to a sample text. It's that simple.</p> <p>Note that we use spaCy's \"blank\" NLP pipeline here. It actually carries a lot of information, and defines spaCy's language-dependent, rule-based tokenizer.</p> <p>Non-destructive processing</p> <p>In EDS-NLP, just like spaCy, non-destructiveness is a core principle. Your detected entities will always be linked to the original text.</p> <p>In other words, <code>nlp(text).text == text</code> is always true.</p>"},{"location":"tutorials/spacy101/#the-doc-abstraction","title":"The <code>Doc</code> abstraction","text":"<p>The <code>doc</code> object carries the result of the entire processing. It's the most important abstraction in spaCy, and holds a token-based representation of the text along with the results of every pipeline components. It also keeps track of the input text in a non-destructive manner, meaning that <code>doc.text == text</code> is always true.</p> <pre><code># \u2191 Omitted code above \u2191\n\n# Text processing in spaCy is non-destructive\ndoc.text == text  # (1)\n\n# You can access a specific token\ntoken = doc[2]  # (2)\n\n# And create a Span using slices\nspan = doc[:3]  # (3)\n\n# Entities are tracked in the ents attribute\ndoc.ents  # (4)\n# Out: ()\n</code></pre> <ol> <li>This feature is a core principle in spaCy. It will always be true in EDS-NLP.</li> <li><code>token</code> is a <code>Token</code> object referencing the third token</li> <li><code>span</code> is a <code>Span</code> object referencing the first three tokens.</li> <li>We have not declared any entity recognizer in our pipeline, hence this attribute is empty.</li> </ol>"},{"location":"tutorials/spacy101/#adding-pipeline-components","title":"Adding pipeline components","text":"<p>You can add pipeline components with the <code>nlp.add_pipe</code> method. Let's add two simple components to our pipeline.</p> <pre><code>import spacy\n\nnlp = spacy.blank(\"fr\")\n\nnlp.add_pipe(\"eds.sentences\")  # (1)\nnlp.add_pipe(\"eds.dates\")  # (2)\ntext = \"Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.\"\n\ndoc = nlp(text)\n</code></pre> <ol> <li>Like the name suggests, this pipeline is declared by EDS-NLP.    <code>eds.sentences</code> is a rule-based sentence boundary prediction.    See its documentation for detail.</li> <li>Like the name suggests, this pipeline is declared by EDS-NLP.    <code>eds.dates</code> is a date extraction and normalisation component.    See its documentation for detail.</li> </ol> <p>The <code>doc</code> object just became more interesting!</p> <pre><code># \u2191 Omitted code above \u2191\n\n# We can split the document into sentences\nlist(doc.sents)  # (1)\n# Out: [Le 5 mai 2005, Jimoth\u00e9 a \u00e9t\u00e9 invit\u00e9 \u00e0 une f\u00eate organis\u00e9e par Michel.]\n\n# And look for dates\ndoc.spans[\"dates\"]  # (2)\n# Out: [5 mai 2005]\n\nspan = doc.spans[\"dates\"][0]  # (3)\nspan._.date.to_datetime()  # (4)\n# Out: DateTime(2005, 5, 5, 0, 0, 0, tzinfo=Timezone('Europe/Paris'))\n</code></pre> <ol> <li>In this example, there is only one sentence...</li> <li>The <code>eds.dates</code> adds a key to the <code>doc.spans</code> attribute</li> <li><code>span</code> is a spaCy <code>Span</code> object.</li> <li>In spaCy, you can declare custom extensions that live in the <code>_</code> attribute.    Here, the <code>eds.dates</code> pipeline uses a <code>Span._.date</code> extension to persist the normalised date.    We use the <code>to_datetime()</code> method to get an object that is usable by Python.</li> </ol>"},{"location":"tutorials/spacy101/#conclusion","title":"Conclusion","text":"<p>This page is just a glimpse of a few possibilities offered by spaCy. To get a sense of what spaCy can help you achieve, we strongly recommend you visit their documentation and take the time to follow the spaCy course.</p> <p>Moreover, be sure to check out spaCy's own crash course, which is an excellent read. It goes into more detail on what's possible with the library.</p>"},{"location":"utilities/","title":"Utilities","text":"<p>EDS-NLP provides a few utilities to deploy pipelines, process RegExps, etc.</p>"},{"location":"utilities/evaluation/","title":"Pipeline evaluation","text":""},{"location":"utilities/matchers/","title":"Matchers","text":"<p>We implemented three pattern matchers that are fit to clinical documents:</p> <ul> <li>the <code>EDSPhraseMatcher</code></li> <li>the <code>RegexMatcher</code></li> <li>the <code>SimstringMatcher</code></li> </ul> <p>However, note that for most use-cases, you should instead use the <code>eds.matcher</code> pipeline that wraps these classes to annotate documents.</p>"},{"location":"utilities/matchers/#edsphrasematcher","title":"EDSPhraseMatcher","text":"<p>The EDSPhraseMatcher lets you efficiently match large terminology lists, by comparing tokenx against a given attribute. This matcher differs from the <code>spacy.PhraseMatcher</code> in that it allows to skip pollution tokens. To make it efficient, we have reimplemented the matching algorithm in Cython, like the original <code>spacy.PhraseMatcher</code>.</p> <p>You can use it as described in the code below.</p> <pre><code>import spacy\nfrom edsnlp.matchers.phrase import EDSPhraseMatcher\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.normalizer\")\ndoc = nlp(\"On ne rel\u00e8ve pas de signe du Corona =============== virus.\")\n\nmatcher = EDSPhraseMatcher(nlp.vocab, attr=\"NORM\")\nmatcher.build_patterns(\n    nlp,\n    {\n        \"covid\": [\"corona virus\", \"coronavirus\", \"covid\"],\n        \"diabete\": [\"diabete\", \"diabetique\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: Corona =============== virus\n</code></pre>"},{"location":"utilities/matchers/#regexmatcher","title":"RegexMatcher","text":"<p>The <code>RegexMatcher</code> performs full-text regex matching. It is especially useful to handle spelling variations like <code>mammo-?graphies?</code>. Like the <code>EDSPhraseMatcher</code>, this class allows to skip pollution tokens. Note that this class is significantly slower than the <code>EDSPhraseMatcher</code>: if you can, try enumerating lexical variations of the target phrases and feed them to the <code>PhraseMatcher</code> instead.</p> <p>You can use it as described in the code below.</p> <pre><code>import spacy\nfrom edsnlp.matchers.regex import RegexMatcher\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.normalizer\")\ndoc = nlp(\"On ne rel\u00e8ve pas de signe du Corona =============== virus.\")\n\nmatcher = RegexMatcher(attr=\"NORM\", ignore_excluded=True)\nmatcher.build_patterns(\n    {\n        \"covid\": [\"corona[ ]*virus\", \"covid\"],\n        \"diabete\": [\"diabete\", \"diabetique\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: Corona =============== virus\n</code></pre>"},{"location":"utilities/matchers/#simstringmatcher","title":"SimstringMatcher","text":"<p>The <code>SimstringMatcher</code> performs fuzzy term matching by comparing spans of text with a similarity metric. It is especially useful to handle spelling variations like <code>paracetomol</code> (instead of <code>paracetamol</code>).</p> <p>The <code>simstring</code> algorithm compares two strings by enumerating their char trigrams and measuring the overlap between the two sets. In the previous example: - <code>paracetomol</code> becomes <code>##p #pa par ara rac ace cet eto tom omo mol ol# l##</code> - <code>paracetamol</code> becomes <code>##p #pa par ara rac ace cet eta tam amo mol ol# l##</code> and the Dice (or F1) similarity between the two sets is 0.75.</p> <p>Like the <code>EDSPhraseMatcher</code>, this class allows to skip pollution tokens. Just like the <code>RegexMatcher</code>, this class is significantly slower than the <code>EDSPhraseMatcher</code>: if you can, try enumerating lexical variations of the target phrases and feed them to the <code>PhraseMatcher</code> instead.</p> <p>You can use it as described in the code below.</p> <pre><code>import spacy\nfrom edsnlp.matchers.simstring import SimstringMatcher\n\nnlp = spacy.blank(\"eds\")\nnlp.add_pipe(\"eds.normalizer\")\ndoc = nlp(\n    \"On ne rel\u00e8ve pas de signe du corona-virus. Historique d'un hepatocellulaire carcinome.\"\n)\n\nmatcher = SimstringMatcher(\n    nlp.vocab,\n    attr=\"NORM\",\n    ignore_excluded=True,\n    measure=\"dice\",\n    threshold=0.75,\n    windows=5,\n)\nmatcher.build_patterns(\n    nlp,\n    {\n        \"covid\": [\"coronavirus\", \"covid\"],\n        \"carcinome\": [\"carcinome hepatocellulaire\"],\n    },\n)\n\nlist(matcher(doc, as_spans=True))[0].text\n# Out: corona-virus\n\nlist(matcher(doc, as_spans=True))[1].text\n# Out: hepatocellulaire carcinome\n</code></pre>"},{"location":"utilities/regex/","title":"Work with RegExp","text":""},{"location":"utilities/connectors/","title":"Overview of connectors","text":"<p>EDS-NLP provides a series of connectors apt to convert back and forth from different formats into spaCy representation.</p> <p>We provide the following connectors:</p> <ul> <li>BRAT</li> <li>OMOP</li> </ul>"},{"location":"utilities/connectors/brat/","title":"BRAT Connector","text":"<p>BRAT is currently the only supported in-text annotation editor at EDS. BRAT annotations are in the standoff format. Consider the following document:</p> <pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>It could be annotated as follows :</p> <pre><code>T1  Patient 4 11    patient\nT2  Disease 31 58   pneumopathie au coronavirus\nT3  Drug 79 90  parac\u00e9tamol\n</code></pre> <p>The point of the BRAT connector is to go from the standoff annotation format to an annotated spaCy document :</p> <pre><code>import spacy\nfrom edsnlp.connectors.brat import BratConnector\n\n# Instantiate the connector\nbrat = BratConnector(\"path/to/brat\")\n\n# Instantiate the spacy pipeline\nnlp = spacy.blank(\"fr\")\n\n# Convert all BRAT files to a list of documents\ndocs = brat.brat2docs(nlp)\ndoc = docs[0]\n\ndoc.ents\n# Out: [patient, pneumopathie au coronavirus, parac\u00e9tamol]\n\ndoc.ents[0].label_\n# Out: Patient\n</code></pre> <p>The connector can also go the other way around, enabling pre-annotations and an ersatz of active learning.</p>"},{"location":"utilities/connectors/labeltool/","title":"LabelTool Connector","text":"<p>LabelTool is an in-house module enabling rapid annotation of pre-extracted entities.</p> <p>We provide a ready-to-use function that converts a list of annotated spaCy documents into a <code>pandas</code> DataFrame that is readable to LabelTool.</p> <pre><code>import spacy\n\nfrom edsnlp.connectors.labeltool import docs2labeltool\n\ncorpus = [\n    \"Ceci est un document m\u00e9dical.\",\n    \"Le patient n'est pas malade.\",\n]\n\n# Instantiate the spacy pipeline\nnlp = spacy.blank(\"fr\")\nnlp.add_pipe(\"eds.sentences\")\nnlp.add_pipe(\"eds.matcher\", config=dict(terms=dict(medical=\"m\u00e9dical\", malade=\"malade\")))\nnlp.add_pipe(\"eds.negation\")\n\n# Convert all BRAT files to a list of documents\ndocs = nlp.pipe(corpus)\n\ndf = docs2labeltool(docs, extensions=[\"negation\"])\n</code></pre> <p>The results:</p> note_id note_text start end label lexical_variant negation 0 Ceci est un document m\u00e9dical. 21 28 medical m\u00e9dical False 1 Le patient n'est pas malade. 21 27 malade malade True"},{"location":"utilities/connectors/omop/","title":"OMOP Connector","text":"<p>We provide a connector between OMOP-formatted dataframes and spaCy documents.</p>"},{"location":"utilities/connectors/omop/#omop-style-dataframes","title":"OMOP-style dataframes","text":"<p>Consider a corpus of just one document:</p> <pre><code>Le patient est admis pour une pneumopathie au coronavirus.\nOn lui prescrit du parac\u00e9tamol.\n</code></pre> <p>And its OMOP-style representation, separated in two tables <code>note</code> and <code>note_nlp</code> (here with selected columns) :</p> <p><code>note</code>:</p> note_id note_text note_datetime 0 Le patient est admis pour une pneumopathie... 2021-10-23 <p><code>note_nlp</code>:</p> note_nlp_id note_id start_char end_char note_nlp_source_value lexical_variant 0 0 46 57 disease coronavirus 1 0 77 88 drug parac\u00e9tamol"},{"location":"utilities/connectors/omop/#using-the-connector","title":"Using the connector","text":"<p>The following snippet expects the tables <code>note</code> and <code>note_nlp</code> to be already defined (eg through PySpark's <code>toPandas()</code> method).</p> <pre><code>import spacy\nfrom edsnlp.connectors.omop import OmopConnector\n\n# Instantiate a spacy pipeline\nnlp = spacy.blank(\"fr\")\n\n# Instantiate the connector\nconnector = OmopConnector(nlp)\n\n# Convert OMOP tables (note and note_nlp) to a list of documents\ndocs = connector.omop2docs(note, note_nlp)\ndoc = docs[0]\n\ndoc.ents\n# Out: [coronavirus, parac\u00e9tamol]\n\ndoc.ents[0].label_\n# Out: 'disease'\n\ndoc.text == note.loc[0].note_text\n# Out: True\n</code></pre> <p>The object <code>docs</code> now contains a list of documents that reflects the information contained in the OMOP-formatted dataframes.</p>"},{"location":"utilities/processing/","title":"Overview of processing","text":""},{"location":"utilities/processing/multi/","title":"Multiprocessing","text":""},{"location":"utilities/processing/single/","title":"Single processing","text":""},{"location":"utilities/processing/spark/","title":"Deploying on Spark","text":"<p>We provide a simple connector to distribute a pipeline on a Spark cluster. We expose a Spark UDF (user-defined function) factory that handles the nitty gritty of distributing a pipeline over a cluster of Spark-enabled machines.</p>"},{"location":"utilities/processing/spark/#distributing-a-pipeline","title":"Distributing a pipeline","text":"<p>Because of the way Spark distributes Python objects, we need to re-declare custom extensions on the executors. To make this step as smooth as possible, EDS-NLP provides a <code>BaseComponent</code> class that implements a <code>set_extensions</code> method. When the pipeline is distributed, every component that extend <code>BaseComponent</code> rerun their <code>set_extensions</code> method.</p> <p>Since spaCy <code>Doc</code> objects cannot easily be serialised, the UDF we provide returns a list of detected entities along with selected qualifiers.</p>"},{"location":"utilities/processing/spark/#example","title":"Example","text":"<p>See the dedicated tutorial for a step-by-step presentation.</p>"},{"location":"utilities/processing/spark/#authors-and-citation","title":"Authors and citation","text":"<p>The Spark connector was developed by AP-HP's Data Science team.</p>"},{"location":"utilities/tests/","title":"Tests Utilities","text":"<p>We provide a few testing utilities that simplify the process of:</p> <ul> <li>creating testing examples for NLP pipelines;</li> <li>testing documentation code blocs.</li> </ul>"},{"location":"utilities/tests/blocs/","title":"Testing Code Blocs","text":"<p>We created a utility that scans through markdown files, extracts code blocs and executes them to check that everything is indeed functional.</p> <p>There is more! Whenever the utility comes across an example (denoted by <code># Out:</code>, see example below), an <code>assert</code> statement is dynamically added to the snippet to check that the output matches.</p> <p>For instance:</p> <pre><code>a = 1\n\na\n# Out: 1\n</code></pre> <p>Is transformed into:</p> <pre><code>a = 1\n\nv = a\nassert repr(v) == \"1\"\n</code></pre> <p>We can disable code checking for a specific code bloc by adding <code>&lt;!-- no-check --&gt;</code> above it:</p> <pre><code>&lt;!-- no-check --&gt;\n\n```python\ntest = undeclared_function(42)\n```\n</code></pre> <p>See the dedicated reference for more information</p>"},{"location":"utilities/tests/examples/","title":"Creating Examples","text":"<p>Testing a NER/qualifier pipeline can be a hassle. We created a utility to simplify that process.</p> <p>Using the <code>parse_example</code> method, you can define a full example in a human-readable way:</p> <pre><code>from edsnlp.utils.examples import parse_example\n\nexample = \"Absence d'&lt;ent negated=true&gt;image osseuse d'allure \u00e9volutive&lt;/ent&gt;.\"\n\ntext, entities = parse_example(example)\n\ntext\n# Out: \"Absence d'image osseuse d'allure \u00e9volutive.\"\n\nentities\n# Out: [Entity(start_char=10, end_char=42, modifiers=[Modifier(key='negated', value=True)])]\n</code></pre> <p>Entities are defined using the <code>&lt;ent&gt;</code> tag. You can encode complexe information by adding keys into the tag (see example above). The <code>parse_example</code> method strips the text of the tags, and outputs a list of <code>Entity</code> objects that contain:</p> <ul> <li>the character indices of the entity ;</li> <li>custom user-defined \"modifiers\".</li> </ul> <p>See the dedicated reference page for more information.</p>"}]}